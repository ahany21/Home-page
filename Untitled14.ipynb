{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eDRsFBfXusxm",
        "8ZCgRSE5wht4",
        "duJnEH5kp7UB",
        "bgTOthIorWT4"
      ],
      "mount_file_id": "10C5mGzqDmzLSOcfJ41izIeMm-8Vr9G5O",
      "authorship_tag": "ABX9TyMvobqSPf8Yj486V8WNDEoJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ff451a3e6b654c599aa7e21a6b87c491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ae6e8f762124fde83ec50dc828e5f49",
              "IPY_MODEL_6c395d04662d44ad9a5ad4d1f4912350",
              "IPY_MODEL_ef02584088b84931a9f5d9b0959a7bfa"
            ],
            "layout": "IPY_MODEL_17831eed1ad54e188917b0219eb50740"
          }
        },
        "2ae6e8f762124fde83ec50dc828e5f49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6a70d8b283246a0b3634cf4a52fbebe",
            "placeholder": "​",
            "style": "IPY_MODEL_5dd3e43e2eca49c4bdd50b727ce0e9fd",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6c395d04662d44ad9a5ad4d1f4912350": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4e3032758e24e36b8b413e2ce662a8e",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e610aeba5e3c4f8d8b352997712f8538",
            "value": 48
          }
        },
        "ef02584088b84931a9f5d9b0959a7bfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4b7f21b75de48afb37601119632c68b",
            "placeholder": "​",
            "style": "IPY_MODEL_90bc7726174642cfaa620766c7ac9381",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.63kB/s]"
          }
        },
        "17831eed1ad54e188917b0219eb50740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6a70d8b283246a0b3634cf4a52fbebe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd3e43e2eca49c4bdd50b727ce0e9fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4e3032758e24e36b8b413e2ce662a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e610aeba5e3c4f8d8b352997712f8538": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4b7f21b75de48afb37601119632c68b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90bc7726174642cfaa620766c7ac9381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "870a80a3d8574730bcda7ffef75d5aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72ac78e2c8a54f02b021baff23de2a84",
              "IPY_MODEL_54f623d981cb43bba0b76c2d1e90418a",
              "IPY_MODEL_eb0acb82b0d54fb0aaf6b8cac3a4a7a1"
            ],
            "layout": "IPY_MODEL_f3df11b5a07b4c0ea506f0082ca0955f"
          }
        },
        "72ac78e2c8a54f02b021baff23de2a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54e179e1d736494189f58cc785f01c24",
            "placeholder": "​",
            "style": "IPY_MODEL_cc8c7468cbdd40089010b86e02fd95e1",
            "value": "config.json: 100%"
          }
        },
        "54f623d981cb43bba0b76c2d1e90418a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3956491227044a6fb488cae61b301682",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59f9a321b5854d11b113d2c98bd9f3a5",
            "value": 570
          }
        },
        "eb0acb82b0d54fb0aaf6b8cac3a4a7a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_149b7be869774e68b9acfa044d4c1c60",
            "placeholder": "​",
            "style": "IPY_MODEL_53feb9f27a8a4eae835ff8ac5faab084",
            "value": " 570/570 [00:00&lt;00:00, 18.4kB/s]"
          }
        },
        "f3df11b5a07b4c0ea506f0082ca0955f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54e179e1d736494189f58cc785f01c24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8c7468cbdd40089010b86e02fd95e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3956491227044a6fb488cae61b301682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59f9a321b5854d11b113d2c98bd9f3a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "149b7be869774e68b9acfa044d4c1c60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53feb9f27a8a4eae835ff8ac5faab084": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f24e784a6a4424493b20208c8743c79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eff4c85cd7d54c1ba5e2bb3a7fdb3ac8",
              "IPY_MODEL_790af4f117b542e2bb35b4e650bd59b8",
              "IPY_MODEL_cb2da0f4496e4200a0e8e536d19159f7"
            ],
            "layout": "IPY_MODEL_9adad984de0d49dda90a346b5b6dba42"
          }
        },
        "eff4c85cd7d54c1ba5e2bb3a7fdb3ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_045a1a262c1941dd96ab3a0c1ae045c5",
            "placeholder": "​",
            "style": "IPY_MODEL_fa042d041f6948d493728bd024de7606",
            "value": "vocab.txt: 100%"
          }
        },
        "790af4f117b542e2bb35b4e650bd59b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b31efd05a4f74e5ea521ad00f86b3a85",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2945fe9f81e24e47b3bb78dce946e315",
            "value": 231508
          }
        },
        "cb2da0f4496e4200a0e8e536d19159f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d09aa129df34d2494551bed1b3b6630",
            "placeholder": "​",
            "style": "IPY_MODEL_72364835451742f6b4a614eb7bf88433",
            "value": " 232k/232k [00:00&lt;00:00, 2.67MB/s]"
          }
        },
        "9adad984de0d49dda90a346b5b6dba42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "045a1a262c1941dd96ab3a0c1ae045c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa042d041f6948d493728bd024de7606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b31efd05a4f74e5ea521ad00f86b3a85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2945fe9f81e24e47b3bb78dce946e315": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d09aa129df34d2494551bed1b3b6630": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72364835451742f6b4a614eb7bf88433": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cfb9922940e4f1496f01ea4c8ff97d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2dfb008084e841a99346e7df704a8fc0",
              "IPY_MODEL_4817fd0a174f4841a0fa34dcc07ca10c",
              "IPY_MODEL_d16283506f414c8ca65a9a6effe2c384"
            ],
            "layout": "IPY_MODEL_840a2d7dc8004a128f1cd4a543457530"
          }
        },
        "2dfb008084e841a99346e7df704a8fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3eb2991c962438d8dbd442e2c0a3917",
            "placeholder": "​",
            "style": "IPY_MODEL_ac5738abdf5a47e484091eaa4b0090a0",
            "value": "tokenizer.json: 100%"
          }
        },
        "4817fd0a174f4841a0fa34dcc07ca10c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95de10945840484c9c74cf64b9eb2fcb",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ec46914678364b5ab666b42131c943eb",
            "value": 466062
          }
        },
        "d16283506f414c8ca65a9a6effe2c384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_941513b472a9458a85a0b409af562c35",
            "placeholder": "​",
            "style": "IPY_MODEL_b311afe0d29f40f1921d13f4424b017b",
            "value": " 466k/466k [00:00&lt;00:00, 6.05MB/s]"
          }
        },
        "840a2d7dc8004a128f1cd4a543457530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3eb2991c962438d8dbd442e2c0a3917": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5738abdf5a47e484091eaa4b0090a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95de10945840484c9c74cf64b9eb2fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec46914678364b5ab666b42131c943eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "941513b472a9458a85a0b409af562c35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b311afe0d29f40f1921d13f4424b017b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d43a1156d1a495fb0971ec3714d6962": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_172749fe2f2d4a76a88f6f270dfc8a3a",
              "IPY_MODEL_73b6e1f4046c42008c02814ba8619bba",
              "IPY_MODEL_b4aaaf94fd9a42cb9a6642bf733fcc57"
            ],
            "layout": "IPY_MODEL_0d47e241da74459db1d90b4109b400ee"
          }
        },
        "172749fe2f2d4a76a88f6f270dfc8a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1edf157dd0bf4b70bae3f40e33ed6452",
            "placeholder": "​",
            "style": "IPY_MODEL_e0e6eef66dc6476a9410fd023ced88d1",
            "value": "model.safetensors: 100%"
          }
        },
        "73b6e1f4046c42008c02814ba8619bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_456e5395de9340aca645af39c6505300",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e0e43c4f31c455296a6f65880619fc3",
            "value": 440449768
          }
        },
        "b4aaaf94fd9a42cb9a6642bf733fcc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6d9d2cebc994625824ba3fd01dd46c8",
            "placeholder": "​",
            "style": "IPY_MODEL_937b0fc2f6f84c629edf39f0d2f4ed53",
            "value": " 440M/440M [00:11&lt;00:00, 58.2MB/s]"
          }
        },
        "0d47e241da74459db1d90b4109b400ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1edf157dd0bf4b70bae3f40e33ed6452": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0e6eef66dc6476a9410fd023ced88d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "456e5395de9340aca645af39c6505300": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e0e43c4f31c455296a6f65880619fc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6d9d2cebc994625824ba3fd01dd46c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "937b0fc2f6f84c629edf39f0d2f4ed53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahany21/Home-page/blob/master/Untitled14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FLUTE SETUP"
      ],
      "metadata": {
        "id": "pSVzsQItuiYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/microsoft/msrflute.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuQgtJVXeSKd",
        "outputId": "53e80937-6dcf-41ca-e738-0c208efbf959",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'msrflute'...\n",
            "remote: Enumerating objects: 854, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 854 (delta 115), reused 103 (delta 103), pack-reused 716 (from 1)\u001b[K\n",
            "Receiving objects: 100% (854/854), 4.74 MiB | 8.97 MiB/s, done.\n",
            "Resolving deltas: 100% (471/471), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd msrflute\n",
        "# %cd experiments\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV0sa4uieSND",
        "outputId": "0b709ada-f842-4407-8ef5-c9c6c0d2e85d",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/msrflute\n",
            "azure-pipelines.yml  configs\t      e2e_trainer.py  NOTICE.txt\ttesting\n",
            "CHANGELOG.md\t     CONTRIBUTING.md  experiments     README.md\t\tutils\n",
            "CITATION.cff\t     core\t      extensions      requirements.txt\n",
            "CODE_OF_CONDUCT.md   doc\t      LICENSE.TXT     SECURITY.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up conflicting packages in requirements.txt\n",
        "!sed -i '/^torch==1.11.0$/d' requirements.txt\n",
        "!sed -i '/^tensorflow$/d' requirements.txt\n",
        "!sed -i '/^azureml-defaults$/d' requirements.txt\n",
        "!sed -i '/^scikit-learn$/d' requirements.txt\n",
        "!sed -i '/^scipy$/d' requirements.txt\n",
        "!sed -i '/^h5py$/d' requirements.txt"
      ],
      "metadata": {
        "id": "0qNG-4gneSPn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy jax jaxlib transformers peft torch torchvision torchaudio tensorflow azureml-defaults azureml-core scikit-learn scipy h5py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4RJLC0sxeSRv",
        "outputId": "5b21d12a-82fa-425e-fa5c-6609511e8d0c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Found existing installation: jax 0.5.2\n",
            "Uninstalling jax-0.5.2:\n",
            "  Successfully uninstalled jax-0.5.2\n",
            "Found existing installation: jaxlib 0.5.1\n",
            "Uninstalling jaxlib-0.5.1:\n",
            "  Successfully uninstalled jaxlib-0.5.1\n",
            "Found existing installation: transformers 4.51.3\n",
            "Uninstalling transformers-4.51.3:\n",
            "  Successfully uninstalled transformers-4.51.3\n",
            "Found existing installation: peft 0.15.2\n",
            "Uninstalling peft-0.15.2:\n",
            "  Successfully uninstalled peft-0.15.2\n",
            "Found existing installation: torch 2.6.0+cu124\n",
            "Uninstalling torch-2.6.0+cu124:\n",
            "  Successfully uninstalled torch-2.6.0+cu124\n",
            "Found existing installation: torchvision 0.21.0+cu124\n",
            "Uninstalling torchvision-0.21.0+cu124:\n",
            "  Successfully uninstalled torchvision-0.21.0+cu124\n",
            "Found existing installation: torchaudio 2.6.0+cu124\n",
            "Uninstalling torchaudio-2.6.0+cu124:\n",
            "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
            "Found existing installation: tensorflow 2.18.0\n",
            "Uninstalling tensorflow-2.18.0:\n",
            "  Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[33mWARNING: Skipping azureml-defaults as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping azureml-core as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: scikit-learn 1.6.1\n",
            "Uninstalling scikit-learn-1.6.1:\n",
            "  Successfully uninstalled scikit-learn-1.6.1\n",
            "Found existing installation: scipy 1.15.2\n",
            "Uninstalling scipy-1.15.2:\n",
            "  Successfully uninstalled scipy-1.15.2\n",
            "Found existing installation: h5py 3.13.0\n",
            "Uninstalling h5py-3.13.0:\n",
            "  Successfully uninstalled h5py-3.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTNPeE0Sj4Np",
        "outputId": "19e2af57-b9a1-4c74-ee37-707cba764a9e",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires jax>=0.1.72, which is not installed.\n",
            "dopamine-rl 4.1.2 requires jaxlib>=0.1.51, which is not installed.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "pytensor 2.30.3 requires scipy<2,>=1, which is not installed.\n",
            "chex 0.1.89 requires jax>=0.4.27, which is not installed.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "accelerate 1.6.0 requires torch>=2.0.0, which is not installed.\n",
            "pymc 5.22.0 requires scipy>=1.4.1, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "shap 0.47.2 requires scipy, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, which is not installed.\n",
            "fastai 2.7.19 requires torchvision>=0.11, which is not installed.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\n",
            "albumentations 2.0.6 requires scipy>=1.10.0, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "lightgbm 4.5.0 requires scipy, which is not installed.\n",
            "keras 3.8.0 requires h5py, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "flax 0.10.6 requires jax>=0.5.1, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "optax 0.2.4 requires jax>=0.4.27, which is not installed.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, which is not installed.\n",
            "clarabel 0.10.0 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "pynndescent 0.5.13 requires scipy>=1.0, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "osqp 1.0.3 requires scipy>=0.13.2, which is not installed.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(f\"NumPy version after force install: {numpy.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzfFDlP7eSWq",
        "outputId": "edae7c93-5932-4f6f-d163-aafdea1210fa",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version after force install: 2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.6.0 torchvision==0.21.0 torchaudio==2.6.0 -f https://download.pytorch.org/whl/cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NrZGYcZeSY9",
        "outputId": "269cc407-b8d3-460e-cf1c-8fabf767e5b4",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/cpu\n",
            "Collecting torch==2.6.0\n",
            "  Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Collecting torchvision==0.21.0\n",
            "  Downloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
            "Collecting torchaudio==2.6.0\n",
            "  Downloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.21.0) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
            "Downloading torch-2.6.0-cp311-cp311-manylinux1_x86_64.whl (766.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.21.0-cp311-cp311-manylinux1_x86_64.whl (7.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchaudio-2.6.0-cp311-cp311-manylinux1_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m56.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scikit-learn, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scipy, which is not installed.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.6.0 torchaudio-2.6.0 torchvision-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.40.2 peft==0.7.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "f-SlCqz5eSbH",
        "outputId": "2a9182b2-bf6d-4bb1-eb12-ee1c8f302eb5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.40.2\n",
            "  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/138.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.7.0\n",
            "  Downloading peft-0.7.0-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (2.32.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.40.2)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.40.2) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0) (2.6.0)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft==0.7.0) (1.6.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.2) (1.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft==0.7.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft==0.7.0) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.40.2) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft==0.7.0) (3.0.2)\n",
            "Downloading transformers-4.40.2-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.7.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers, peft\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires scikit-learn, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scipy, which is not installed.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed peft-0.7.0 tokenizers-0.19.1 transformers-4.40.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jax==0.4.25 jaxlib==0.4.25"
      ],
      "metadata": {
        "id": "JaqEBihMeSdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13099f91-82b0-42b7-87bb-31d20d6906bb",
        "collapsed": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jax==0.4.25\n",
            "  Downloading jax-0.4.25-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting jaxlib==0.4.25\n",
            "  Downloading jaxlib-0.4.25-cp311-cp311-manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.25) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.25) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax==0.4.25) (3.4.0)\n",
            "Collecting scipy>=1.9 (from jax==0.4.25)\n",
            "  Downloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax-0.4.25-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.25-cp311-cp311-manylinux2014_x86_64.whl (79.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.2/79.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.15.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/37.7 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, jaxlib, jax\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, which is not installed.\n",
            "yellowbrick 1.5 requires scikit-learn>=1.0.0, which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scikit-learn>=0.23.0, which is not installed.\n",
            "librosa 0.11.0 requires scikit-learn>=1.1.0, which is not installed.\n",
            "shap 0.47.2 requires scikit-learn, which is not installed.\n",
            "fastai 2.7.19 requires scikit-learn, which is not installed.\n",
            "umap-learn 0.5.7 requires scikit-learn>=0.22, which is not installed.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, which is not installed.\n",
            "libpysal 4.13.0 requires scikit-learn>=1.1, which is not installed.\n",
            "tsfresh 0.21.0 requires scikit-learn>=0.22.0, which is not installed.\n",
            "sentence-transformers 4.1.0 requires scikit-learn, which is not installed.\n",
            "hdbscan 0.8.40 requires scikit-learn>=0.20, which is not installed.\n",
            "pynndescent 0.5.13 requires scikit-learn>=0.18, which is not installed.\n",
            "chex 0.1.89 requires jax>=0.4.27, but you have jax 0.4.25 which is incompatible.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, but you have jaxlib 0.4.25 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.25 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.25 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\n",
            "optax 0.2.4 requires jax>=0.4.27, but you have jax 0.4.25 which is incompatible.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, but you have jaxlib 0.4.25 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.25 jaxlib-0.4.25 scipy-1.15.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.15.0\n",
        "# If you removed azureml-core, uncomment the line below\n",
        "# !pip install azureml-core"
      ],
      "metadata": {
        "id": "2Nei0HxeeSfy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0696740d-eb56-429b-acf6-b0d00968a9c0",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow==2.15.0\n",
            "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.2.0)\n",
            "Collecting h5py>=2.9.0 (from tensorflow==2.15.0)\n",
            "  Downloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0)\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.0)\n",
            "  Downloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (4.13.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0)\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0) (1.71.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.0)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.0) (0.45.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.8)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.1.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.0) (3.2.2)\n",
            "Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5py-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.7-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, tensorflow-estimator, protobuf, ml-dtypes, keras, h5py, tensorboard, tensorflow\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 3.8.0\n",
            "    Uninstalling keras-3.8.0:\n",
            "      Successfully uninstalled keras-3.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.89 requires jax>=0.4.27, but you have jax 0.4.25 which is incompatible.\n",
            "chex 0.1.89 requires jaxlib>=0.4.27, but you have jaxlib 0.4.25 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.7 which is incompatible.\n",
            "yfinance 0.2.59 requires protobuf<6,>=5.29.0, but you have protobuf 4.25.7 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.25 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.0 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.25 which is incompatible.\n",
            "tensorstore 0.1.74 requires ml_dtypes>=0.3.1, but you have ml-dtypes 0.2.0 which is incompatible.\n",
            "optax 0.2.4 requires jax>=0.4.27, but you have jax 0.4.25 which is incompatible.\n",
            "optax 0.2.4 requires jaxlib>=0.4.27, but you have jaxlib 0.4.25 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h5py-3.13.0 keras-2.15.0 ml-dtypes-0.2.0 protobuf-4.25.7 tensorboard-2.15.2 tensorflow-2.15.0 tensorflow-estimator-2.15.0 wrapt-1.14.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn scipy h5py\n"
      ],
      "metadata": {
        "id": "AsU_3EVCeSiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8672b676-c062-4ae0-e1bf-084c34fbeee7",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Downloading scikit_learn-1.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m81.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-learn\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.40.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikit-learn-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "JuaRAfqVeSlR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e01e49d-a023-431c-917d-210bab9dc3d3",
        "collapsed": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py (from -r requirements.txt (line 1))\n",
            "  Downloading mpi4py-4.0.3.tar.gz (466 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/466.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m460.8/466.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.13)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (5.9.5)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (4.40.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (2.2.2)\n",
            "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/sphinx-rtd-theme/\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting sphinx_rtd_theme (from -r requirements.txt (line 7))\n",
            "  Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting azureml-core (from -r requirements.txt (line 8))\n",
            "  Downloading azureml_core-1.60.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (6.0.2)\n",
            "Collecting cerberus (from -r requirements.txt (line 10))\n",
            "  Downloading Cerberus-1.3.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 11)) (4.25.7)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 12)) (0.2.0)\n",
            "Requirement already satisfied: googledrivedownloader in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.1.0)\n",
            "Collecting wget (from -r requirements.txt (line 14))\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (0.31.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 4)) (4.67.1)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 5)) (2.6.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->-r requirements.txt (line 5)) (11.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision->-r requirements.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 6)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
            "Requirement already satisfied: sphinx<9,>=6 in /usr/local/lib/python3.11/dist-packages (from sphinx_rtd_theme->-r requirements.txt (line 7)) (8.2.3)\n",
            "Requirement already satisfied: docutils<0.22,>0.18 in /usr/local/lib/python3.11/dist-packages (from sphinx_rtd_theme->-r requirements.txt (line 7)) (0.21.2)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx_rtd_theme->-r requirements.txt (line 7))\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting backports.tempfile (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading backports.tempfile-1.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pathspec<1.0.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting msal<2.0.0,>=1.15.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading msal-1.32.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions<=2.0.0,>=0.3.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting knack<0.13.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading knack-0.12.0-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting azure-core<2.0.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_core-1.34.0-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.9/42.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pkginfo (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading pkginfo-1.12.1.2-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting argcomplete<4 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading argcomplete-3.6.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting humanfriendly<11.0,>=4.7 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting paramiko<4.0.0,>=2.0.8 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading paramiko-3.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting azure-mgmt-resource<=24.0.0,>=15.0.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_mgmt_resource-23.3.0-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-mgmt-containerregistry<11,>=8.2.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_mgmt_containerregistry-10.3.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting azure-mgmt-storage<=23.0.0,>=16.0.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_mgmt_storage-22.2.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting azure-mgmt-keyvault<11.0.0,>=0.40.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_mgmt_keyvault-10.3.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting azure-mgmt-authorization<5,>=0.40.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_mgmt_authorization-4.0.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting azure-mgmt-network<=29.0.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_mgmt_network-28.1.0-py3-none-any.whl.metadata (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting azure-graphrbac<1.0.0,>=0.40.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_graphrbac-0.61.2-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-common<2.0.0,>=1.1.12 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting msrest<=0.7.1,>=0.5.1 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading msrest-0.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting msrestazure<=0.7,>=0.4.33 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading msrestazure-0.6.4.post1-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: urllib3<3.0.0,>1.26.17 in /usr/local/lib/python3.11/dist-packages (from azureml-core->-r requirements.txt (line 8)) (2.4.0)\n",
            "Collecting ndg-httpsclient<=0.5.1 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading ndg_httpsclient-0.5.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: SecretStorage<4.0.0 in /usr/local/lib/python3.11/dist-packages (from azureml-core->-r requirements.txt (line 8)) (3.3.3)\n",
            "Requirement already satisfied: jsonpickle<5.0.0 in /usr/local/lib/python3.11/dist-packages (from azureml-core->-r requirements.txt (line 8)) (4.0.5)\n",
            "Collecting contextlib2<22.0.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting docker<8.0.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: PyJWT<3.0.0 in /usr/local/lib/python3.11/dist-packages (from azureml-core->-r requirements.txt (line 8)) (2.10.1)\n",
            "Collecting adal<=1.2.7,>=1.2.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading adal-1.2.7-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: pyopenssl<26.0.0 in /usr/local/lib/python3.11/dist-packages (from azureml-core->-r requirements.txt (line 8)) (24.2.1)\n",
            "Collecting jmespath<2.0.0 (from azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: cryptography>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from adal<=1.2.7,>=1.2.0->azureml-core->-r requirements.txt (line 8)) (43.0.3)\n",
            "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from azure-core<2.0.0->azureml-core->-r requirements.txt (line 8)) (1.17.0)\n",
            "Collecting isodate<1.0.0,>=0.6.1 (from azure-mgmt-authorization<5,>=0.40.0->azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-mgmt-core<2.0.0,>=1.3.2 (from azure-mgmt-authorization<5,>=0.40.0->azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading azure_mgmt_core-1.5.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from knack<0.13.0->azureml-core->-r requirements.txt (line 8)) (2.19.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from knack<0.13.0->azureml-core->-r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from msrest<=0.7.1,>=0.5.1->azureml-core->-r requirements.txt (line 8)) (2025.4.26)\n",
            "Requirement already satisfied: requests-oauthlib>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from msrest<=0.7.1,>=0.5.1->azureml-core->-r requirements.txt (line 8)) (2.0.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from ndg-httpsclient<=0.5.1->azureml-core->-r requirements.txt (line 8)) (0.6.1)\n",
            "Collecting bcrypt>=3.2 (from paramiko<4.0.0,>=2.0.8->azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Collecting pynacl>=1.5 (from paramiko<4.0.0,>=2.0.8->azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]<3.0.0,>=2.19.1->azureml-core->-r requirements.txt (line 8)) (1.7.1)\n",
            "Requirement already satisfied: jeepney>=0.6 in /usr/local/lib/python3.11/dist-packages (from SecretStorage<4.0.0->azureml-core->-r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (2.0.0)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (2.2.0)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx<9,>=6->sphinx_rtd_theme->-r requirements.txt (line 7)) (3.1.0)\n",
            "Collecting backports.weakref (from backports.tempfile->azureml-core->-r requirements.txt (line 8))\n",
            "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=1.1.0->adal<=1.2.7,>=1.2.0->azureml-core->-r requirements.txt (line 8)) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision->-r requirements.txt (line 5)) (3.0.2)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.5.0->msrest<=0.7.1,>=0.5.1->azureml-core->-r requirements.txt (line 8)) (3.2.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=1.1.0->adal<=1.2.7,>=1.2.0->azureml-core->-r requirements.txt (line 8)) (2.22)\n",
            "Downloading sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azureml_core-1.60.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Cerberus-1.3.7-py3-none-any.whl (30 kB)\n",
            "Downloading adal-1.2.7-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading argcomplete-3.6.2-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_core-1.34.0-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.4/207.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_graphrbac-0.61.2-py2.py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_authorization-4.0.0-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_containerregistry-10.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_keyvault-10.3.1-py3-none-any.whl (901 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m901.4/901.4 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_network-28.1.0-py3-none-any.whl (575 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m575.3/575.3 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_resource-23.3.0-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading azure_mgmt_storage-22.2.0-py3-none-any.whl (569 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.5/569.5 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading knack-0.12.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal-1.32.3-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.4/115.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Downloading msrest-0.7.1-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading msrestazure-0.6.4.post1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ndg_httpsclient-0.5.1-py3-none-any.whl (34 kB)\n",
            "Downloading paramiko-3.5.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports.tempfile-1.0-py2.py3-none-any.whl (4.4 kB)\n",
            "Downloading pkginfo-1.12.1.2-py3-none-any.whl (32 kB)\n",
            "Downloading azure_mgmt_core-1.5.0-py3-none-any.whl (30 kB)\n",
            "Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backports.weakref-1.0.post1-py2.py3-none-any.whl (5.2 kB)\n",
            "Building wheels for collected packages: mpi4py, wget\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.3-cp311-cp311-linux_x86_64.whl size=4438169 sha256=a064d6a480937bb914a93c490cbb7ea0350a2ef0d011400c9b3b6041d954f127\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/56/17/bf6ba37aa971a191a8b9eaa188bf5ec855b8911c1c56fb1f84\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=08652a049d75ce643b4f287d0854ed4e40f3f730bed45f894634adf710867980\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built mpi4py wget\n",
            "Installing collected packages: wget, backports.weakref, azure-common, pkginfo, pathspec, mpi4py, jmespath, isodate, humanfriendly, contextlib2, cerberus, bcrypt, backports.tempfile, argcomplete, pynacl, knack, docker, azure-core, sphinxcontrib-jquery, paramiko, msrest, azure-mgmt-core, adal, sphinx_rtd_theme, ndg-httpsclient, msrestazure, msal, azure-mgmt-storage, azure-mgmt-resource, azure-mgmt-network, azure-mgmt-keyvault, azure-mgmt-containerregistry, azure-mgmt-authorization, msal-extensions, azure-graphrbac, azureml-core\n",
            "Successfully installed adal-1.2.7 argcomplete-3.6.2 azure-common-1.1.28 azure-core-1.34.0 azure-graphrbac-0.61.2 azure-mgmt-authorization-4.0.0 azure-mgmt-containerregistry-10.3.0 azure-mgmt-core-1.5.0 azure-mgmt-keyvault-10.3.1 azure-mgmt-network-28.1.0 azure-mgmt-resource-23.3.0 azure-mgmt-storage-22.2.0 azureml-core-1.60.0 backports.tempfile-1.0 backports.weakref-1.0.post1 bcrypt-4.3.0 cerberus-1.3.7 contextlib2-21.6.0 docker-7.1.0 humanfriendly-10.0 isodate-0.7.2 jmespath-1.0.1 knack-0.12.0 mpi4py-4.0.3 msal-1.32.3 msal-extensions-1.3.1 msrest-0.7.1 msrestazure-0.6.4.post1 ndg-httpsclient-0.5.1 paramiko-3.5.1 pathspec-0.12.1 pkginfo-1.12.1.2 pynacl-1.5.0 sphinx_rtd_theme-3.0.2 sphinxcontrib-jquery-4.1 wget-3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "backports"
                ]
              },
              "id": "c456b1acb5fd42338b42245dff55c4b1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets accelerate kagglehub[hf-datasets]"
      ],
      "metadata": {
        "id": "ahfpAJ4UhYhc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71bc5121-d559-46c3-eaa4-43659fcd97d6",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: kagglehub[hf-datasets] in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "print(f\"NumPy version: {numpy.__version__}\")\n"
      ],
      "metadata": {
        "id": "xSDl1mRVjAOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "047cb688-d737-4bfd-d049-978224591b2e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torchvision\n",
        "import sys\n",
        "import peft\n",
        "import numpy\n",
        "import jax # Import jax explicitly\n",
        "import jaxlib # Import jaxlib directly\n",
        "import datasets # Add other key libraries to check\n",
        "import accelerate\n",
        "import kagglehub\n",
        "import tensorflow # Import tensorflow\n",
        "# Remove azureml imports as they are no longer installed\n",
        "# import azureml.core\n",
        "# import azureml.defaults\n",
        "import sklearn # Import scikit-learn\n",
        "import scipy # Import scipy\n",
        "import h5py # Import h5py\n",
        "\n",
        "\n",
        "print(f\"NumPy version: {numpy.__version__}\")\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"PEFT version: {peft.__version__}\")\n",
        "print(f\"JAX version: {jax.__version__}\")\n",
        "print(f\"JAXlib version: {jaxlib.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "print(f\"KaggleHub version: {kagglehub.__version__}\")\n",
        "print(f\"Tensorflow version: {tensorflow.__version__}\") # Print tensorflow version\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\") # Print scikit-learn version\n",
        "print(f\"SciPy version: {scipy.__version__}\") # Print scipy version\n",
        "print(f\"h5py version: {h5py.__version__}\") # Print h5py version\n",
        "\n",
        "\n",
        "print(\"\\nFinal environment setup check.\")\n"
      ],
      "metadata": {
        "id": "sXtI9QN4eSot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43541181-ef14-48d6-fcd0-2ed2e395c9da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy version: 1.26.4\n",
            "Torch version: 2.6.0+cu124\n",
            "Transformers version: 4.40.2\n",
            "PEFT version: 0.7.0\n",
            "JAX version: 0.4.25\n",
            "JAXlib version: 0.4.25\n",
            "Datasets version: 2.14.4\n",
            "Accelerate version: 1.6.0\n",
            "KaggleHub version: 0.3.12\n",
            "Tensorflow version: 2.15.0\n",
            "Scikit-learn version: 1.6.1\n",
            "SciPy version: 1.15.3\n",
            "h5py version: 3.13.0\n",
            "\n",
            "Final environment setup check.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import transformers\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "import jax\n",
        "import datasets\n",
        "import accelerate\n",
        "import peft\n",
        "import h5py\n",
        "import scipy\n",
        "\n",
        "print(\"✅ All imports successful.\")\n"
      ],
      "metadata": {
        "id": "CClW-yWoleox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad9a8ead-e475-48e3-96dc-f6ccd0165c00"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All imports successful.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"sentiment-analysis\")\n",
        "print(pipe(\"I love how smooth this setup went!\"))\n"
      ],
      "metadata": {
        "id": "KcccjjMUlg0S",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORTING MODEL"
      ],
      "metadata": {
        "id": "eDRsFBfXusxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7i03t45pumJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from transformers import AutoConfig\n",
        "\n",
        "\n",
        "# # Replace this with the path of the directory you want to list contents from\n",
        "# directory_to_list = \"/content/drive/My Drive/clinical_peft_model\"\n",
        "\n",
        "# print(f\"Listing all contents in: {directory_to_list}\")\n",
        "\n",
        "# # Check if the directory exists\n",
        "# if os.path.exists(directory_to_list):\n",
        "#     print(\"Directory exists. Contents:\")\n",
        "#     # List all items (files and folders) in the directory\n",
        "#     contents = os.listdir(directory_to_list)\n",
        "#     if contents:\n",
        "#         for item in contents:\n",
        "#             print(f\"- {item}\")\n",
        "#     else:\n",
        "#         print(\"The directory is empty.\")\n",
        "# else:\n",
        "#     print(f\"Directory does NOT exist: {directory_to_list}\")"
      ],
      "metadata": {
        "id": "fHjwlhvAvBeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOADING DATASETS"
      ],
      "metadata": {
        "id": "8ZCgRSE5wht4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import kagglehub\n",
        "# from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# file_path = \"Symptom2Disease.csv\"\n",
        "\n",
        "# Symptom2Disease = kagglehub.load_dataset(\n",
        "#   KaggleDatasetAdapter.HUGGING_FACE,\n",
        "#   \"niyarrbarman/symptom2disease\",\n",
        "#   file_path\n",
        "# )\n",
        "\n",
        "# print(\"Hugging Face Dataset:\", Symptom2Disease)\n",
        "\n",
        "\n",
        "# unique_labels_Symptom2Disease = sorted(Symptom2Disease.unique(\"label\"))\n",
        "# num_unique_Symptom2Disease = len(unique_labels_Symptom2Disease)\n",
        "\n",
        "# print(f\"Number of unique labels in the first dataset: {unique_labels_Symptom2Disease}\")\n",
        "# print(f\"Unique labels in the first dataset: {num_unique_Symptom2Disease}\")\n",
        "# print(Symptom2Disease.column_names)"
      ],
      "metadata": {
        "id": "giGFCJLcvFzl",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming 'hf_dataset' is your loaded Hugging Face Dataset\n",
        "# first_5_rows = Symptom2Disease[:5]\n",
        "# print(first_5_rows)"
      ],
      "metadata": {
        "id": "8FSemLCbwu97",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #PRINTING 1ST ROW\n",
        "# first_row = Symptom2Disease[:1]\n",
        "# print(first_row)"
      ],
      "metadata": {
        "id": "df2MwPh3wyKq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #PRINTING AND COUNTING DISTINCT DISEASES\n",
        "# unique_labels_Symptom2Disease = sorted(Symptom2Disease.unique(\"label\"))\n",
        "# for label in unique_labels_Symptom2Disease:\n",
        "#     print(label)\n",
        "# print(len(unique_labels_Symptom2Disease))"
      ],
      "metadata": {
        "id": "hf8H0k7Ww1E5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #PRINTING NUMBER OF ROWS\n",
        "# def print_column_values(dataset, column_name):\n",
        "#     unique_values = sorted(dataset.unique(column_name))\n",
        "#     print(f\"Unique values in column '{column_name}':\")\n",
        "#     for value in unique_values:\n",
        "#         print(f\"- {value}\")\n",
        "\n",
        "# Symptom2Disease_all_labels = Symptom2Disease.sort(\"label\")\n",
        "# print_column_values(Symptom2Disease_all_labels, \"label\")\n",
        "# print(len(Symptom2Disease_all_labels))"
      ],
      "metadata": {
        "id": "Cu0dy4Asw1qF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install dependencies as needed:\n",
        "# # pip install kagglehub[hf-datasets]\n",
        "# import kagglehub\n",
        "# from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# # Set the path to the file you'd like to load\n",
        "# file_path = \"Disease_symptom_and_patient_profile_dataset.csv\"\n",
        "\n",
        "# # Load the latest version\n",
        "# disease_symptom_and_patient_profile = kagglehub.load_dataset(\n",
        "#   KaggleDatasetAdapter.HUGGING_FACE,\n",
        "#   \"uom190346a/disease-symptoms-and-patient-profile-dataset\",\n",
        "#   file_path\n",
        "# )\n",
        "\n",
        "# print(\"Hugging Face Dataset:\", disease_symptom_and_patient_profile)\n",
        "\n",
        "# unique_diseases_dataset_patient_profile = disease_symptom_and_patient_profile.unique(\"Disease\")\n",
        "# num_unique_diseases_dataset_patient_profile = len(unique_diseases_dataset_patient_profile)\n",
        "\n",
        "# print(f\"Number of unique diseases in the second dataset: {num_unique_diseases_dataset_patient_profile}\")\n",
        "# print(f\"Unique diseases in the second dataset: {unique_diseases_dataset_patient_profile}\")\n",
        "# print(disease_symptom_and_patient_profile.column_names)"
      ],
      "metadata": {
        "id": "pmA35gt2w4Iu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming 'hf_dataset' is your loaded Hugging Face Dataset\n",
        "# first_5_rows = disease_symptom_and_patient_profile[:5]\n",
        "# print(first_5_rows)"
      ],
      "metadata": {
        "id": "yDoxj72Dw7sx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Assuming 'hf_dataset' is your loaded Hugging Face Dataset\n",
        "# first_row = disease_symptom_and_patient_profile[:1]\n",
        "# print(first_row)"
      ],
      "metadata": {
        "id": "pWdkUtwWw-P5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_labels_patient_profile = sorted(disease_symptom_and_patient_profile.unique(\"Disease\"))\n",
        "# for label in unique_labels_patient_profile:\n",
        "#     print(label)\n",
        "\n",
        "# print(len(unique_labels_patient_profile))"
      ],
      "metadata": {
        "id": "9o46c2EAxAwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #TO CHECK OVERLAPPING DISEASES\n",
        "# # Convert the lists to sets\n",
        "# set1 = set(unique_labels_patient_profile)\n",
        "# set2 = set(unique_labels_Symptom2Disease)\n",
        "\n",
        "# overlapping_diseases = set1.intersection(set2)\n",
        "\n",
        "# # Print the overlapping diseases\n",
        "# print(\"Overlapping diseases:\", overlapping_diseases)\n",
        "\n",
        "# # Print the number of overlapping diseases\n",
        "# print(\"Number of overlapping diseases:\", len(overlapping_diseases))"
      ],
      "metadata": {
        "id": "ERZAlkxnxDJy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Disease_symptom_and_patient_profile_dataset processing\n",
        "# # we made columns be in one text and modified name of column disease to label to be the same as the other dataset\n",
        "\n",
        "# symptom_cols_2 = ['Fever', 'Cough', 'Fatigue', 'Difficulty Breathing']  # <--- Explicitly list your symptom columns\n",
        "# additional_cols_to_keep = ['Age', 'Gender', 'Blood Pressure','Cholesterol Level'] # <--- Add other relevant columns here\n",
        "\n",
        "# def create_text_column_2(examples):\n",
        "#     texts = []\n",
        "#     for i in range(len(examples['Disease'])):\n",
        "#         symptoms = []\n",
        "#         for col in symptom_cols_2:\n",
        "#             if examples[col][i].lower() == 'yes':\n",
        "#                 symptoms.append(col)\n",
        "#         texts.append(', '.join(symptoms))\n",
        "#     return {'text': texts}\n",
        "\n",
        "# patient_profile_processed = disease_symptom_and_patient_profile.map(create_text_column_2, batched=True)\n",
        "\n",
        "# # Rename the 'Disease' column to 'label'\n",
        "# patient_profile_processed = patient_profile_processed.rename_column(\"Disease\", \"label\")\n",
        "\n",
        "# def add_features_to_text(example):\n",
        "#     text = example['text']\n",
        "#     age = example.get('Age', 'N/A')\n",
        "#     gender = example.get('Gender', 'N/A')\n",
        "#     blood_pressure = example.get('Blood Pressure', 'N/A')\n",
        "#     cholesterol_level = example.get('Cholesterol Level', 'N/A')\n",
        "\n",
        "#     full_text = f\"Symptoms: {text}. Age: {age}, Gender: {gender}, Blood Pressure: {blood_pressure}, Cholesterol Level: {cholesterol_level}.\"\n",
        "#     return {'text': full_text}\n",
        "\n",
        "# patient_profile_processed = patient_profile_processed.map(add_features_to_text)\n",
        "\n",
        "# # Now, select only the 'text' and 'label' columns to remove the others\n",
        "# patient_profile_processed = patient_profile_processed.select_columns([\"text\", \"label\"])\n",
        "\n",
        "# print(\"Processed disease_symptom_and_patient_profile with full text (first 5 rows):\")\n",
        "# for i in range(min(5, len(patient_profile_processed))):\n",
        "#     print(patient_profile_processed[i])\n",
        "\n",
        "# patient_profile_random = patient_profile_processed.shuffle() # You can use any integer as the seed for reproducibility\n",
        "# print(\"Shuffled hf_dataset2_processed with full text (first 5 rows):\")\n",
        "# for i in range(min(5, len(patient_profile_random))):\n",
        "#     print(patient_profile_random[i])\n",
        "\n",
        "# print(\"Columns in patient_profile_processed:\", patient_profile_processed.column_names)"
      ],
      "metadata": {
        "id": "myw7T-56xFxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Symptom2Disease Dataset\n",
        "\n",
        "# Symptom2Disease_processed = Symptom2Disease.select_columns([\"text\", \"label\"])\n",
        "# print(\"Processed Symptom2Disease Dataset (first 5 rows):\")\n",
        "# for i in range(min(5, len(Symptom2Disease_processed))):\n",
        "#     print(Symptom2Disease_processed[i])\n",
        "\n",
        "# print(\"-\" * 30)\n",
        "\n",
        "# Symptom2Disease_processed_random = Symptom2Disease_processed.shuffle()\n",
        "# print(\"Shuffled Symptom2Disease_processed_random (first 5 rows):\")\n",
        "# for i in range(min(5, len(Symptom2Disease_processed_random))):\n",
        "#     print(Symptom2Disease_processed_random[i])\n",
        "\n",
        "# print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "WMQGOHbxxJXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import concatenate_datasets\n",
        "\n",
        "# # Make sure you have run the processing for hf_dataset2 and have it in hf_dataset2_processed\n",
        "\n",
        "# if 'Symptom2Disease_processed_random' in locals() and 'patient_profile_random' in locals():\n",
        "#     merged_dataset = concatenate_datasets([Symptom2Disease_processed_random, patient_profile_random])\n",
        "#     # Assuming 'your_dataset' is the name of your Dataset object\n",
        "\n",
        "#     print(merged_dataset.column_names)\n",
        "\n",
        "#     print(\"Merged Dataset (first 10 rows):\")\n",
        "#     for i in range(min(10, len(merged_dataset))):\n",
        "#         print(merged_dataset[i])\n",
        "\n",
        "#     print(\"\\nNumber of samples in the merged dataset:\", len(merged_dataset))\n",
        "\n",
        "#     # It's often a good idea to shuffle the merged dataset as well\n",
        "#     merged_dataset_shuffled = merged_dataset.shuffle()\n",
        "#     print(\"\\nShuffled Merged Dataset (first 10 rows):\")\n",
        "#     for i in range(min(10, len(merged_dataset_shuffled))):\n",
        "#         print(merged_dataset_shuffled[i])\n",
        "# else:\n",
        "#     print(\"Make sure both hf_dataset_random and hf_dataset2_random are defined.\")"
      ],
      "metadata": {
        "id": "xJHKcZnVxLlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def format_prompt_completion(example):\n",
        "#     prompt = f\"Symptoms: {example['text']}. Diagnosis:\"\n",
        "#     completion = f\" {example['label']}\"\n",
        "#     return {\"prompt\": prompt, \"completion\": completion}\n",
        "\n",
        "# formatted_dataset = merged_dataset_shuffled.map(format_prompt_completion)\n",
        "\n",
        "# # Let's see the first few examples of the formatted dataset, printing each row\n",
        "# print(\"Formatted Dataset (first 5 rows):\")\n",
        "# for i in range(min(5, len(formatted_dataset))):\n",
        "#     print(f\"Row {i+1}:\")\n",
        "#     print(f\"  Prompt: {formatted_dataset[i]['prompt']}\")\n",
        "#     print(f\"  Completion: {formatted_dataset[i]['completion']}\")\n",
        "#     print(\"-\" * 20)"
      ],
      "metadata": {
        "id": "zmJwNEQDxPtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# from datasets import Dataset # Import Dataset if not already imported\n",
        "\n",
        "# # Assume 'formatted_dataset' is your Hugging Face Dataset object\n",
        "# # Make sure this variable is defined and contains your dataset\n",
        "\n",
        "# # --- Step 1: Save the formatted_dataset to local files ---\n",
        "\n",
        "# # Define the local directory where you want to save the dataset files temporarily\n",
        "# # This will be a temporary directory in your Colab environment\n",
        "# local_save_directory = '/content/formatted_dataset_files/'\n",
        "\n",
        "# # Create the local directory if it doesn't exist\n",
        "# os.makedirs(local_save_directory, exist_ok=True)\n",
        "\n",
        "# print(f\"Saving formatted dataset to local directory: {local_save_directory}\")\n",
        "\n",
        "# # Save the formatted_dataset to JSON Lines files in the specified local directory\n",
        "# # The datasets library will handle splitting into multiple files if needed\n",
        "# formatted_dataset.to_json(\n",
        "#     os.path.join(local_save_directory, 'formatted_dataset.jsonl'), # Define the base filename\n",
        "#     num_proc=os.cpu_count() # Use multiple processes for faster saving if available\n",
        "# )\n",
        "\n",
        "# print(\"Local dataset saving complete.\")\n",
        "# # You can list the files saved locally:\n",
        "# # !ls {local_save_directory}\n",
        "\n",
        "# # --- Step 2: Copy the local files to Google Drive ---\n",
        "\n",
        "# # Define the path in your Google Drive where you want to save the dataset persistently\n",
        "# # Make sure Google Drive is mounted before running this step\n",
        "# drive_data_path = '/content/drive/My Drive/my_flute_formatted_dataset/' # Customize this path\n",
        "\n",
        "# # Create the directory in Google Drive if it doesn't exist\n",
        "# os.makedirs(drive_data_path, exist_ok=True)\n",
        "\n",
        "# print(f\"Copying local dataset files to Google Drive at: {drive_data_path}\")\n",
        "\n",
        "# # Copy files from the local temporary path to the Google Drive path\n",
        "# if os.path.exists(local_save_directory):\n",
        "#     for item_name in os.listdir(local_save_directory):\n",
        "#         local_item_path = os.path.join(local_save_directory, item_name)\n",
        "#         drive_item_path = os.path.join(drive_data_path, item_name)\n",
        "#         if os.path.isdir(local_item_path):\n",
        "#             shutil.copytree(local_item_path, drive_item_path, symlinks=True)\n",
        "#         else:\n",
        "#             shutil.copy2(local_item_path, drive_item_path)\n",
        "#     print(f\"Dataset files successfully copied to Google Drive.\")\n",
        "# else:\n",
        "#     print(f\"Local data path not found: {local_save_directory}\")\n",
        "\n",
        "# # After copying, you can verify by listing files in the Drive folder\n",
        "# # !ls \"{drive_data_path}\" # Use quotes if the path has spaces\n"
      ],
      "metadata": {
        "id": "xgoPYDexxTn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORTING THE DATASET"
      ],
      "metadata": {
        "id": "duJnEH5kp7UB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from google.colab import drive\n",
        "# from datasets import load_dataset, Dataset # Import necessary functions/classes\n",
        "\n",
        "# drive_data_path = '/content/drive/My Drive/my_flute_formatted_dataset/'\n",
        "# print(f\"Looking for dataset files in Google Drive at: {drive_data_path}\")\n",
        "\n",
        "# try:\n",
        "#     loaded_dataset = load_dataset('json', data_files=os.path.join(drive_data_path, '*.jsonl'))\n",
        "#     if isinstance(loaded_dataset, dict):\n",
        "\n",
        "#         if 'train' in loaded_dataset:\n",
        "#             loaded_dataset = loaded_dataset['train']\n",
        "#             print(\"Dataset loaded successfully from Google Drive (accessed 'train' split).\")\n",
        "#         else:\n",
        "#             print(\"Warning: Dataset loaded but 'train' split not found. Inspect loaded_dataset.\")\n",
        "#             print(loaded_dataset)\n",
        "\n",
        "#     else:\n",
        "#          print(\"Dataset loaded successfully from Google Drive.\")\n",
        "\n",
        "#     if loaded_dataset is not None:\n",
        "#         print(\"\\nFirst 5 samples of the loaded dataset:\")\n",
        "#         for i in range(min(5, len(loaded_dataset))):\n",
        "#             print(f\"Sample {i+1}: {loaded_dataset[i]}\")\n",
        "#         print(\"-\" * 20)\n",
        "\n",
        "# except FileNotFoundError:\n",
        "#     print(f\"Error: The directory {drive_data_path} was not found.\")\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred while loading the dataset: {e}\")"
      ],
      "metadata": {
        "id": "oT6OvTH21m_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FLUTE PREPARATION"
      ],
      "metadata": {
        "id": "bgTOthIorWT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import shutil\n",
        "# from datasets import Dataset\n",
        "\n",
        "\n",
        "# num_artificial_users = 6\n",
        "# local_save_directory = '/content/artificial_users_datasets/'\n",
        "# os.makedirs(local_save_directory, exist_ok=True)\n",
        "\n",
        "# print(f\"Splitting dataset into {num_artificial_users} artificial users and saving locally...\")\n",
        "# total_samples = len(loaded_dataset)\n",
        "# print(f\"Total samples in the dataset: {total_samples}\")\n",
        "\n",
        "# samples_per_shard = total_samples // num_artificial_users\n",
        "# print(f\"Approximately {samples_per_shard} samples per user.\")\n",
        "# for i in range(num_artificial_users):\n",
        "#     user_dataset = loaded_dataset.shard(index=i, num_shards=num_artificial_users)\n",
        "#     user_filename = f\"user_{i}.jsonl\"\n",
        "#     user_filepath = os.path.join(local_save_directory, user_filename)\n",
        "#     print(f\"Saving data for user {i} ({len(user_dataset)} samples) to {user_filepath}\")\n",
        "#     user_dataset.to_json(user_filepath)\n",
        "\n",
        "# print(\"\\nArtificial user datasets saved locally.\")\n",
        "# print(f\"Check the directory: {local_save_directory}\")"
      ],
      "metadata": {
        "id": "ZVecpKv8rZTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drive_data_path = '/content/drive/My Drive/my_flute_formatted_datasets_users/' # Customize this path\n",
        "\n",
        "# # Create the directory in Google Drive if it doesn't exist\n",
        "# os.makedirs(drive_data_path, exist_ok=True)\n",
        "\n",
        "# print(f\"\\nCopying local user files to Google Drive at: {drive_data_path}\")\n",
        "# if os.path.exists(local_save_directory):\n",
        "#     for item_name in os.listdir(local_save_directory):\n",
        "#         local_item_path = os.path.join(local_save_directory, item_name)\n",
        "#         drive_item_path = os.path.join(drive_data_path, item_name)\n",
        "#         # Use copy2 to preserve metadata, or copy if metadata isn't critical\n",
        "#         shutil.copy2(local_item_path, drive_item_path)\n",
        "#     print(f\"User dataset files successfully copied to Google Drive.\")\n",
        "# else:\n",
        "#     print(f\"Local data path not found: {local_save_directory}\")\n"
      ],
      "metadata": {
        "id": "_u6CeKwqrzrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Folder with code\n"
      ],
      "metadata": {
        "id": "RgmbguCeSvIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to run in a Google Colab cell to test BachelorDataset and BachelorDataLoader\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import unittest\n",
        "import torch\n",
        "import logging\n",
        "import sys\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "\n",
        "# Import necessary components for BaseDataset and BaseModel\n",
        "from abc import ABC, abstractmethod\n",
        "from torch.utils.data import Dataset as PyTorchDataset\n",
        "from torch.utils.data import DataLoader as PyTorchDataLoader\n",
        "# Import Samplers explicitly for use within the cell\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "import torch.nn as T_nn # Using T_nn to avoid conflict if torch is imported as T\n",
        "\n",
        "# --- Set up Python Path for Imports ---\n",
        "# Add your project root directory to the Python path\n",
        "# Assuming your Colab notebook is in the project root, or you adjust this path\n",
        "# If your project root is, for example, '/content/drive/MyDrive/YourBachelorProject',\n",
        "# you would set PROJECT_ROOT = '/content/drive/drive/MyDrive/YourBachelorProject' # Corrected path structure\n",
        "PROJECT_ROOT = './' # Adjust this path if your notebook is not in the project root\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# Now try importing your classes and dependencies\n",
        "try:\n",
        "    # Import from Hugging Face transformers\n",
        "    from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "    # Import the dummy print_rank if available, or use the one defined below\n",
        "    try:\n",
        "        # Adjust this import path if your utils.py is elsewhere\n",
        "        from experiments.bachelor.utils import print_rank\n",
        "    except ImportError:\n",
        "        # Dummy print_rank if utils.py or print_rank is not found\n",
        "        def print_rank(msg, loglevel=logging.INFO):\n",
        "            level_map = {logging.DEBUG: 'DEBUG', logging.INFO: 'INFO', logging.WARNING: 'WARNING', logging.ERROR: 'ERROR'}\n",
        "            print(f\"[{level_map.get(loglevel, 'INFO')}] {msg}\")\n",
        "            # logging.log(loglevel, msg) # Avoids double logging if root logger is configured\n",
        "\n",
        "    # Import your custom classes (these will be defined below in this cell)\n",
        "    # We still list them here for clarity of what the cell contains\n",
        "    # from experiments.bachelor.dataloaders.dataset import BachelorDataset # Defined below\n",
        "    # from experiments.bachelor.dataloaders.dataloader import BachelorDataLoader # Defined below\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"Error importing necessary libraries (transformers, utils).\")\n",
        "    print(f\"Please ensure they are installed and accessible.\")\n",
        "    print(f\"ImportError: {e}\")\n",
        "    # You might want to stop execution here if imports fail\n",
        "    raise e # Re-raise the exception to stop the cell execution\n",
        "\n",
        "\n",
        "# --- FLUTE Base Classes Definitions ---\n",
        "# Include the definitions of the Base classes here so they are available\n",
        "# before your custom classes inherit from them.\n",
        "\n",
        "class BaseDataset(ABC, PyTorchDataset):\n",
        "    '''This is a wrapper class for PyTorch datasets.'''\n",
        "\n",
        "    @abstractmethod\n",
        "    def __init__(self,**kwargs):\n",
        "        super(BaseDataset, self).__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def __getitem__(self, idx, **kwargs):\n",
        "        '''Fetches a data sample for a given key'''\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def __len__(self):\n",
        "        '''Returns the size of the dataset'''\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_data(self,**kwargs):\n",
        "        '''Wrapper method to read/instantiate the dataset'''\n",
        "        pass\n",
        "\n",
        "class BaseDataLoader(ABC, PyTorchDataLoader):\n",
        "    '''This is a wrapper class for PyTorch dataloaders.'''\n",
        "\n",
        "    def create_loader(self):\n",
        "        '''Returns the dataloader'''\n",
        "        return self\n",
        "\n",
        "\n",
        "# --- Dummy Data Setup ---\n",
        "# Define the directory structure simulating the drive location\n",
        "# IMPORTANT: If you are mounting Google Drive, the path will likely start with '/content/drive/MyDrive/'\n",
        "# Make sure PROJECT_ROOT is set correctly above if your notebook is not in the project root.\n",
        "DUMMY_DRIVE_DATA_DIR = os.path.join(PROJECT_ROOT, \"simulated_drive_data\")\n",
        "TRAIN_DATA_DIR = os.path.join(DUMMY_DRIVE_DATA_DIR, \"train_clients\") # Directory for client train files\n",
        "VAL_DATA_DIR = os.path.join(DUMMY_DRIVE_DATA_DIR, \"val_aggregated\") # Directory for aggregated val file\n",
        "TEST_DATA_DIR = os.path.join(DUMMY_DRIVE_DATA_DIR, \"test_aggregated\") # Directory for aggregated test file\n",
        "\n",
        "\n",
        "# Dummy data following the JSONL format you provided\n",
        "# Use a few different labels to test mapping\n",
        "# Ensure some clients have more than 3 samples\n",
        "DUMMY_CLIENT_DATA = {\n",
        "    \"client_0\": [\n",
        "        {\"text\": \"Symptoms: Fever, Cough, Fatigue.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Fever, Cough, Fatigue.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "        {\"text\": \"Extreme itchiness, nausea, and fatigue.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Extreme itchiness, nausea, and fatigue.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "        {\"text\": \"Feeling really tired and weak lately.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Feeling really tired and weak lately.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "        {\"text\": \"Another symptom description.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Another symptom description.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "        {\"text\": \"Fifth symptom entry.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Fifth symptom entry.. Diagnosis:\", \"completion\": \" Jaundice\"}, # More than 3\n",
        "    ],\n",
        "    \"client_1\": [\n",
        "        {\"text\": \"Shortness of breath and chest pain.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Shortness of breath and chest pain.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "        {\"text\": \"Yellow skin and eyes.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Yellow skin and eyes.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "        {\"text\": \"Third symptom for client 1.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Third symptom for client 1.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "        {\"text\": \"Fourth symptom for client 1.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Fourth symptom for client 1.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"}, # More than 3\n",
        "    ],\n",
        "    \"client_2\": [\n",
        "        {\"text\": \"High blood pressure readings.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: High blood pressure readings.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "        {\"text\": \"Feeling unwell.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Feeling unwell.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "        {\"text\": \"Coughing a lot.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Coughing a lot.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "    ],\n",
        "    \"client_3\": [\n",
        "         {\"text\": \"Headache and dizziness.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Headache and dizziness.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "         {\"text\": \"Only two samples here.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Only two samples here.. Diagnosis:\", \"completion\": \" Jaundice\"}, # Less than 3\n",
        "    ],\n",
        "    \"client_4\": [\n",
        "         {\"text\": \"Loss of appetite.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Loss of appetite.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "         {\"text\": \"Wheezing sound.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Wheezing sound.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "         {\"text\": \"Third sample for client 4.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Third sample for client 4.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "         {\"text\": \"Fourth sample for client 4.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Fourth sample for client 4.. Diagnosis:\", \"completion\": \" Jaundice\"}, # More than 3\n",
        "    ],\n",
        "    \"client_5\": [\n",
        "         {\"text\": \"Persistent cough.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Persistent cough.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "         {\"text\": \"Nausea and vomiting.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Nausea and vomiting.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "         {\"text\": \"Chest tightness.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Chest tightness.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "         {\"text\": \"Sample 4.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Sample 4.. Diagnosis:\", \"completion\": \" Hypertension\"}, # More than 3\n",
        "         {\"text\": \"Sample 5.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Sample 5.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "         {\"text\": \"Sample 6.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Sample 6.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Dummy data for validation/testing (aggregated)\n",
        "# This data simulates a single file containing eval data, not split by client\n",
        "DUMMY_AGGREGATED_EVAL_DATA = [\n",
        "    {\"text\": \"Evaluation symptom 1.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Evaluation symptom 1.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "    {\"text\": \"Evaluation symptom 2.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Evaluation symptom 2.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "    {\"text\": \"Evaluation symptom 3.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Evaluation symptom 3.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "    {\"text\": \"Evaluation symptom 4.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Evaluation symptom 4.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "]\n",
        "\n",
        "\n",
        "def create_dummy_client_data_files(base_dir, data_dict):\n",
        "    \"\"\"Creates dummy JSONL files for each client.\"\"\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    for client_id, data_entries in data_dict.items():\n",
        "        file_path = os.path.join(base_dir, f\"{client_id}.jsonl\")\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            for entry in data_entries:\n",
        "                f.write(json.dumps(entry) + '\\n')\n",
        "    print_rank(f\"Created dummy client data files in: {base_dir}\", loglevel=logging.INFO)\n",
        "\n",
        "def create_dummy_aggregated_data_file(base_dir, data_list, file_name=\"aggregated_eval_data.jsonl\"):\n",
        "    \"\"\"Creates a single dummy JSONL file for aggregated data.\"\"\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    file_path = os.path.join(base_dir, file_name)\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        for entry in data_list:\n",
        "            f.write(json.dumps(entry) + '\\n')\n",
        "    print_rank(f\"Created dummy aggregated data file in: {base_dir}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "def cleanup_dummy_data():\n",
        "    \"\"\"Removes the dummy data directory.\"\"\"\n",
        "    if os.path.exists(DUMMY_DRIVE_DATA_DIR):\n",
        "        shutil.rmtree(DUMMY_DRIVE_DATA_DIR)\n",
        "        print_rank(f\"Cleaned up dummy data directory: {DUMMY_DRIVE_DATA_DIR}\", loglevel=logging.INFO)\n",
        "\n",
        "# --- Corrected BachelorDataset Class Definition ---\n",
        "# This class definition is now placed AFTER the BaseDataset definition\n",
        "\n",
        "# experiments/bachelor/dataloaders/dataset.py\n",
        "\n",
        "class BachelorDataset(BaseDataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for the Bachelor classification task.\n",
        "    Loads data from JSONL files, tokenizes 'prompt' text, and maps string labels to integers.\n",
        "    Inherits from core.dataset.BaseDataset.\n",
        "    Handles loading either client-specific files or a single aggregated file.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_path, args, tokenizer=None, test_only=False, user_idx=0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the directory containing client JSONL files (for train)\n",
        "                             or the aggregated eval file (for val/test).\n",
        "                             This path will be provided via the config.yaml.\n",
        "            args (dict): Dictionary of arguments from config.yaml under the client_config or server_config\n",
        "                         data_config sections.\n",
        "            tokenizer (transformers.PreTrainedTokenizer, optional): Pre-trained tokenizer instance.\n",
        "                                                                     If None, it will be loaded from args.\n",
        "            test_only (bool, optional): True if this dataset instance is for evaluation/testing only (server-side).\n",
        "                                        Defaults to False (client-side training).\n",
        "            user_idx (int, optional): Index of the current user/client this dataset instance should load data for.\n",
        "                                      FLUTE provides this. -1 typically indicates aggregated data (e.g., server's test set).\n",
        "            **kwargs: Additional keyword arguments passed by FLUTE.\n",
        "        \"\"\"\n",
        "        # Initialize the BaseDataset (and thus PyTorchDataset)\n",
        "        super().__init__()\n",
        "\n",
        "        self.test_only = test_only\n",
        "        # Get max_seq_length from args, default to 512 if not specified\n",
        "        self.max_seq_length = args.get('max_seq_length', 512)\n",
        "        # Get padding preference from args, default to True\n",
        "        self.padding = args.get('padding', True)\n",
        "        self.user = None # Will store the current user ID processed by this dataset instance\n",
        "\n",
        "        # --- Tokenizer Initialization ---\n",
        "        # The tokenizer is essential for processing text data.\n",
        "        if tokenizer is not None:\n",
        "            self.tokenizer = tokenizer\n",
        "        else:\n",
        "            # Load tokenizer from Hugging Face based on config args\n",
        "            tokenizer_kwargs = {\n",
        "                \"cache_dir\": args.get('cache_dir', None), # Cache directory for downloaded models/tokenizers\n",
        "                \"use_fast\": args.get('tokenizer_type_fast', True), # Use faster Rust-based tokenizers if available\n",
        "                \"use_auth_token\": None # For accessing models requiring authentication\n",
        "            }\n",
        "            if 'tokenizer_name' in args and args['tokenizer_name']:\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(args['tokenizer_name'], **tokenizer_kwargs)\n",
        "            elif 'model_name_or_path' in args and args['model_name_or_path']:\n",
        "                 # Use model_name_or_path if tokenizer_name is not specified\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(args['model_name_or_path'], **tokenizer_kwargs)\n",
        "            else:\n",
        "                # Raise error if no tokenizer source is provided\n",
        "                raise ValueError(\"You must provide a tokenizer instance or specify 'tokenizer_name' or 'model_name_or_path' in args.\")\n",
        "\n",
        "        # Adjust max_seq_length based on the tokenizer's model_max_length\n",
        "        if self.max_seq_length > self.tokenizer.model_max_length:\n",
        "            print_rank(\n",
        "                f\"The max_seq_length passed ({self.max_seq_length}) is larger than the maximum length for the\"\n",
        "                f\"model ({self.tokenizer.model_max_length}). Using max_seq_length={self.tokenizer.model_max_length}.\", loglevel=logging.WARNING\n",
        "            )\n",
        "            self.max_seq_length = self.tokenizer.model_max_length\n",
        "\n",
        "        # --- FLUTE Required Attributes (populated in load_data) ---\n",
        "        # These attributes are part of the BaseDataset contract with FLUTE.\n",
        "        # They are typically populated with raw or initially processed data structure overview.\n",
        "        self.user_list = []       # List of all user/client IDs found in the data directory (for train)\n",
        "        self.num_samples = []     # List containing the total number of samples for each user (in order of user_list)\n",
        "        # user_data and user_data_labels will store the raw prompts and labels per user (for train)\n",
        "        # or aggregated raw data (for eval), matching the structure expected by BaseDataset.\n",
        "        self.user_data = defaultdict(list)  # Dictionary storing raw 'prompt' strings per user {user_id: [prompt1, prompt2, ...]}\n",
        "        self.user_data_labels = defaultdict(list) # Dictionary storing raw 'label' strings per user {user_id: [label1, label2, ...]}\n",
        "\n",
        "\n",
        "        # --- Internal Dataset State ---\n",
        "        # Global label mapping (string label to integer ID) built during data loading\n",
        "        self.label_map = {}\n",
        "        self.current_label_id = 0\n",
        "        self.all_labels = [] # List to store all unique string labels found for consistent mapping\n",
        "\n",
        "        # Internal list to hold the *processed* (tokenized and integer-labeled) samples\n",
        "        # for the *current* dataset instance (either a single client or aggregated eval data).\n",
        "        # This is the list that __getitem__ will iterate over.\n",
        "        self.utt_list = []\n",
        "\n",
        "        # --- Data Loading and Processing ---\n",
        "        # Call the abstract method load_data to perform the actual data loading and initial processing\n",
        "        self.load_data(data_path, user_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of processed samples in this dataset instance (self.utt_list).\n",
        "        This is required by PyTorch's Dataset class.\n",
        "        \"\"\"\n",
        "        return len(self.utt_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches a single processed data sample for a given index.\n",
        "        This is required by PyTorch's Dataset class and used by the DataLoader.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the tokenized input ('input_ids', 'attention_mask')\n",
        "                  and the integer label ('labels').\n",
        "        \"\"\"\n",
        "        # The data in self.utt_list is already tokenized and ready to be returned\n",
        "        return self.utt_list[idx]\n",
        "\n",
        "    def load_data(self, data_dir_path, user_idx):\n",
        "        \"\"\"\n",
        "        Reads the data from JSONL files.\n",
        "        If loading for clients, it finds client-specific files.\n",
        "        If loading aggregated eval data, it finds a single aggregated file.\n",
        "        Populates FLUTE's required attributes and processes data for the current instance.\n",
        "\n",
        "        This method implements the abstract load_data from BaseDataset.\n",
        "\n",
        "        Args:\n",
        "            data_dir_path (str): Path to the directory containing client JSONL files (for train)\n",
        "                                 or the aggregated eval file (for val/test).\n",
        "            user_idx (int): Index of the current user/client. -1 for aggregated test/val data.\n",
        "        \"\"\"\n",
        "        print_rank(f\"Loading data from directory: {data_dir_path}\", loglevel=logging.INFO)\n",
        "\n",
        "        raw_prompts_to_process = []\n",
        "        raw_labels_str_to_process = []\n",
        "\n",
        "        # Determine if we are loading client-specific data or aggregated data\n",
        "        # We assume client data is in a directory with files starting with 'client_'\n",
        "        # and aggregated data is a single file in its directory.\n",
        "        # Check for client files first\n",
        "        client_files_in_dir = [f for f in os.listdir(data_dir_path) if f.startswith('client_') and f.endswith('.jsonl')]\n",
        "        is_client_data_dir = len(client_files_in_dir) > 0\n",
        "\n",
        "        if is_client_data_dir and user_idx != -1 and not self.test_only:\n",
        "            # --- Scenario 1: Loading data for a specific client (Training) ---\n",
        "            print_rank(f\"Detected client data directory: {data_dir_path}\", loglevel=logging.DEBUG)\n",
        "            client_files = sorted(client_files_in_dir) # Use the list we already found\n",
        "\n",
        "            # Populate global user_list and num_samples, and collect raw data for all clients\n",
        "            for file_name in client_files:\n",
        "                file_path = os.path.join(data_dir_path, file_name)\n",
        "                client_id = os.path.splitext(file_name)[0]\n",
        "\n",
        "                self.user_list.append(client_id)\n",
        "                current_client_prompts = []\n",
        "                current_client_labels_str = []\n",
        "                num_samples_for_client = 0\n",
        "\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        try:\n",
        "                            data_entry = json.loads(line.strip())\n",
        "                            prompt_text = data_entry.get('prompt')\n",
        "                            label_str = data_entry.get('label')\n",
        "\n",
        "                            if prompt_text is None or label_str is None:\n",
        "                                print_rank(f\"Skipping line missing 'prompt' or 'label' in {file_name}: {line.strip()}\", loglevel=logging.WARNING)\n",
        "                                continue\n",
        "\n",
        "                            # Build global label map (string label to integer ID)\n",
        "                            if label_str not in self.label_map:\n",
        "                                self.label_map[label_str] = self.current_label_id\n",
        "                                self.current_label_id += 1\n",
        "                                self.all_labels.append(label_str)\n",
        "\n",
        "                            current_client_prompts.append(prompt_text)\n",
        "                            current_client_labels_str.append(label_str)\n",
        "                            num_samples_for_client += 1\n",
        "                        except json.JSONDecodeError as e:\n",
        "                            print_rank(f\"Skipping malformed JSONL line in {file_name}: {line.strip()} - Error: {e}\", loglevel=logging.WARNING)\n",
        "\n",
        "\n",
        "                self.user_data[client_id] = current_client_prompts\n",
        "                self.user_data_labels[client_id] = current_client_labels_str\n",
        "                self.num_samples.append(num_samples_for_client)\n",
        "\n",
        "            print_rank(f\"Global label map created: {self.label_map}\", loglevel=logging.INFO)\n",
        "            print_rank(f\"Discovered {len(self.user_list)} clients with sample counts: {self.num_samples}\", loglevel=logging.INFO)\n",
        "\n",
        "            # Select data for the specific client instance\n",
        "            if user_idx < 0 or user_idx >= len(self.user_list):\n",
        "                 raise IndexError(f\"user_idx {user_idx} is out of range for {len(self.user_list)} clients.\")\n",
        "\n",
        "            self.user = self.user_list[user_idx]\n",
        "            raw_prompts_to_process = self.user_data[self.user]\n",
        "            raw_labels_str_to_process = self.user_data_labels[self.user]\n",
        "            print_rank(f\"Processing training data for client: {self.user}\", loglevel=logging.INFO)\n",
        "\n",
        "        else:\n",
        "            # --- Scenario 2: Loading aggregated data (Evaluation/Testing or user_idx == -1) ---\n",
        "            print_rank(f\"Detected aggregated data directory: {data_dir_path}\", loglevel=logging.DEBUG)\n",
        "            # We expect a single aggregated file in this directory.\n",
        "            # Find the first JSONL file in the directory that does NOT start with 'client_'\n",
        "            aggregated_files = [f for f in os.listdir(data_dir_path) if f.endswith('.jsonl') and not f.startswith('client_')]\n",
        "\n",
        "            if len(aggregated_files) != 1:\n",
        "                raise FileNotFoundError(f\"Expected exactly one aggregated JSONL file (not starting with 'client_') in {data_dir_path} but found {len(aggregated_files)}.\")\n",
        "\n",
        "            file_name = aggregated_files[0]\n",
        "            file_path = os.path.join(data_dir_path, file_name)\n",
        "\n",
        "            self.user = 'aggregated_eval' # Assign a descriptive user ID for aggregated data\n",
        "            print_rank(f\"Processing aggregated evaluation data from {file_path}.\", loglevel=logging.INFO)\n",
        "\n",
        "            # Read all data from the single aggregated file\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        data_entry = json.loads(line.strip())\n",
        "                        prompt_text = data_entry.get('prompt')\n",
        "                        label_str = data_entry.get('label')\n",
        "\n",
        "                        if prompt_text is None or label_str is None:\n",
        "                            print_rank(f\"Skipping line missing 'prompt' or 'label' in {file_path}: {line.strip()}\", loglevel=logging.WARNING)\n",
        "                            continue\n",
        "\n",
        "                        # Build global label map (string label to integer ID)\n",
        "                        # This ensures eval data labels are also mapped consistently\n",
        "                        if label_str not in self.label_map:\n",
        "                            self.label_map[label_str] = self.current_label_id\n",
        "                            self.current_label_id += 1\n",
        "                            self.all_labels.append(label_str)\n",
        "\n",
        "                        raw_prompts_to_process.append(prompt_text)\n",
        "                        raw_labels_str_to_process.append(label_str)\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print_rank(f\"Skipping malformed JSONL line in {file_path}: {line.strip()} - Error: {e}\", loglevel=logging.WARNING)\n",
        "\n",
        "            # For aggregated data, user_list and num_samples might not be strictly necessary\n",
        "            # in the same format as for client data, but we can populate them conceptually.\n",
        "            self.user_list = ['aggregated_eval']\n",
        "            self.num_samples = [len(raw_prompts_to_process)]\n",
        "            # Store aggregated raw data under the 'aggregated_eval' key\n",
        "            self.user_data['aggregated_eval'] = raw_prompts_to_process\n",
        "            self.user_data_labels['aggregated_eval'] = raw_labels_str_to_process\n",
        "\n",
        "            print_rank(f\"Global label map created: {self.label_map}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "        # --- Step 3: Process the collected raw data for the current instance ---\n",
        "        # Tokenize prompts and map string labels to integer IDs\n",
        "        self.process_user_data(raw_prompts_to_process, raw_labels_str_to_process)\n",
        "\n",
        "        print_rank(f\"Dataset for user '{self.user}' initialized with {len(self.utt_list)} processed samples.\", loglevel=logging.DEBUG)\n",
        "\n",
        "    def process_user_data(self, prompts, labels_str):\n",
        "        \"\"\"\n",
        "        Tokenizes the raw prompt text data and prepares it into the format expected by __getitem__.\n",
        "        Maps string labels to integer IDs.\n",
        "        Populates self.utt_list with the processed samples for the current user/aggregation.\n",
        "\n",
        "        Args:\n",
        "            prompts (list): List of raw prompt strings for the current user/aggregation.\n",
        "            labels_str (list): List of raw string labels for the current user/aggregation.\n",
        "        \"\"\"\n",
        "        if not prompts:\n",
        "            print_rank(f\"Warning: No raw data provided for user {self.user}. Appending dummy sample.\", loglevel=logging.WARNING)\n",
        "            # Append a dummy sample if a user has no data to avoid issues with DataLoader\n",
        "            self.utt_list.append({\n",
        "                'input_ids': [self.tokenizer.cls_token_id, self.tokenizer.sep_token_id] if self.tokenizer.cls_token_id is not None and self.tokenizer.sep_token_id is not None else [0, 0],\n",
        "                'attention_mask': [1, 1],\n",
        "                'labels': 0 # Default label (assuming 0 is a valid class ID)\n",
        "            })\n",
        "            return\n",
        "\n",
        "        # --- Tokenization ---\n",
        "        # Use the Hugging Face tokenizer to convert text prompts into token IDs and attention masks.\n",
        "        tokenized_batch = self.tokenizer(\n",
        "            prompts,\n",
        "            max_length=self.max_seq_length, # Truncate to max_seq_length\n",
        "            truncation=True,\n",
        "            padding=False, # DO NOT pad here. DataCollatorWithPadding in DataLoader handles batch padding.\n",
        "            return_tensors=None # Return as Python lists, not PyTorch tensors yet.\n",
        "        )\n",
        "\n",
        "        # --- Prepare Final Samples ---\n",
        "        # Create the list of dictionaries, where each dictionary is a single sample\n",
        "        # containing tokenized inputs and integer labels.\n",
        "        self.utt_list = []\n",
        "        for i in range(len(prompts)):\n",
        "            # Ensure we have input_ids and attention_mask for each sample\n",
        "            input_ids = tokenized_batch['input_ids'][i] if 'input_ids' in tokenized_batch else []\n",
        "            attention_mask = tokenized_batch['attention_mask'][i] if 'attention_mask' in tokenized_batch else []\n",
        "\n",
        "            if not input_ids:\n",
        "                 print_rank(f\"Warning: Tokenization resulted in empty input_ids for a sample from user {self.user}. Skipping sample.\", loglevel=logging.WARNING)\n",
        "                 continue # Skip samples that resulted in empty tokenization\n",
        "\n",
        "            # Convert string label to integer ID using the global label map\n",
        "            # Use .get() with a default in case a label appears in eval data but not train (less likely with global map)\n",
        "            label_id = self.label_map.get(labels_str[i], 0) # Default to 0 if label not found\n",
        "\n",
        "            self.utt_list.append({\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'labels': label_id # Integer ID for the classification target\n",
        "                # Add other potential fields if needed by the model (e.g., 'token_type_ids')\n",
        "            })\n",
        "\n",
        "        if not self.utt_list:\n",
        "             print_rank(f\"Warning: All samples for user {self.user} were skipped after processing. Appending dummy sample.\", loglevel=logging.WARNING)\n",
        "             # Append dummy if processing resulted in no valid samples\n",
        "             self.utt_list.append({\n",
        "                'input_ids': [self.tokenizer.cls_token_id, self.tokenizer.sep_token_id] if self.tokenizer.cls_token_id is not None and self.tokenizer.sep_token_id is not None else [0, 0],\n",
        "                'attention_mask': [1, 1],\n",
        "                'labels': 0\n",
        "            })\n",
        "\n",
        "\n",
        "# --- BachelorDataLoader Class Definition (Same as before) ---\n",
        "# This class definition is placed AFTER the BaseDataLoader definition\n",
        "\n",
        "# experiments/bachelor/dataloaders/dataloader.py\n",
        "\n",
        "class BachelorDataLoader(BaseDataLoader):\n",
        "    \"\"\"\n",
        "    PyTorch DataLoader for the Bachelor classification task.\n",
        "    Instantiates BachelorDataset and uses DataCollatorWithPadding.\n",
        "    Inherits from core.dataloader.BaseDataLoader and torch.utils.data.DataLoader.\n",
        "    \"\"\"\n",
        "    def __init__(self, mode, data, num_workers=0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the DataLoader.\n",
        "\n",
        "        Args:\n",
        "            mode (str): The mode of the dataloader ('train', 'val', or 'test').\n",
        "                        FLUTE provides this.\n",
        "            data: This will be the path to the data directory containing client files (train)\n",
        "                  or the aggregated eval file (val/test), as specified in config.yaml.\n",
        "            num_workers (int, optional): Number of subprocesses to use for data loading.\n",
        "                                         Defaults to 0 (main process).\n",
        "            **kwargs: Contains the 'args' dictionary from config.yaml.\n",
        "        \"\"\"\n",
        "        # Add print statements to inspect kwargs and args\n",
        "        print_rank(f\"DataLoader __init__ called with mode={mode}, data={data}, num_workers={num_workers}\", loglevel=logging.DEBUG)\n",
        "        print_rank(f\"DataLoader __init__ kwargs: {kwargs}\", loglevel=logging.DEBUG) # Inspect kwargs\n",
        "\n",
        "        # Store mode and num_workers\n",
        "        self.mode = mode\n",
        "        self.num_workers = num_workers\n",
        "        self.utt_ids = None # Will store the user ID associated with the underlying dataset\n",
        "\n",
        "        # Get configuration arguments\n",
        "        args = kwargs.get('args', {})\n",
        "        print_rank(f\"DataLoader __init__ args: {args}\", loglevel=logging.DEBUG) # Inspect args\n",
        "\n",
        "        # Get batch_size from args, default to 4 if not specified\n",
        "        self.batch_size = args.get('batch_size', 4)\n",
        "        print_rank(f\"DataLoader __init__ setting batch_size to: {self.batch_size}\", loglevel=logging.DEBUG) # Inspect final batch_size\n",
        "\n",
        "\n",
        "        # --- Tokenizer Initialization ---\n",
        "        # Initialize the tokenizer. It's important that this matches the one used in the Dataset\n",
        "        # to ensure consistent tokenization and vocabulary.\n",
        "        tokenizer_kwargs = {\n",
        "            \"cache_dir\": args.get('cache_dir', None),\n",
        "            \"use_fast\": args.get('tokenizer_type_fast', True),\n",
        "            \"use_auth_token\": None\n",
        "        }\n",
        "        if 'tokenizer_name' in args and args['tokenizer_name']:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(args['tokenizer_name'], **tokenizer_kwargs)\n",
        "        elif 'model_name_or_path' in args and args['model_name_or_path']:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(args['model_name_or_path'], **tokenizer_kwargs)\n",
        "        else:\n",
        "            raise ValueError(\"You must provide a tokenizer instance or specify 'tokenizer_name' or 'model_name_or_path' in args.\")\n",
        "\n",
        "        print_rank(f\"DataLoader using tokenizer: {self.tokenizer.name_or_path}\", loglevel=logging.DEBUG)\n",
        "\n",
        "        # --- Dataset Instantiation ---\n",
        "        # Instantiate our custom BachelorDataset.\n",
        "        # The 'data' argument passed here is the data_dir_path from config.yaml.\n",
        "        # user_idx is passed by FLUTE to the DataLoader's constructor, and we pass it along to the Dataset.\n",
        "        user_idx = kwargs.get('user_idx', -1)\n",
        "        self.dataset = BachelorDataset(\n",
        "            data_path=data, # Pass the data directory path\n",
        "            args=args,      # Pass the configuration arguments\n",
        "            tokenizer=self.tokenizer, # Pass the initialized tokenizer\n",
        "            test_only=(self.mode != 'train'), # Set test_only flag based on mode\n",
        "            user_idx=user_idx, # Pass the user_idx provided by FLUTE\n",
        "        )\n",
        "        # Get the user ID from the dataset instance (useful for logging/tracking)\n",
        "        self.utt_ids = self.dataset.user\n",
        "\n",
        "        # --- Data Collator ---\n",
        "        # Use Hugging Face's DataCollatorWithPadding. This is crucial for handling batches\n",
        "        # of sequences with varying lengths. It pads sequences to the length of the longest\n",
        "        # sequence in the current batch and creates the appropriate attention mask.\n",
        "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer, padding='longest')\n",
        "\n",
        "        # --- Sampler Selection ---\n",
        "        # Choose sampler based on the mode. RandomSampler for training, SequentialSampler for evaluation.\n",
        "        if self.mode == 'train':\n",
        "            sampler = RandomSampler(self.dataset)\n",
        "        elif self.mode == 'val' or self.mode == 'test':\n",
        "            sampler = SequentialSampler(self.dataset)\n",
        "        else:\n",
        "            raise Exception(f\"Invalid mode parameter: {self.mode}. Must be 'train', 'val', or 'test'.\")\n",
        "\n",
        "        # --- PyTorch DataLoader Initialization ---\n",
        "        # Finally, initialize the standard PyTorch DataLoader using the dataset, sampler, batch size,\n",
        "        # and our custom data collator.\n",
        "        super(BachelorDataLoader, self).__init__(\n",
        "            self.dataset,\n",
        "            batch_size=self.batch_size, # Use the batch_size determined above\n",
        "            sampler=sampler,\n",
        "            collate_fn=self.data_collator, # Use the data collator for batch processing\n",
        "            drop_last=False, # Whether to drop the last incomplete batch (usually False for eval)\n",
        "            num_workers=self.num_workers, # Number of worker processes for data loading\n",
        "            pin_memory=True, # Pin memory for faster data transfer to GPU (useful for future GPU use)\n",
        "        )\n",
        "\n",
        "    def get_user(self):\n",
        "        \"\"\"\n",
        "        Returns the user ID associated with the underlying dataset instance.\n",
        "        Useful for FLUTE's internal tracking.\n",
        "        \"\"\"\n",
        "        return self.utt_ids\n",
        "\n",
        "    # The create_loader method from BaseDataLoader is inherited and simply returns self.\n",
        "    # def create_loader(self):\n",
        "    #     return self\n",
        "\n",
        "\n",
        "# --- Test Classes (Same as before) ---\n",
        "\n",
        "class TestBachelorDataset(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"Set up dummy data simulating drive access before running tests.\"\"\"\n",
        "        # Ensure cleanup before creating new data\n",
        "        cleanup_dummy_data()\n",
        "        create_dummy_client_data_files(TRAIN_DATA_DIR, DUMMY_CLIENT_DATA)\n",
        "        create_dummy_aggregated_data_file(VAL_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"val_data.jsonl\")\n",
        "        create_dummy_aggregated_data_file(TEST_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"test_data.jsonl\")\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"Clean up dummy data after running tests.\"\"\"\n",
        "        # cleanup_dummy_data() # Optional: keep data after run for inspection\n",
        "        pass # Keep dummy data directory after run for inspection\n",
        "\n",
        "\n",
        "    def test_dataset_client_loading(self):\n",
        "        \"\"\"Tests loading data for individual clients from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataset Client Loading (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        # Dummy args similar to what would come from config.yaml\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased', # Or 'BioClinicalBERT' if available publicly\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "        }\n",
        "\n",
        "        # Test loading for each client (0 to 5)\n",
        "        for client_idx in range(6):\n",
        "            client_id = f\"client_{client_idx}\"\n",
        "            print_rank(f\"Testing client: {client_id}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Pass the directory path containing all client files\n",
        "            dataset = BachelorDataset(\n",
        "                data_path=TRAIN_DATA_DIR, # Pass the directory path\n",
        "                args=dummy_args,\n",
        "                test_only=False,\n",
        "                user_idx=client_idx # FLUTE provides the user_idx\n",
        "            )\n",
        "\n",
        "            # Assertions\n",
        "            expected_sample_count = len(DUMMY_CLIENT_DATA.get(client_id, []))\n",
        "            self.assertEqual(len(dataset), expected_sample_count, f\"Dataset length mismatch for {client_id}\")\n",
        "            self.assertEqual(dataset.user, client_id, f\"Dataset user ID mismatch for {client_id}\")\n",
        "            self.assertIsNotNone(dataset.tokenizer)\n",
        "            self.assertGreater(len(dataset.label_map), 0, \"Label map should not be empty\")\n",
        "            # Check that user_list and num_samples contain info for ALL clients in the directory\n",
        "            self.assertEqual(len(dataset.user_list), len(DUMMY_CLIENT_DATA), \"user_list should contain all client IDs\")\n",
        "            self.assertEqual(len(dataset.num_samples), len(DUMMY_CLIENT_DATA), \"num_samples should contain counts for all clients\")\n",
        "\n",
        "            if expected_sample_count > 0:\n",
        "                # Test __getitem__ for the first sample if data exists\n",
        "                sample = dataset[0]\n",
        "                self.assertIn('input_ids', sample)\n",
        "                self.assertIn('attention_mask', sample)\n",
        "                self.assertIn('labels', sample)\n",
        "                self.assertIsInstance(sample['input_ids'], list)\n",
        "                self.assertIsInstance(sample['attention_mask'], list)\n",
        "                self.assertIsInstance(sample['labels'], int)\n",
        "                self.assertGreater(len(sample['input_ids']), 0, f\"Tokenization failed for a sample in {client_id}\")\n",
        "                self.assertIn(sample['labels'], dataset.label_map.values(), f\"Label mapping failed for a sample in {client_id}\")\n",
        "\n",
        "            print_rank(f\"Successfully loaded and tested data for {client_id}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def test_dataset_aggregated_loading(self):\n",
        "        \"\"\"Tests loading aggregated data from simulated drive file.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataset Aggregated Loading (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "        }\n",
        "\n",
        "        # Test loading aggregated validation data (user_idx=-1, test_only=True)\n",
        "        # The data_path points to the directory containing the single aggregated file\n",
        "        aggregated_dataset_val = BachelorDataset(\n",
        "            data_path=VAL_DATA_DIR, # Pass the directory path\n",
        "            args=dummy_args,\n",
        "            test_only=True, # Indicates this is for evaluation\n",
        "            user_idx=-1 # Indicates aggregated data\n",
        "        )\n",
        "\n",
        "        total_eval_samples = len(DUMMY_AGGREGATED_EVAL_DATA)\n",
        "        self.assertEqual(len(aggregated_dataset_val), total_eval_samples, \"Aggregated dataset length mismatch\")\n",
        "        self.assertEqual(aggregated_dataset_val.user, 'aggregated_eval')\n",
        "        self.assertIsNotNone(aggregated_dataset_val.tokenizer)\n",
        "        self.assertGreater(len(aggregated_dataset_val.label_map), 0, \"Label map should not be empty for aggregated data\")\n",
        "\n",
        "        if total_eval_samples > 0:\n",
        "             # Test __getitem__ for aggregated data\n",
        "            sample = aggregated_dataset_val[0]\n",
        "            self.assertIn('input_ids', sample)\n",
        "            self.assertIn('labels', sample)\n",
        "            self.assertIsInstance(sample['labels'], int)\n",
        "            self.assertIn(sample['labels'], aggregated_dataset_val.label_map.values())\n",
        "\n",
        "        print_rank(f\"Successfully loaded and tested aggregated validation data ({total_eval_samples} samples)\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "class TestBachelorDataLoader(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"Set up dummy data simulating drive access before running tests.\"\"\"\n",
        "        # Ensure cleanup before creating new data\n",
        "        cleanup_dummy_data()\n",
        "        create_dummy_client_data_files(TRAIN_DATA_DIR, DUMMY_CLIENT_DATA)\n",
        "        create_dummy_aggregated_data_file(VAL_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"val_data.jsonl\")\n",
        "        create_dummy_aggregated_data_file(TEST_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"test_data.jsonl\")\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"Clean up dummy data after running tests.\"\"\"\n",
        "        # cleanup_dummy_data() # Optional: keep data after run for inspection\n",
        "        pass # Keep dummy data directory after run for inspection\n",
        "\n",
        "\n",
        "    def test_dataloader_client_train(self):\n",
        "        \"\"\"Tests DataLoader for a client in training mode from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataLoader Client Train (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        # Dummy args including batch size\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "            'batch_size': 2, # Use a small batch size for testing\n",
        "        }\n",
        "        # Add print to inspect dummy_args before passing\n",
        "        print_rank(f\"test_dataloader_client_train dummy_args: {dummy_args}\", loglevel=logging.DEBUG)\n",
        "\n",
        "        # Test DataLoader for client 0\n",
        "        client_idx = 0\n",
        "        client_id = f\"client_{client_idx}\"\n",
        "        dataloader = BachelorDataLoader(\n",
        "            mode='train',\n",
        "            data=TRAIN_DATA_DIR, # Pass the directory path\n",
        "            num_workers=0, # Use 0 workers for simplicity in testing\n",
        "            args=dummy_args, # args is passed as a keyword argument\n",
        "            user_idx=client_idx # Specify the client index\n",
        "        )\n",
        "\n",
        "        # Assertions on DataLoader and underlying Dataset\n",
        "        self.assertEqual(dataloader.get_user(), client_id)\n",
        "        self.assertIsInstance(dataloader.dataset, BachelorDataset)\n",
        "        expected_sample_count = len(DUMMY_CLIENT_DATA.get(client_id, []))\n",
        "        self.assertEqual(len(dataloader.dataset), expected_sample_count, f\"Dataset length mismatch for {client_id} in DataLoader\")\n",
        "        self.assertEqual(dataloader.batch_size, 2) # Asserting against the value set in dummy_args\n",
        "\n",
        "        # Iterate through batches\n",
        "        num_batches = 0\n",
        "        total_samples_yielded = 0\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            num_batches += 1\n",
        "            print_rank(f\"Client {client_id} Train Batch {i+1}: Keys - {batch.keys()}, Batch Size - {batch['input_ids'].shape[0]}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Assert batch structure and types\n",
        "            self.assertIn('input_ids', batch)\n",
        "            self.assertIn('attention_mask', batch)\n",
        "            self.assertIn('labels', batch)\n",
        "\n",
        "            self.assertIsInstance(batch['input_ids'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['attention_mask'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['labels'], torch.Tensor)\n",
        "\n",
        "            # Check shapes\n",
        "            self.assertEqual(batch['input_ids'].shape, batch['attention_mask'].shape)\n",
        "            self.assertEqual(batch['input_ids'].shape[0], len(batch['labels']))\n",
        "            self.assertLessEqual(batch['input_ids'].shape[0], dummy_args['batch_size'])\n",
        "\n",
        "            # Check padding\n",
        "            self.assertEqual(batch['input_ids'].shape[1], batch['attention_mask'].shape[1])\n",
        "            self.assertGreater(batch['input_ids'].shape[1], 0)\n",
        "\n",
        "            # Check label tensor type\n",
        "            self.assertEqual(batch['labels'].dtype, torch.long)\n",
        "\n",
        "            total_samples_yielded += batch['input_ids'].shape[0]\n",
        "\n",
        "        self.assertGreater(num_batches, 0, f\"DataLoader for {client_id} should yield at least one batch\")\n",
        "        # If drop_last is False, the total samples yielded should match the dataset size\n",
        "        self.assertEqual(total_samples_yielded, expected_sample_count, f\"Total samples yielded mismatch for {client_id}\")\n",
        "\n",
        "        print_rank(f\"Successfully tested DataLoader for {client_id} in train mode.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def test_dataloader_aggregated_eval(self):\n",
        "        \"\"\"Tests DataLoader for aggregated evaluation data from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataLoader Aggregated Eval (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "            'batch_size': 3, # Use a small batch size for testing\n",
        "        }\n",
        "        # Add print to inspect dummy_args before passing\n",
        "        print_rank(f\"test_dataloader_aggregated_eval dummy_args: {dummy_args}\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "        # Test loading aggregated validation data (mode='val', user_idx=-1)\n",
        "        dataloader = BachelorDataLoader(\n",
        "            mode='val', # Or 'test'\n",
        "            data=VAL_DATA_DIR, # Pass the directory path\n",
        "            num_workers=0,\n",
        "            args=dummy_args, # args is passed as a keyword argument\n",
        "            user_idx=-1 # Indicates aggregated data\n",
        "        )\n",
        "\n",
        "        # Assertions on DataLoader and underlying Dataset\n",
        "        self.assertEqual(dataloader.get_user(), 'aggregated_eval')\n",
        "        self.assertIsInstance(dataloader.dataset, BachelorDataset)\n",
        "        total_eval_samples = len(DUMMY_AGGREGATED_EVAL_DATA)\n",
        "        self.assertEqual(len(dataloader.dataset), total_eval_samples, \"Aggregated dataset length mismatch in DataLoader\")\n",
        "        self.assertEqual(dataloader.batch_size, 3) # Asserting against the value set in dummy_args\n",
        "\n",
        "\n",
        "        # Iterate through all batches for evaluation\n",
        "        num_batches = 0\n",
        "        total_samples_yielded = 0\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            num_batches += 1\n",
        "            print_rank(f\"Aggregated Eval Batch {i+1}: Keys - {batch.keys()}, Batch Size - {batch['input_ids'].shape[0]}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Assert batch structure and types\n",
        "            self.assertIn('input_ids', batch)\n",
        "            self.assertIn('attention_mask', batch)\n",
        "            self.assertIn('labels', batch)\n",
        "\n",
        "            self.assertIsInstance(batch['input_ids'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['attention_mask'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['labels'], torch.Tensor)\n",
        "\n",
        "            # Check shapes\n",
        "            self.assertEqual(batch['input_ids'].shape, batch['attention_mask'].shape)\n",
        "            self.assertEqual(batch['input_ids'].shape[0], len(batch['labels']))\n",
        "            self.assertLessEqual(batch['input_ids'].shape[0], dummy_args['batch_size'])\n",
        "\n",
        "            # Check padding\n",
        "            self.assertEqual(batch['input_ids'].shape[1], batch['attention_mask'].shape[1])\n",
        "            self.assertGreater(batch['input_ids'].shape[1], 0)\n",
        "\n",
        "            # Check label tensor type\n",
        "            self.assertEqual(batch['labels'].dtype, torch.long)\n",
        "\n",
        "            total_samples_yielded += batch['input_ids'].shape[0]\n",
        "\n",
        "\n",
        "        self.assertGreater(num_batches, 0, \"Aggregated DataLoader should yield at least one batch\")\n",
        "        self.assertEqual(total_samples_yielded, total_eval_samples, \"Total samples yielded mismatch for aggregated data\")\n",
        "\n",
        "        print_rank(f\"Successfully tested DataLoader for aggregated evaluation data.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "# --- Test Execution in Colab ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This block runs when the script is executed directly\n",
        "    # In Colab, the entire cell is executed, so this acts like the main entry point\n",
        "\n",
        "    print(\"--- Running Bachelor Dataset and DataLoader Tests in Colab ---\")\n",
        "\n",
        "    # Create a TestSuite and add tests from both classes\n",
        "    suite = unittest.TestSuite()\n",
        "    suite.addTest(unittest.makeSuite(TestBachelorDataset))\n",
        "    suite.addTest(unittest.makeSuite(TestBachelorDataLoader))\n",
        "\n",
        "    # Create a TextTestRunner and run the tests\n",
        "    # verbosity=2 provides more detailed output\n",
        "    runner = unittest.TextTestRunner(verbosity=2)\n",
        "    result = runner.run(suite)\n",
        "\n",
        "    print(\"--- Test Run Finished ---\")\n",
        "\n",
        "    # Optional: Provide feedback based on test results\n",
        "    if result.wasSuccessful():\n",
        "        print(\"All tests passed!\")\n",
        "    else:\n",
        "        print(\"Some tests failed. Check the output above for details.\")\n",
        "\n",
        "    # Optional: Cleanup dummy data after tests (uncomment tearDownClass pass if you want to keep it)\n",
        "    # cleanup_dummy_data()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ff451a3e6b654c599aa7e21a6b87c491",
            "2ae6e8f762124fde83ec50dc828e5f49",
            "6c395d04662d44ad9a5ad4d1f4912350",
            "ef02584088b84931a9f5d9b0959a7bfa",
            "17831eed1ad54e188917b0219eb50740",
            "a6a70d8b283246a0b3634cf4a52fbebe",
            "5dd3e43e2eca49c4bdd50b727ce0e9fd",
            "b4e3032758e24e36b8b413e2ce662a8e",
            "e610aeba5e3c4f8d8b352997712f8538",
            "d4b7f21b75de48afb37601119632c68b",
            "90bc7726174642cfaa620766c7ac9381",
            "870a80a3d8574730bcda7ffef75d5aa1",
            "72ac78e2c8a54f02b021baff23de2a84",
            "54f623d981cb43bba0b76c2d1e90418a",
            "eb0acb82b0d54fb0aaf6b8cac3a4a7a1",
            "f3df11b5a07b4c0ea506f0082ca0955f",
            "54e179e1d736494189f58cc785f01c24",
            "cc8c7468cbdd40089010b86e02fd95e1",
            "3956491227044a6fb488cae61b301682",
            "59f9a321b5854d11b113d2c98bd9f3a5",
            "149b7be869774e68b9acfa044d4c1c60",
            "53feb9f27a8a4eae835ff8ac5faab084",
            "9f24e784a6a4424493b20208c8743c79",
            "eff4c85cd7d54c1ba5e2bb3a7fdb3ac8",
            "790af4f117b542e2bb35b4e650bd59b8",
            "cb2da0f4496e4200a0e8e536d19159f7",
            "9adad984de0d49dda90a346b5b6dba42",
            "045a1a262c1941dd96ab3a0c1ae045c5",
            "fa042d041f6948d493728bd024de7606",
            "b31efd05a4f74e5ea521ad00f86b3a85",
            "2945fe9f81e24e47b3bb78dce946e315",
            "3d09aa129df34d2494551bed1b3b6630",
            "72364835451742f6b4a614eb7bf88433",
            "7cfb9922940e4f1496f01ea4c8ff97d2",
            "2dfb008084e841a99346e7df704a8fc0",
            "4817fd0a174f4841a0fa34dcc07ca10c",
            "d16283506f414c8ca65a9a6effe2c384",
            "840a2d7dc8004a128f1cd4a543457530",
            "e3eb2991c962438d8dbd442e2c0a3917",
            "ac5738abdf5a47e484091eaa4b0090a0",
            "95de10945840484c9c74cf64b9eb2fcb",
            "ec46914678364b5ab666b42131c943eb",
            "941513b472a9458a85a0b409af562c35",
            "b311afe0d29f40f1921d13f4424b017b"
          ]
        },
        "id": "m9Q_EDVih-22",
        "outputId": "79ec2c14-04f0-49c9-96cd-f7380ebb71b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-e63f0df464cb>:893: DeprecationWarning: unittest.makeSuite() is deprecated and will be removed in Python 3.13. Please use unittest.TestLoader.loadTestsFromTestCase() instead.\n",
            "  suite.addTest(unittest.makeSuite(TestBachelorDataset))\n",
            "<ipython-input-4-e63f0df464cb>:894: DeprecationWarning: unittest.makeSuite() is deprecated and will be removed in Python 3.13. Please use unittest.TestLoader.loadTestsFromTestCase() instead.\n",
            "  suite.addTest(unittest.makeSuite(TestBachelorDataLoader))\n",
            "test_dataset_aggregated_loading (__main__.TestBachelorDataset.test_dataset_aggregated_loading)\n",
            "Tests loading aggregated data from simulated drive file. ... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Running Bachelor Dataset and DataLoader Tests in Colab ---\n",
            "[INFO] Created dummy client data files in: ./simulated_drive_data/train_clients\n",
            "[INFO] Created dummy aggregated data file in: ./simulated_drive_data/val_aggregated\n",
            "[INFO] Created dummy aggregated data file in: ./simulated_drive_data/test_aggregated\n",
            "[INFO] \n",
            "--- Testing BachelorDataset Aggregated Loading (Simulated Drive) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff451a3e6b654c599aa7e21a6b87c491"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "870a80a3d8574730bcda7ffef75d5aa1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f24e784a6a4424493b20208c8743c79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cfb9922940e4f1496f01ea4c8ff97d2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ok\n",
            "test_dataset_client_loading (__main__.TestBachelorDataset.test_dataset_client_loading)\n",
            "Tests loading data for individual clients from simulated drive directory. ... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading data from directory: ./simulated_drive_data/val_aggregated\n",
            "[DEBUG] Detected aggregated data directory: ./simulated_drive_data/val_aggregated\n",
            "[INFO] Processing aggregated evaluation data from ./simulated_drive_data/val_aggregated/val_data.jsonl.\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[DEBUG] Dataset for user 'aggregated_eval' initialized with 4 processed samples.\n",
            "[INFO] Successfully loaded and tested aggregated validation data (4 samples)\n",
            "[INFO] \n",
            "--- Testing BachelorDataset Client Loading (Simulated Drive) ---\n",
            "[DEBUG] Testing client: client_0\n",
            "[INFO] Loading data from directory: ./simulated_drive_data/train_clients\n",
            "[DEBUG] Detected client data directory: ./simulated_drive_data/train_clients\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[INFO] Discovered 6 clients with sample counts: [5, 4, 3, 2, 4, 6]\n",
            "[INFO] Processing training data for client: client_0\n",
            "[DEBUG] Dataset for user 'client_0' initialized with 5 processed samples.\n",
            "[INFO] Successfully loaded and tested data for client_0\n",
            "[DEBUG] Testing client: client_1\n",
            "[INFO] Loading data from directory: ./simulated_drive_data/train_clients\n",
            "[DEBUG] Detected client data directory: ./simulated_drive_data/train_clients\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[INFO] Discovered 6 clients with sample counts: [5, 4, 3, 2, 4, 6]\n",
            "[INFO] Processing training data for client: client_1\n",
            "[DEBUG] Dataset for user 'client_1' initialized with 4 processed samples.\n",
            "[INFO] Successfully loaded and tested data for client_1\n",
            "[DEBUG] Testing client: client_2\n",
            "[INFO] Loading data from directory: ./simulated_drive_data/train_clients\n",
            "[DEBUG] Detected client data directory: ./simulated_drive_data/train_clients\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[INFO] Discovered 6 clients with sample counts: [5, 4, 3, 2, 4, 6]\n",
            "[INFO] Processing training data for client: client_2\n",
            "[DEBUG] Dataset for user 'client_2' initialized with 3 processed samples.\n",
            "[INFO] Successfully loaded and tested data for client_2\n",
            "[DEBUG] Testing client: client_3\n",
            "[INFO] Loading data from directory: ./simulated_drive_data/train_clients\n",
            "[DEBUG] Detected client data directory: ./simulated_drive_data/train_clients\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[INFO] Discovered 6 clients with sample counts: [5, 4, 3, 2, 4, 6]\n",
            "[INFO] Processing training data for client: client_3\n",
            "[DEBUG] Dataset for user 'client_3' initialized with 2 processed samples.\n",
            "[INFO] Successfully loaded and tested data for client_3\n",
            "[DEBUG] Testing client: client_4\n",
            "[INFO] Loading data from directory: ./simulated_drive_data/train_clients\n",
            "[DEBUG] Detected client data directory: ./simulated_drive_data/train_clients\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[INFO] Discovered 6 clients with sample counts: [5, 4, 3, 2, 4, 6]\n",
            "[INFO] Processing training data for client: client_4\n",
            "[DEBUG] Dataset for user 'client_4' initialized with 4 processed samples.\n",
            "[INFO] Successfully loaded and tested data for client_4\n",
            "[DEBUG] Testing client: client_5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ok\n",
            "test_dataloader_aggregated_eval (__main__.TestBachelorDataLoader.test_dataloader_aggregated_eval)\n",
            "Tests DataLoader for aggregated evaluation data from simulated drive directory. ... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading data from directory: ./simulated_drive_data/train_clients\n",
            "[DEBUG] Detected client data directory: ./simulated_drive_data/train_clients\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[INFO] Discovered 6 clients with sample counts: [5, 4, 3, 2, 4, 6]\n",
            "[INFO] Processing training data for client: client_5\n",
            "[DEBUG] Dataset for user 'client_5' initialized with 6 processed samples.\n",
            "[INFO] Successfully loaded and tested data for client_5\n",
            "[INFO] Cleaned up dummy data directory: ./simulated_drive_data\n",
            "[INFO] Created dummy client data files in: ./simulated_drive_data/train_clients\n",
            "[INFO] Created dummy aggregated data file in: ./simulated_drive_data/val_aggregated\n",
            "[INFO] Created dummy aggregated data file in: ./simulated_drive_data/test_aggregated\n",
            "[INFO] \n",
            "--- Testing BachelorDataLoader Aggregated Eval (Simulated Drive) ---\n",
            "[DEBUG] test_dataloader_aggregated_eval dummy_args: {'model_name_or_path': 'bert-base-uncased', 'max_seq_length': 128, 'padding': True, 'cache_dir': None, 'tokenizer_type_fast': True, 'batch_size': 3}\n",
            "[DEBUG] DataLoader __init__ called with mode=val, data=./simulated_drive_data/val_aggregated, num_workers=0\n",
            "[DEBUG] DataLoader __init__ kwargs: {'args': {'model_name_or_path': 'bert-base-uncased', 'max_seq_length': 128, 'padding': True, 'cache_dir': None, 'tokenizer_type_fast': True, 'batch_size': 3}, 'user_idx': -1}\n",
            "[DEBUG] DataLoader __init__ args: {'model_name_or_path': 'bert-base-uncased', 'max_seq_length': 128, 'padding': True, 'cache_dir': None, 'tokenizer_type_fast': True, 'batch_size': 3}\n",
            "[DEBUG] DataLoader __init__ setting batch_size to: 3\n",
            "[DEBUG] DataLoader using tokenizer: bert-base-uncased\n",
            "[INFO] Loading data from directory: ./simulated_drive_data/val_aggregated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ok\n",
            "test_dataloader_client_train (__main__.TestBachelorDataLoader.test_dataloader_client_train)\n",
            "Tests DataLoader for a client in training mode from simulated drive directory. ... "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] Detected aggregated data directory: ./simulated_drive_data/val_aggregated\n",
            "[INFO] Processing aggregated evaluation data from ./simulated_drive_data/val_aggregated/val_data.jsonl.\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[DEBUG] Dataset for user 'aggregated_eval' initialized with 4 processed samples.\n",
            "[DEBUG] Aggregated Eval Batch 1: Keys - dict_keys(['input_ids', 'attention_mask', 'labels']), Batch Size - 3\n",
            "[DEBUG] Aggregated Eval Batch 2: Keys - dict_keys(['input_ids', 'attention_mask', 'labels']), Batch Size - 1\n",
            "[INFO] Successfully tested DataLoader for aggregated evaluation data.\n",
            "[INFO] \n",
            "--- Testing BachelorDataLoader Client Train (Simulated Drive) ---\n",
            "[DEBUG] test_dataloader_client_train dummy_args: {'model_name_or_path': 'bert-base-uncased', 'max_seq_length': 128, 'padding': True, 'cache_dir': None, 'tokenizer_type_fast': True, 'batch_size': 2}\n",
            "[DEBUG] DataLoader __init__ called with mode=train, data=./simulated_drive_data/train_clients, num_workers=0\n",
            "[DEBUG] DataLoader __init__ kwargs: {'args': {'model_name_or_path': 'bert-base-uncased', 'max_seq_length': 128, 'padding': True, 'cache_dir': None, 'tokenizer_type_fast': True, 'batch_size': 2}, 'user_idx': 0}\n",
            "[DEBUG] DataLoader __init__ args: {'model_name_or_path': 'bert-base-uncased', 'max_seq_length': 128, 'padding': True, 'cache_dir': None, 'tokenizer_type_fast': True, 'batch_size': 2}\n",
            "[DEBUG] DataLoader __init__ setting batch_size to: 2\n",
            "[DEBUG] DataLoader using tokenizer: bert-base-uncased\n",
            "[INFO] Loading data from directory: ./simulated_drive_data/train_clients\n",
            "[DEBUG] Detected client data directory: ./simulated_drive_data/train_clients\n",
            "[INFO] Global label map created: {'Hypertension': 0, 'Jaundice': 1, 'Bronchial Asthma': 2}\n",
            "[INFO] Discovered 6 clients with sample counts: [5, 4, 3, 2, 4, 6]\n",
            "[INFO] Processing training data for client: client_0\n",
            "[DEBUG] Dataset for user 'client_0' initialized with 5 processed samples.\n",
            "[DEBUG] Client client_0 Train Batch 1: Keys - dict_keys(['input_ids', 'attention_mask', 'labels']), Batch Size - 2\n",
            "[DEBUG] Client client_0 Train Batch 2: Keys - dict_keys(['input_ids', 'attention_mask', 'labels']), Batch Size - 2\n",
            "[DEBUG] Client client_0 Train Batch 3: Keys - dict_keys(['input_ids', 'attention_mask', 'labels']), Batch Size - 1\n",
            "[INFO] Successfully tested DataLoader for client_0 in train mode.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 4 tests in 3.858s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Test Run Finished ---\n",
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Test Suite for Bachelor Project (Colab Cell)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import unittest\n",
        "import torch as T # Keep T alias for use in method bodies\n",
        "import torch # Import torch explicitly for type hints\n",
        "import logging\n",
        "import sys\n",
        "import itertools\n",
        "from collections import defaultdict\n",
        "from typing import (Dict,\n",
        "                    List,\n",
        "                    Optional,\n",
        "                    Tuple,\n",
        "                    Union,\n",
        "                    Any)\n",
        "\n",
        "# Import necessary components for BaseDataset, BaseModel, and PyTorch\n",
        "from abc import ABC, abstractmethod\n",
        "from torch.utils.data import Dataset as PyTorchDataset\n",
        "from torch.utils.data import DataLoader as PyTorchDataLoader\n",
        "# Import Samplers explicitly for use within the cell\n",
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "\n",
        "import torch.nn as T_nn # Using T_nn to avoid conflict if torch is imported as T\n",
        "\n",
        "# Import from Hugging Face transformers\n",
        "try:\n",
        "    from transformers import (\n",
        "        AutoConfig,\n",
        "        AutoModelForSequenceClassification, # Use this for classification\n",
        "        AutoTokenizer, # Needed potentially for model config\n",
        "        DataCollatorWithPadding, # Needed for DataLoader tests\n",
        "        set_seed,\n",
        "        # Import specific model class for more precise assertion if needed,\n",
        "        # but checking for nn.Module and classifier is more general.\n",
        "        BertForSequenceClassification # Example specific model class\n",
        "    )\n",
        "except ImportError:\n",
        "     print(\"Error: Hugging Face transformers library not found. Please install it (`pip install transformers torch`).\")\n",
        "     # In a real script, you might want to exit or raise here\n",
        "     # For this standalone file, we'll just print the error\n",
        "     pass # Or raise ImportError(\"transformers or torch not found\")\n",
        "\n",
        "\n",
        "# --- Set up Python Path for Imports ---\n",
        "# Add your project root directory to the Python path\n",
        "# Assuming your Colab notebook is in the project root, or you adjust this path\n",
        "# If your project root is, for example, '/content/drive/MyDrive/YourBachelorProject',\n",
        "# you would set PROJECT_ROOT = '/content/drive/drive/MyDrive/YourBachelorProject' # Corrected path structure\n",
        "PROJECT_ROOT = './' # Adjust this path if your notebook is not in the project root\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# Import Base classes from core FLUTE (defined below in this cell)\n",
        "# from core.model import BaseModel\n",
        "# from core.dataset import BaseDataset\n",
        "# from core.dataloader import BaseDataLoader\n",
        "\n",
        "# Import your custom classes (defined below in this cell)\n",
        "# from experiments.bachelor.dataloaders.dataset import BachelorDataset\n",
        "# from experiments.bachelor.dataloaders.dataloader import BachelorDataLoader\n",
        "# from experiments.bachelor.models.model import BachelorModel # Defined below\n",
        "\n",
        "# --- print_rank and to_device ---\n",
        "# Attempt to import print_rank from utils, otherwise use dummy.\n",
        "# Always define and use a to_device function that accepts the device argument.\n",
        "print_rank_func = None\n",
        "\n",
        "try:\n",
        "    # Adjust this import path if your utils.py is elsewhere\n",
        "    from utils import print_rank as imported_print_rank\n",
        "    print_rank_func = imported_print_rank\n",
        "    print(f\"[INFO] Using print_rank from {os.path.join(PROJECT_ROOT, 'utils.py')}\")\n",
        "except ImportError:\n",
        "    # Dummy implementation if print_rank is not found\n",
        "    def dummy_print_rank(msg, loglevel=logging.INFO):\n",
        "        level_map = {logging.DEBUG: 'DEBUG', logging.INFO: 'INFO', logging.WARNING: 'WARNING', logging.ERROR: 'ERROR'}\n",
        "        print(f\"[{level_map.get(loglevel, 'INFO')}] {msg}\")\n",
        "    print_rank_func = dummy_print_rank\n",
        "    print(\"[INFO] Using dummy print_rank implementation.\")\n",
        "\n",
        "# Assign the chosen print_rank function\n",
        "print_rank = print_rank_func\n",
        "\n",
        "# Define a to_device function that accepts the device argument,\n",
        "# regardless of whether utils.py has one.\n",
        "def to_device(tensor: Any, device: str = 'cuda' if T.cuda.is_available() else 'cpu') -> Any:\n",
        "    \"\"\"Function to move tensor to device, accepts device argument.\"\"\"\n",
        "    if isinstance(tensor, T.Tensor):\n",
        "        # print_rank(f\"[DEBUG] to_device: Moving tensor to device: {device}\", loglevel=logging.DEBUG)\n",
        "        return tensor.to(device)\n",
        "    # Handle nested structures like lists, tuples, and dictionaries\n",
        "    if isinstance(tensor, (list, tuple)):\n",
        "        return type(tensor)(to_device(item, device) for item in tensor)\n",
        "    if isinstance(tensor, dict):\n",
        "        return {k: to_device(v, device) for k, v in tensor.items()}\n",
        "    return tensor # Return non-tensor types as is\n",
        "\n",
        "print_rank(\"[INFO] Using custom to_device implementation that accepts device argument.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "# --- FLUTE Base Classes Definitions ---\n",
        "# Include the definitions of the Base classes here as dependencies.\n",
        "# In a real FLUTE project, these would be imported from core.\n",
        "\n",
        "class BaseModel(ABC, T_nn.Module): # Inherit from torch.nn.Module\n",
        "    '''This is a wrapper class for PyTorch models.'''\n",
        "\n",
        "    @abstractmethod\n",
        "    def __init__(self,**kwargs):\n",
        "        super(BaseModel, self).__init__() # Initialize nn.Module\n",
        "\n",
        "    @abstractmethod\n",
        "    def loss(self, input):\n",
        "        '''Performs forward step and computes the loss'''\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def inference(self, input):\n",
        "        '''Performs forward step and computes metrics'''\n",
        "        pass\n",
        "\n",
        "    def set_eval(self):\n",
        "        '''Bring the model into evaluation mode'''\n",
        "        self.eval()\n",
        "\n",
        "    def set_train(self):\n",
        "        '''Bring the model into training mode'''\n",
        "        self.train()\n",
        "\n",
        "class BaseDataset(ABC, PyTorchDataset):\n",
        "    '''This is a wrapper class for PyTorch datasets.'''\n",
        "\n",
        "    @abstractmethod\n",
        "    def __init__(self,**kwargs):\n",
        "        super(BaseDataset, self).__init__()\n",
        "\n",
        "    @abstractmethod\n",
        "    def __getitem__(self, idx, **kwargs):\n",
        "        '''Fetches a data sample for a given key'''\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def __len__(self):\n",
        "        '''Returns the size of the dataset'''\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def load_data(self,**kwargs):\n",
        "        '''Wrapper method to read/instantiate the dataset'''\n",
        "        pass\n",
        "\n",
        "class BaseDataLoader(ABC, PyTorchDataLoader):\n",
        "    '''This is a wrapper class for PyTorch dataloaders.'''\n",
        "\n",
        "    def create_loader(self):\n",
        "        '''Returns the dataloader'''\n",
        "        return self\n",
        "\n",
        "# --- Dummy Data Setup ---\n",
        "# Define the directory structure simulating the drive location\n",
        "# IMPORTANT: If you are mounting Google Drive, the path will likely start with '/content/drive/MyDrive/'\n",
        "# Make sure PROJECT_ROOT is set correctly above if your notebook is not in the project root.\n",
        "DUMMY_DRIVE_DATA_DIR = \"./simulated_drive_data\"\n",
        "TRAIN_DATA_DIR = os.path.join(DUMMY_DRIVE_DATA_DIR, \"train_clients\") # Directory for client train files\n",
        "VAL_DATA_DIR = os.path.join(DUMMY_DRIVE_DATA_DIR, \"val_aggregated\") # Directory for aggregated val file\n",
        "TEST_DATA_DIR = os.path.join(DUMMY_DRIVE_DATA_DIR, \"test_aggregated\") # Directory for aggregated test file\n",
        "\n",
        "\n",
        "# Dummy data following the JSONL format you provided\n",
        "# Use a few different labels to test mapping\n",
        "# Ensure some clients have more than 3 samples\n",
        "DUMMY_CLIENT_DATA = {\n",
        "    \"client_0\": [\n",
        "        {\"text\": \"Symptoms: Fever, Cough, Fatigue.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Fever, Cough, Fatigue.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "        {\"text\": \"Extreme itchiness, nausea, and fatigue.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Extreme itchiness, nausea, and fatigue.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "        {\"text\": \"Feeling really tired and weak lately.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Feeling really tired and weak lately.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "        {\"text\": \"Another symptom description.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Another symptom description.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "        {\"text\": \"Fifth symptom entry.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Fifth symptom entry.. Diagnosis:\", \"completion\": \" Jaundice\"}, # More than 3\n",
        "    ],\n",
        "    \"client_1\": [\n",
        "        {\"text\": \"Shortness of breath and chest pain.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Shortness of breath and chest pain.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "        {\"text\": \"Yellow skin and eyes.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Yellow skin and eyes.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "        {\"text\": \"Third symptom for client 1.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Third symptom for client 1.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "        {\"text\": \"Fourth symptom for client 1.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Fourth symptom for client 1.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"}, # More than 3\n",
        "    ],\n",
        "    \"client_2\": [\n",
        "        {\"text\": \"High blood pressure readings.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: High blood pressure readings.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "        {\"text\": \"Feeling unwell.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Feeling unwell.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "        {\"text\": \"Coughing a lot.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Coughing a lot.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "    ],\n",
        "    \"client_3\": [\n",
        "         {\"text\": \"Headache and dizziness.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Headache and dizziness.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "         {\"text\": \"Only two samples here.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Only two samples here.. Diagnosis:\", \"completion\": \" Jaundice\"}, # Less than 3\n",
        "    ],\n",
        "    \"client_4\": [\n",
        "         {\"text\": \"Loss of appetite.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Loss of appetite.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "         {\"text\": \"Wheezing sound.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Wheezing sound.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "         {\"text\": \"Third sample for client 4.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Third sample for client 4.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "         {\"text\": \"Fourth sample for client 4.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Fourth sample for client 4.. Diagnosis:\", \"completion\": \" Jaundice\"}, # More than 3\n",
        "    ],\n",
        "    \"client_5\": [\n",
        "         {\"text\": \"Persistent cough.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Persistent cough.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "         {\"text\": \"Nausea and vomiting.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Nausea and vomiting.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "         {\"text\": \"Chest tightness.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Chest tightness.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "         {\"text\": \"Sample 4.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Sample 4.. Diagnosis:\", \"completion\": \" Hypertension\"}, # More than 3\n",
        "         {\"text\": \"Sample 5.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Sample 5.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "         {\"text\": \"Sample 6.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Sample 6.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Dummy data for validation/testing (aggregated)\n",
        "# This data simulates a single file containing eval data, not split by client\n",
        "DUMMY_AGGREGATED_EVAL_DATA = [\n",
        "    {\"text\": \"Evaluation symptom 1.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Evaluation symptom 1.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "    {\"text\": \"Evaluation symptom 2.\", \"label\": \"Jaundice\", \"prompt\": \"Symptoms: Evaluation symptom 2.. Diagnosis:\", \"completion\": \" Jaundice\"},\n",
        "    {\"text\": \"Evaluation symptom 3.\", \"label\": \"Bronchial Asthma\", \"prompt\": \"Symptoms: Evaluation symptom 3.. Diagnosis:\", \"completion\": \" Bronchial Asthma\"},\n",
        "    {\"text\": \"Evaluation symptom 4.\", \"label\": \"Hypertension\", \"prompt\": \"Symptoms: Evaluation symptom 4.. Diagnosis:\", \"completion\": \" Hypertension\"},\n",
        "]\n",
        "\n",
        "\n",
        "def create_dummy_client_data_files(base_dir, data_dict):\n",
        "    \"\"\"Creates dummy JSONL files for each client.\"\"\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    for client_id, data_entries in data_dict.items():\n",
        "        file_path = os.path.join(base_dir, f\"{client_id}.jsonl\")\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            for entry in data_entries:\n",
        "                f.write(json.dumps(entry) + '\\n')\n",
        "    print_rank(f\"Created dummy client data files in: {base_dir}\", loglevel=logging.INFO)\n",
        "\n",
        "def create_dummy_aggregated_data_file(base_dir, data_list, file_name=\"aggregated_eval_data.jsonl\"):\n",
        "    \"\"\"Creates a single dummy JSONL file for aggregated data.\"\"\"\n",
        "    os.makedirs(base_dir, exist_ok=True)\n",
        "    file_path = os.path.join(base_dir, file_name)\n",
        "    with open(file_path, 'w', encoding='utf-8') as f:\n",
        "        for entry in data_list:\n",
        "            f.write(json.dumps(entry) + '\\n')\n",
        "    print_rank(f\"Created dummy aggregated data file in: {base_dir}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "def cleanup_dummy_data():\n",
        "    \"\"\"Removes the dummy data directory.\"\"\"\n",
        "    if os.path.exists(DUMMY_DRIVE_DATA_DIR):\n",
        "        shutil.rmtree(DUMMY_DRIVE_DATA_DIR)\n",
        "        print_rank(f\"Cleaned up dummy data directory: {DUMMY_DRIVE_DATA_DIR}\", loglevel=logging.INFO)\n",
        "\n",
        "# --- BachelorDataset Class Definition ---\n",
        "# This class definition is now placed AFTER the BaseDataset definition\n",
        "\n",
        "# experiments/bachelor/dataloaders/dataset.py\n",
        "\n",
        "class BachelorDataset(BaseDataset):\n",
        "    \"\"\"\n",
        "    Custom PyTorch Dataset for the Bachelor classification task.\n",
        "    Loads data from JSONL files, tokenizes 'prompt' text, and maps string labels to integers.\n",
        "    Inherits from core.dataset.BaseDataset.\n",
        "    Handles loading either client-specific files or a single aggregated file.\n",
        "    \"\"\"\n",
        "    def __init__(self, data_path, args, tokenizer=None, test_only=False, user_idx=0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the dataset.\n",
        "\n",
        "        Args:\n",
        "            data_path (str): Path to the directory containing client JSONL files (for train)\n",
        "                             or the aggregated eval file (for val/test).\n",
        "                             This path will be provided via the config.yaml.\n",
        "            args (dict): Dictionary of arguments from config.yaml under the client_config or server_config\n",
        "                         data_config sections.\n",
        "            tokenizer (transformers.PreTrainedTokenizer, optional): Pre-trained tokenizer instance.\n",
        "                                                                     If None, it will be loaded from args.\n",
        "            test_only (bool, optional): True if this dataset instance is for evaluation/testing only (server-side).\n",
        "                                        Defaults to False (client-side training).\n",
        "            user_idx (int, optional): Index of the current user/client this dataset instance should load data for.\n",
        "                                      FLUTE provides this. -1 typically indicates aggregated data (e.g., server's test set).\n",
        "            **kwargs: Additional keyword arguments passed by FLUTE.\n",
        "        \"\"\"\n",
        "        # Initialize the BaseDataset (and thus PyTorchDataset)\n",
        "        super().__init__()\n",
        "\n",
        "        self.test_only = test_only\n",
        "        # Get max_seq_length from args, default to 512 if not specified\n",
        "        self.max_seq_length = args.get('max_seq_length', 512)\n",
        "        # Get padding preference from args, default to True\n",
        "        self.padding = args.get('padding', True)\n",
        "        self.user = None # Will store the current user ID processed by this dataset instance\n",
        "\n",
        "        # --- Tokenizer Initialization ---\n",
        "        # The tokenizer is essential for processing text data.\n",
        "        if tokenizer is not None:\n",
        "            self.tokenizer = tokenizer\n",
        "        else:\n",
        "            # Load tokenizer from Hugging Face based on config args\n",
        "            tokenizer_kwargs = {\n",
        "                \"cache_dir\": args.get('cache_dir', None), # Cache directory for downloaded models/tokenizers\n",
        "                \"revision\": args.get('model_revision', 'main'), # Use model_revision for tokenizer consistency\n",
        "                \"use_fast\": args.get('use_fast_tokenizer', True), # Default to using fast tokenizer\n",
        "                \"use_auth_token\": args.get('use_auth_token', None), # For accessing models requiring authentication\n",
        "            }\n",
        "            if 'tokenizer_name' in args and args['tokenizer_name']:\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(args['tokenizer_name'], **tokenizer_kwargs)\n",
        "            elif 'model_name_or_path' in args and args['model_name_or_path']:\n",
        "                 # Use model_name_or_path if tokenizer_name is not specified\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(args['model_name_or_path'], **tokenizer_kwargs)\n",
        "            else:\n",
        "                 # If no tokenizer name or model path, try loading from model_name\n",
        "                 self.tokenizer = AutoTokenizer.from_pretrained(args.get('model_name', 'bert-base-uncased'), **tokenizer_kwargs) # Use model_name from args\n",
        "\n",
        "\n",
        "        # Adjust max_seq_length based on the tokenizer's model_max_length\n",
        "        if self.max_seq_length > self.tokenizer.model_max_length:\n",
        "            print_rank(\n",
        "                f\"The max_seq_length passed ({self.max_seq_length}) is larger than the maximum length for the\"\n",
        "                f\"model ({self.tokenizer.model_max_length}). Picking {self.tokenizer.model_max_length} instead.\", loglevel=logging.WARNING\n",
        "            )\n",
        "            self.max_seq_length = self.tokenizer.model_max_length\n",
        "\n",
        "        # --- FLUTE Required Attributes (populated in load_data) ---\n",
        "        # These attributes are part of the BaseDataset contract with FLUTE.\n",
        "        # They are typically populated with raw or initially processed data structure overview.\n",
        "        self.user_list = []       # List of all user/client IDs found in the data directory (for train)\n",
        "        self.num_samples = []     # List containing the total number of samples for each user (in order of user_list)\n",
        "        # user_data and user_data_labels will store the raw prompts and labels per user (for train)\n",
        "        # or aggregated raw data (for eval), matching the structure expected by BaseDataset.\n",
        "        self.user_data = defaultdict(list)  # Dictionary storing raw 'prompt' strings per user {user_id: [prompt1, prompt2, ...]}\n",
        "        self.user_data_labels = defaultdict(list) # Dictionary storing raw 'label' strings per user {user_id: [label1, label2, ...]}\n",
        "\n",
        "\n",
        "        # --- Internal Dataset State ---\n",
        "        # Global label mapping (string label to integer ID) built during data loading\n",
        "        self.label_map = {}\n",
        "        self.current_label_id = 0\n",
        "        self.all_labels = [] # List to store all unique string labels found for consistent mapping\n",
        "\n",
        "        # Internal list to hold the *processed* (tokenized and integer-labeled) samples\n",
        "        # for the *current* dataset instance (either a single client or aggregated eval data).\n",
        "        # This is the list that __getitem__ will iterate over.\n",
        "        self.utt_list = []\n",
        "\n",
        "        # --- Data Loading and Processing ---\n",
        "        # Call the abstract method load_data to perform the actual data loading and initial processing\n",
        "        self.load_data(data_path, user_idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of processed samples in this dataset instance (self.utt_list).\n",
        "        This is required by PyTorch's Dataset class.\n",
        "        \"\"\"\n",
        "        return len(self.utt_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Fetches a single processed data sample for a given index.\n",
        "        This is required by PyTorch's Dataset class and used by the DataLoader.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing the tokenized input ('input_ids', 'attention_mask')\n",
        "                  and the integer label ('labels').\n",
        "        \"\"\"\n",
        "        # The data in self.utt_list is already tokenized and ready to be returned\n",
        "        return self.utt_list[idx]\n",
        "\n",
        "    def load_data(self, data_dir_path, user_idx):\n",
        "        \"\"\"\n",
        "        Reads the data from JSONL files.\n",
        "        If loading for clients, it finds client-specific files.\n",
        "        If loading aggregated eval data, it finds a single aggregated file.\n",
        "        Populates FLUTE's required attributes and processes data for the current instance.\n",
        "\n",
        "        This method implements the abstract load_data from BaseDataset.\n",
        "\n",
        "        Args:\n",
        "            data_dir_path (str): Path to the directory containing client JSONL files (for train)\n",
        "                                 or the aggregated eval file (for val/test).\n",
        "            user_idx (int): Index of the current user/client. -1 for aggregated test/val data.\n",
        "        \"\"\"\n",
        "        print_rank(f\"Loading data from directory: {data_dir_path}\", loglevel=logging.INFO)\n",
        "\n",
        "        raw_prompts_to_process = []\n",
        "        raw_labels_str_to_process = []\n",
        "\n",
        "        # Determine if we are loading client-specific data or aggregated data\n",
        "        # We assume client data is in a directory with files starting with 'client_'\n",
        "        # and aggregated data is a single file in its directory.\n",
        "        # Check for client files first\n",
        "        client_files_in_dir = [f for f in os.listdir(data_dir_path) if f.startswith('client_') and f.endswith('.jsonl')]\n",
        "        is_client_data_dir = len(client_files_in_dir) > 0\n",
        "\n",
        "        if is_client_data_dir and user_idx != -1 and not self.test_only:\n",
        "            # --- Scenario 1: Loading data for a specific client (Training) ---\n",
        "            print_rank(f\"Detected client data directory: {data_dir_path}\", loglevel=logging.DEBUG)\n",
        "            client_files = sorted(client_files_in_dir) # Use the list we already found\n",
        "\n",
        "            # Populate global user_list and num_samples, and collect raw data for all clients\n",
        "            for file_name in client_files:\n",
        "                file_path = os.path.join(data_dir_path, file_name)\n",
        "                client_id = os.path.splitext(file_name)[0]\n",
        "\n",
        "                self.user_list.append(client_id)\n",
        "                current_client_prompts = []\n",
        "                current_client_labels_str = []\n",
        "                num_samples_for_client = 0\n",
        "\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    for line in f:\n",
        "                        try:\n",
        "                            data_entry = json.loads(line.strip())\n",
        "                            prompt_text = data_entry.get('prompt')\n",
        "                            label_str = data_entry.get('label')\n",
        "\n",
        "                            if prompt_text is None or label_str is None:\n",
        "                                print_rank(f\"Skipping line missing 'prompt' or 'label' in {file_name}: {line.strip()}\", loglevel=logging.WARNING)\n",
        "                                continue\n",
        "\n",
        "                            # Build global label map (string label to integer ID)\n",
        "                            if label_str not in self.label_map:\n",
        "                                self.label_map[label_str] = self.current_label_id\n",
        "                                self.current_label_id += 1\n",
        "                                self.all_labels.append(label_str)\n",
        "\n",
        "                            current_client_prompts.append(prompt_text)\n",
        "                            current_client_labels_str.append(label_str)\n",
        "                            num_samples_for_client += 1\n",
        "                        except json.JSONDecodeError as e:\n",
        "                            print_rank(f\"Skipping malformed JSONL line in {file_name}: {line.strip()} - Error: {e}\", loglevel=logging.WARNING)\n",
        "\n",
        "\n",
        "                self.user_data[client_id] = current_client_prompts\n",
        "                self.user_data_labels[client_id] = current_client_labels_str\n",
        "                self.num_samples.append(num_samples_for_client)\n",
        "\n",
        "            print_rank(f\"Global label map created: {self.label_map}\", loglevel=logging.INFO)\n",
        "            print_rank(f\"Discovered {len(self.user_list)} clients with sample counts: {self.num_samples}\", loglevel=logging.INFO)\n",
        "\n",
        "            # Select data for the specific client instance\n",
        "            if user_idx < 0 or user_idx >= len(self.user_list):\n",
        "                 raise IndexError(f\"user_idx {user_idx} is out of range for {len(self.user_list)} clients.\")\n",
        "\n",
        "            self.user = self.user_list[user_idx]\n",
        "            raw_prompts_to_process = self.user_data[self.user]\n",
        "            raw_labels_str_to_process = self.user_data_labels[self.user]\n",
        "            print_rank(f\"Processing training data for client: {self.user}\", loglevel=logging.INFO)\n",
        "\n",
        "        else:\n",
        "            # --- Scenario 2: Loading aggregated data (Evaluation/Testing or user_idx == -1) ---\n",
        "            print_rank(f\"Detected aggregated data directory: {data_dir_path}\", loglevel=logging.DEBUG)\n",
        "            # We expect a single aggregated file in this directory.\n",
        "            # Find the first JSONL file in the directory that does NOT start with 'client_'\n",
        "            aggregated_files = [f for f in os.listdir(data_dir_path) if f.endswith('.jsonl') and not f.startswith('client_')]\n",
        "\n",
        "            if len(aggregated_files) != 1:\n",
        "                raise FileNotFoundError(f\"Expected exactly one aggregated JSONL file (not starting with 'client_') in {data_dir_path} but found {len(aggregated_files)}.\")\n",
        "\n",
        "            file_name = aggregated_files[0]\n",
        "            file_path = os.path.join(data_dir_path, file_name)\n",
        "\n",
        "            self.user = 'aggregated_eval' # Assign a descriptive user ID for aggregated data\n",
        "            print_rank(f\"Processing aggregated evaluation data from {file_path}.\", loglevel=logging.INFO)\n",
        "\n",
        "            # Read all data from the single aggregated file\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        data_entry = json.loads(line.strip())\n",
        "                        prompt_text = data_entry.get('prompt')\n",
        "                        label_str = data_entry.get('label')\n",
        "\n",
        "                        if prompt_text is None or label_str is None:\n",
        "                            print_rank(f\"Skipping line missing 'prompt' or 'label' in {file_path}: {line.strip()}\", loglevel=logging.WARNING)\n",
        "                            continue\n",
        "\n",
        "                        # Build global label map (string label to integer ID)\n",
        "                        # This ensures eval data labels are also mapped consistently\n",
        "                        if label_str not in self.label_map:\n",
        "                            self.label_map[label_str] = self.current_label_id\n",
        "                            self.current_label_id += 1\n",
        "                            self.all_labels.append(label_str)\n",
        "\n",
        "                        raw_prompts_to_process.append(prompt_text)\n",
        "                        raw_labels_str_to_process.append(label_str)\n",
        "                    except json.JSONDecodeError as e:\n",
        "                        print_rank(f\"Skipping malformed JSONL line in {file_path}: {line.strip()} - Error: {e}\", loglevel=logging.WARNING)\n",
        "\n",
        "            # For aggregated data, user_list and num_samples might not be strictly necessary\n",
        "            # in the same format as for client data, but we can populate them conceptually.\n",
        "            self.user_list = ['aggregated_eval']\n",
        "            self.num_samples = [len(raw_prompts_to_process)]\n",
        "            # Store aggregated raw data under the 'aggregated_eval' key\n",
        "            self.user_data['aggregated_eval'] = raw_prompts_to_process\n",
        "            self.user_data_labels['aggregated_eval'] = raw_labels_str_to_process\n",
        "\n",
        "            print_rank(f\"Global label map created: {self.label_map}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "        # --- Step 3: Process the collected raw data for the current instance ---\n",
        "        # Tokenize prompts and map string labels to integer IDs\n",
        "        self.process_user_data(raw_prompts_to_process, raw_labels_str_to_process)\n",
        "\n",
        "        print_rank(f\"Dataset for user '{self.user}' initialized with {len(self.utt_list)} processed samples.\", loglevel=logging.DEBUG)\n",
        "\n",
        "    def process_user_data(self, prompts, labels_str):\n",
        "        \"\"\"\n",
        "        Tokenizes the raw prompt text data and prepares it into the format expected by __getitem__.\n",
        "        Maps string labels to integer IDs.\n",
        "        Populates self.utt_list with the processed samples for the current user/aggregation.\n",
        "\n",
        "        Args:\n",
        "            prompts (list): List of raw prompt strings for the current user/aggregation.\n",
        "            labels_str (list): List of raw string labels for the current user/aggregation.\n",
        "        \"\"\"\n",
        "        if not prompts:\n",
        "            print_rank(f\"Warning: No raw data provided for user {self.user}. Appending dummy sample.\", loglevel=logging.WARNING)\n",
        "            # Append a dummy sample if a user has no data to avoid issues with DataLoader\n",
        "            self.utt_list.append({\n",
        "                'input_ids': [self.tokenizer.cls_token_id, self.tokenizer.sep_token_id] if self.tokenizer.cls_token_id is not None and self.tokenizer.sep_token_id is not None else [0, 0],\n",
        "                'attention_mask': [1, 1],\n",
        "                'labels': 0 # Default label (assuming 0 is a valid class ID)\n",
        "            })\n",
        "            return\n",
        "\n",
        "        # --- Tokenization ---\n",
        "        # Use the Hugging Face tokenizer to convert text prompts into token IDs and attention masks.\n",
        "        tokenized_batch = self.tokenizer(\n",
        "            prompts,\n",
        "            max_length=self.max_seq_length, # Truncate to max_seq_length\n",
        "            truncation=True,\n",
        "            padding=False, # DO NOT pad here. DataCollatorWithPadding in DataLoader handles batch padding.\n",
        "            return_tensors=None # Return as Python lists, not PyTorch tensors yet.\n",
        "        )\n",
        "\n",
        "        # --- Prepare Final Samples ---\n",
        "        # Create the list of dictionaries, where each dictionary is a single sample\n",
        "        # containing tokenized inputs and integer labels.\n",
        "        self.utt_list = []\n",
        "        for i in range(len(prompts)):\n",
        "            # Ensure we have input_ids and attention_mask for each sample\n",
        "            input_ids = tokenized_batch['input_ids'][i] if 'input_ids' in tokenized_batch else []\n",
        "            attention_mask = tokenized_batch['attention_mask'][i] if 'attention_mask' in tokenized_batch else []\n",
        "\n",
        "            if not input_ids:\n",
        "                 print_rank(f\"Warning: Tokenization resulted in empty input_ids for a sample from user {self.user}. Skipping sample.\", loglevel=logging.WARNING)\n",
        "                 continue # Skip samples that resulted in empty tokenization\n",
        "\n",
        "            # Convert string label to integer ID using the global label map\n",
        "            # Use .get() with a default in case a label appears in eval data but not train (less likely with global map)\n",
        "            label_id = self.label_map.get(labels_str[i], 0) # Default to 0 if label not found\n",
        "\n",
        "            self.utt_list.append({\n",
        "                'input_ids': input_ids,\n",
        "                'attention_mask': attention_mask,\n",
        "                'labels': label_id # Integer ID for the classification target\n",
        "                # Add other potential fields if needed by the model (e.g., 'token_type_ids')\n",
        "            })\n",
        "\n",
        "        if not self.utt_list:\n",
        "             print_rank(f\"Warning: All samples for user {self.user} were skipped after processing. Appending dummy sample.\", loglevel=logging.WARNING)\n",
        "             # Append dummy if processing resulted in no valid samples\n",
        "             self.utt_list.append({\n",
        "                'input_ids': [self.tokenizer.cls_token_id, self.tokenizer.sep_token_id] if self.tokenizer.cls_token_id is not None and self.tokenizer.sep_token_id is not None else [0, 0],\n",
        "                'attention_mask': [1, 1],\n",
        "                'labels': 0\n",
        "            })\n",
        "\n",
        "\n",
        "# --- BachelorDataLoader Class Definition ---\n",
        "# This class definition is placed AFTER the BaseDataLoader definition\n",
        "\n",
        "# experiments/bachelor/dataloaders/dataloader.py\n",
        "\n",
        "class BachelorDataLoader(BaseDataLoader):\n",
        "    \"\"\"\n",
        "    PyTorch DataLoader for the Bachelor classification task.\n",
        "    Instantiates BachelorDataset and uses DataCollatorWithPadding.\n",
        "    Inherits from core.dataloader.BaseDataLoader and torch.utils.data.DataLoader.\n",
        "    \"\"\"\n",
        "    def __init__(self, mode, data, num_workers=0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the DataLoader.\n",
        "\n",
        "        Args:\n",
        "            mode (str): The mode of the dataloader ('train', 'val', or 'test').\n",
        "                        FLUTE provides this.\n",
        "            data: This will be the path to the data directory containing client files (train)\n",
        "                  or the aggregated eval file (val/test), as specified in config.yaml.\n",
        "            num_workers (int, optional): Number of subprocesses to use for data loading.\n",
        "                                         Defaults to 0 (main process).\n",
        "            **kwargs: Contains the 'args' dictionary from config.yaml.\n",
        "        \"\"\"\n",
        "        # Add print statements to inspect kwargs and args\n",
        "        # print_rank(f\"DataLoader __init__ called with mode={mode}, data={data}, num_workers={num_workers}\", loglevel=logging.DEBUG)\n",
        "        # print_rank(f\"DataLoader __init__ kwargs: {kwargs}\", loglevel=logging.DEBUG) # Inspect kwargs\n",
        "\n",
        "        # Store mode and num_workers\n",
        "        self.mode = mode\n",
        "        self.num_workers = num_workers\n",
        "        self.utt_ids = None # Will store the user ID associated with the underlying dataset\n",
        "\n",
        "        # Get configuration arguments\n",
        "        args = kwargs.get('args', {})\n",
        "        # print_rank(f\"DataLoader __init__ args: {args}\", loglevel=logging.DEBUG) # Inspect args\n",
        "\n",
        "        # Get batch_size from args, default to 4 if not specified\n",
        "        self.batch_size = args.get('batch_size', 4)\n",
        "        # print_rank(f\"DataLoader __init__ setting batch_size to: {self.batch_size}\", loglevel=logging.DEBUG) # Inspect final batch_size\n",
        "\n",
        "\n",
        "        # --- Tokenizer Initialization ---\n",
        "        # Initialize the tokenizer. It's important that this matches the one used in the Dataset\n",
        "        # to ensure consistent tokenization and vocabulary.\n",
        "        tokenizer_kwargs = {\n",
        "            \"cache_dir\": args.get('cache_dir', None),\n",
        "            \"revision\": args.get('model_revision', 'main'), # Use model_revision for tokenizer consistency\n",
        "            \"use_fast\": args.get('use_fast_tokenizer', True), # Default to using fast tokenizer\n",
        "            \"use_auth_token\": args.get('use_auth_token', None),\n",
        "        }\n",
        "        if 'tokenizer_name' in args and args['tokenizer_name']:\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(args['tokenizer_name'], **tokenizer_kwargs)\n",
        "        elif 'model_name_or_path' in args and args['model_name_or_path']:\n",
        "            # print_rank(f\"Loading Tokenizer from Pretrained: {args['model_name_or_path']}\", loglevel=logging.INFO)\n",
        "            self.tokenizer = AutoTokenizer.from_pretrained(args['model_name_or_path'], **tokenizer_kwargs)\n",
        "        else:\n",
        "             # If no tokenizer name or model path, try loading from model_name\n",
        "             self.tokenizer = AutoTokenizer.from_pretrained(args.get('model_name', 'bert-base-uncased'), **tokenizer_kwargs) # Use model_name from args\n",
        "\n",
        "\n",
        "        # print_rank(f\"DataLoader using tokenizer: {self.tokenizer.name_or_path}\", loglevel=logging.DEBUG)\n",
        "\n",
        "        # --- Dataset Instantiation ---\n",
        "        # Instantiate our custom BachelorDataset.\n",
        "        # The 'data' argument passed here is the data_dir_path from config.yaml.\n",
        "        # user_idx is passed by FLUTE to the DataLoader's constructor, and we pass it along to the Dataset.\n",
        "        user_idx = kwargs.get('user_idx', -1)\n",
        "        self.dataset = BachelorDataset(\n",
        "            data_path=data, # Pass the data directory path\n",
        "            args=args,      # Pass the configuration arguments\n",
        "            tokenizer=self.tokenizer, # Pass the initialized tokenizer\n",
        "            test_only=(self.mode != 'train'), # Set test_only flag based on mode\n",
        "            user_idx=user_idx, # Pass the user_idx provided by FLUTE\n",
        "        )\n",
        "        # Get the user ID from the dataset instance (useful for logging/tracking)\n",
        "        self.utt_ids = self.dataset.user\n",
        "\n",
        "        # --- Data Collator ---\n",
        "        # Use Hugging Face's DataCollatorWithPadding. This is crucial for handling batches\n",
        "        # of sequences with varying lengths. It pads sequences to the length of the longest\n",
        "        # sequence in the current batch and creates the appropriate attention mask.\n",
        "        self.data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer, padding='longest')\n",
        "\n",
        "        # --- Sampler Selection ---\n",
        "        # Choose sampler based on the mode. RandomSampler for training, SequentialSampler for evaluation.\n",
        "        if self.mode == 'train':\n",
        "            sampler = RandomSampler(self.dataset)\n",
        "        elif self.mode == 'val' or self.mode == 'test':\n",
        "            sampler = SequentialSampler(self.dataset)\n",
        "        else:\n",
        "            raise Exception(f\"Invalid mode parameter: {self.mode}. Must be 'train', 'val', or 'test'.\")\n",
        "\n",
        "        # --- PyTorch DataLoader Initialization ---\n",
        "        # Finally, initialize the standard PyTorch DataLoader using the dataset, sampler, batch size,\n",
        "        # and our custom data collator.\n",
        "        super(BachelorDataLoader, self).__init__(\n",
        "            self.dataset,\n",
        "            batch_size=self.batch_size, # Use the batch_size determined above\n",
        "            sampler=sampler,\n",
        "            collate_fn=self.data_collator, # Use the data collator for batch processing\n",
        "            drop_last=False, # Whether to drop the last incomplete batch (usually False for eval)\n",
        "            num_workers=self.num_workers, # Number of worker processes for data loading\n",
        "            pin_memory=True, # Pin memory for faster data transfer to GPU (useful for future GPU use)\n",
        "        )\n",
        "\n",
        "    def get_user(self):\n",
        "        \"\"\"\n",
        "        Returns the user ID associated with the underlying dataset instance.\n",
        "        Useful for FLUTE's internal tracking.\n",
        "        \"\"\"\n",
        "        return self.utt_ids\n",
        "\n",
        "    # The create_loader method from BaseDataLoader is inherited and simply returns self.\n",
        "    # def create_loader(self):\n",
        "    #     return self\n",
        "\n",
        "\n",
        "# --- BachelorModel Class Definition ---\n",
        "# This class definition is placed AFTER the BaseModel definition\n",
        "\n",
        "# experiments/bachelor/models/model.py\n",
        "\n",
        "class BachelorModel(BaseModel):\n",
        "    \"\"\"\n",
        "    FLUTE BaseModel wrapper for Hugging Face AutoModelForSequenceClassification.\n",
        "    Configured for the Bachelor text classification task.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the BachelorModel.\n",
        "\n",
        "        Args:\n",
        "            model_config (dict): Dictionary containing model-specific configuration\n",
        "                                 from config.yaml (e.g., model_config['BachelorModel']).\n",
        "            **kwargs: Additional keyword arguments passed by FLUTE.\n",
        "                      Expected to include 'num_labels'.\n",
        "        \"\"\"\n",
        "        super(BachelorModel, self).__init__() # Initialize the BaseModel (and thus nn.Module)\n",
        "\n",
        "        # Extract configuration parameters specific to this model\n",
        "        # Assuming model_config['BachelorModel'] holds the relevant args\n",
        "        args = model_config.get('BachelorModel', {})\n",
        "        if not args:\n",
        "             raise ValueError(\"Model configuration for 'BachelorModel' not found in model_config.\")\n",
        "\n",
        "        # Split configuration into model and training arguments\n",
        "        model_args = args.get('model', {})\n",
        "        training_args = args.get('training', {})\n",
        "\n",
        "        # Set seed for reproducibility\n",
        "        set_seed(training_args.get('seed', 42)) # Default seed to 42 if not specified\n",
        "\n",
        "        # Get necessary parameters from config\n",
        "        self.model_name = model_args.get('model_name', 'bert-base-uncased') # Default model name\n",
        "        # gradient_accumulation_steps, eval_accumulation_steps, batch_size\n",
        "        # are typically handled by the FLUTE trainer/orchestrator, not the BaseModel itself.\n",
        "        # self.gradient_accumulation_steps = training_args.get('gradient_accumulation_steps', 1)\n",
        "        # self.eval_accumulation_steps = training_args.get('eval_accumulation_steps', None)\n",
        "        # self.batch_size = training_args.get('batch_size', 4)\n",
        "\n",
        "\n",
        "        # --- Determine number of labels ---\n",
        "        # The number of output labels for the classification head must match the dataset.\n",
        "        # This information is available in the dataset's label_map after it's loaded.\n",
        "        # FLUTE is expected to pass 'num_labels' via kwargs when instantiating the model.\n",
        "        self.num_labels = kwargs.get('num_labels')\n",
        "        if self.num_labels is None:\n",
        "             # Fallback: try to get it from model_args if explicitly set there,\n",
        "             # but getting it from the dataset via kwargs is preferred and more robust.\n",
        "             self.num_labels = model_args.get('num_labels')\n",
        "             if self.num_labels is None:\n",
        "                  # If still not available, raise an error to ensure correct setup.\n",
        "                  raise ValueError(\"Number of classification labels ('num_labels') must be provided to the Model constructor (preferably via kwargs from FLUTE).\")\n",
        "\n",
        "        print_rank(f\"Model expects {self.num_labels} classification labels.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "        # --- Load Model Configuration ---\n",
        "        # Load the configuration for the pre-trained model\n",
        "        config_kwargs = {\n",
        "            \"cache_dir\": model_args.get('cache_dir', None),\n",
        "            \"revision\": model_args.get('model_revision', 'main'), # Default revision\n",
        "            \"use_auth_token\": model_args.get('use_auth_token', None),\n",
        "            \"num_labels\": self.num_labels # Pass the number of labels to the config\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try loading config by name or path\n",
        "            if 'config_name' in model_args and model_args['config_name']:\n",
        "                config = AutoConfig.from_pretrained(model_args['config_name'], **config_kwargs)\n",
        "            elif 'model_name_or_path' in model_args and model_args['model_name_or_path']:\n",
        "                config = AutoConfig.from_pretrained(model_args['model_name_or_path'], **config_kwargs)\n",
        "            else:\n",
        "                 # If no config name or model path, try loading from model_name\n",
        "                 config = AutoConfig.from_pretrained(self.model_name, **config_kwargs)\n",
        "\n",
        "        except Exception as e:\n",
        "             raise ValueError(f\"Failed to load model configuration for {self.model_name}: {e}\")\n",
        "\n",
        "\n",
        "        # --- Load Tokenizer ---\n",
        "        # Load the tokenizer associated with the model\n",
        "        tokenizer_kwargs = {\n",
        "            \"cache_dir\": model_args.get('cache_dir', None),\n",
        "            \"revision\": model_args.get('model_revision', 'main'), # Use model_revision for tokenizer consistency\n",
        "            \"use_fast\": model_args.get('use_fast_tokenizer', True), # Default to using fast tokenizer\n",
        "            \"use_auth_token\": model_args.get('use_auth_token', None),\n",
        "        }\n",
        "        try:\n",
        "            if 'tokenizer_name' in model_args and model_args['tokenizer_name']:\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(model_args['tokenizer_name'], **tokenizer_kwargs)\n",
        "            elif 'model_name_or_path' in model_args and model_args['model_name_or_path']:\n",
        "                print_rank(f\"Loading Tokenizer from Pretrained: {model_args['model_name_or_path']}\", loglevel=logging.INFO)\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(model_args['model_name_or_path'], **tokenizer_kwargs)\n",
        "            else:\n",
        "                 # If no tokenizer name or model path, try loading from model_name\n",
        "                 self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, **tokenizer_kwargs)\n",
        "\n",
        "        except Exception as e:\n",
        "             raise ValueError(f\"Failed to load tokenizer for {self.model_name}: {e}\")\n",
        "\n",
        "\n",
        "        # --- Load the Model ---\n",
        "        # Load the pre-trained model for sequence classification\n",
        "        try:\n",
        "            if 'model_name_or_path' in model_args and model_args['model_name_or_path']:\n",
        "                print_rank(f\"Loading Model from Pretrained: {model_args['model_name_or_path']}\", loglevel=logging.INFO)\n",
        "                self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    model_args['model_name_or_path'],\n",
        "                    from_tf=False, # Set to True if loading from a TensorFlow checkpoint\n",
        "                    config=config, # Use the configured config with num_labels\n",
        "                    cache_dir=model_args.get('cache_dir', None),\n",
        "                    use_auth_token=model_args.get('use_auth_token', None),\n",
        "                )\n",
        "            else:\n",
        "                 # If no model_name_or_path, try loading from model_name with the configured config\n",
        "                 self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    config=config, # Use the configured config with num_labels\n",
        "                    cache_dir=model_args.get('cache_dir', None),\n",
        "                    use_auth_token=model_args.get('use_auth_token', None),\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "             raise ValueError(f\"Failed to load model {self.model_name} for sequence classification: {e}\")\n",
        "\n",
        "\n",
        "        # Resize token embeddings if the tokenizer's vocabulary size is different\n",
        "        # from the model's embedding size (e.g., after adding special tokens)\n",
        "        if len(self.tokenizer) != self.model.get_input_embeddings().weight.shape[0]:\n",
        "             print_rank(f\"Resizing token embeddings from {self.model.get_input_embeddings().weight.shape[0]} to {len(self.tokenizer)}\", loglevel=logging.INFO)\n",
        "             self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "\n",
        "        # --- PEFT Integration (Placeholder for now) ---\n",
        "        # This is where PEFT (e.g., LoRA) would be applied *after* loading the base model.\n",
        "        # We will add this later as requested.\n",
        "        # Example (LoRA):\n",
        "        # from peft import LoraConfig, get_peft_model\n",
        "        # lora_config = LoraConfig(...) # Define LoRA config based on model_args\n",
        "        # self.model = get_peft_model(self.model, lora_config)\n",
        "        # self.model.print_trainable_parameters() # Check trainable parameters\n",
        "\n",
        "\n",
        "        # --- Pruning Integration (Placeholder for now) ---\n",
        "        # Pruning setup would typically happen here after loading the model.\n",
        "        # We will add this later as requested.\n",
        "\n",
        "\n",
        "        # --- Log parameter counts ---\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "        print_rank(f\"Total model parameters: {total_params}\", loglevel=logging.INFO)\n",
        "        print_rank(f\"Trainable model parameters: {trainable_params}\", loglevel=logging.INFO)\n",
        "\n",
        "        # If PEFT is applied, trainable_params will be much smaller than total_params.\n",
        "        # If not using PEFT, trainable_params should equal total_params (unless some layers are frozen).\n",
        "        if total_params > 0:\n",
        "             print_rank(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "        # Move model to device (CPU/GPU)\n",
        "        self.device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        print_rank(f\"Model moved to device: {self.device}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def copy_state_dict(self, state_dict):\n",
        "        \"\"\"\n",
        "        Copies the provided state_dict into the model's state_dict.\n",
        "        Required by BaseModel for federated learning updates.\n",
        "        \"\"\"\n",
        "        # Ensure the state_dict is on the correct device before loading\n",
        "        state_dict = to_device(state_dict, self.device)\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        print_rank(\"Model state_dict updated.\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        \"\"\"\n",
        "        Returns the underlying PyTorch model instance.\n",
        "        Required by BaseModel.\n",
        "        \"\"\"\n",
        "        return self.model\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the model.\n",
        "        Args are typically a batch dictionary from the DataLoader.\n",
        "        \"\"\"\n",
        "        # Move inputs to the correct device\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        # Pass inputs to the Hugging Face model\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs # Returns model output (logits, loss if labels provided)\n",
        "\n",
        "    def loss(self, inputs):\n",
        "        \"\"\"\n",
        "        Calculates the loss for a batch of inputs.\n",
        "        Required by BaseModel for training.\n",
        "        \"\"\"\n",
        "        # The Hugging Face AutoModelForSequenceClassification calculates loss\n",
        "        # automatically if 'labels' are provided in the inputs.\n",
        "        # We just need to ensure inputs are on the correct device and pass them.\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # Perform forward pass; loss is returned in the output if labels are present\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        # Extract the loss tensor\n",
        "        # The loss is typically the first element or accessed via .loss\n",
        "        loss = outputs.loss if hasattr(outputs, 'loss') and outputs.loss is not None else outputs[0]\n",
        "\n",
        "        # Apply gradient accumulation scaling if needed (though FLUTE handles this differently,\n",
        "        # keeping the structure might be useful if adapting parts of Hugging Face Trainer)\n",
        "        # loss = loss / self.gradient_accumulation_steps # FLUTE's optimizer handles accumulation\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n",
        "        \"\"\"\n",
        "        Prepare inputs by moving tensors to the correct device.\n",
        "        \"\"\"\n",
        "        # Move all tensor inputs to the model's device\n",
        "        for k, v in inputs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                # Call the local to_device function which we've ensured accepts the device argument\n",
        "                inputs[k] = to_device(v, self.device)\n",
        "        return inputs\n",
        "\n",
        "\n",
        "    def inference(\n",
        "            self, inputs, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = \"eval\"\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Performs inference (forward pass without gradient calculation) and returns metrics.\n",
        "        Required by BaseModel for evaluation.\n",
        "\n",
        "        Args:\n",
        "            inputs (Dict[str, Union[torch.Tensor, Any]]): Batch of inputs from the DataLoader.\n",
        "            ignore_keys (List[str], optional): Keys to ignore in model output.\n",
        "            metric_key_prefix (str): Prefix for metric keys (e.g., 'eval').\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary of computed metrics (e.g., loss, accuracy).\n",
        "        \"\"\"\n",
        "        self.model.eval() # Set model to evaluation mode\n",
        "\n",
        "        # Move inputs to the correct device\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # Perform forward pass without calculating gradients\n",
        "        with T.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Extract logits and labels\n",
        "        # Logits are typically the second element or accessed via .logits\n",
        "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[1]\n",
        "        labels = inputs.get('labels') # Get labels from the input batch\n",
        "\n",
        "        metrics = {}\n",
        "        # Calculate loss if labels are available\n",
        "        if labels is not None:\n",
        "            # The model might return loss directly if labels were provided\n",
        "            loss = outputs.loss if hasattr(outputs, 'loss') and outputs.loss is not None else None\n",
        "            if loss is not None:\n",
        "                 metrics[f\"{metric_key_prefix}_loss\"] = loss.mean().item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            # Get predicted class IDs by finding the max logit\n",
        "            predictions = T.argmax(logits, dim=-1)\n",
        "            # Compare predictions to true labels\n",
        "            correct_predictions = (predictions == labels).sum().item()\n",
        "            total_samples = labels.size(0)\n",
        "            accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
        "            metrics[f\"{metric_key_prefix}_accuracy\"] = accuracy\n",
        "\n",
        "        # Return metrics dictionary\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    # The prediction_loop and prediction_step methods from the example\n",
        "    # are more complex and often part of a Hugging Face Trainer.\n",
        "    # For a basic FLUTE BaseModel, the inference method should be sufficient\n",
        "    # to calculate metrics on a batch. We don't need to replicate the full Trainer loop here.\n",
        "    # If more complex evaluation logic is needed (e.g., gathering predictions across batches/devices),\n",
        "    # that might be handled outside the BaseModel or within a custom evaluation function.\n",
        "\n",
        "    # The floating_point_ops method is for logging FLOPS and is optional for basic setup.\n",
        "    # def floating_point_ops(self, inputs):\n",
        "    #     ...\n",
        "\n",
        "    def set_eval(self):\n",
        "        \"\"\"\n",
        "        Sets the underlying PyTorch model to evaluation mode.\n",
        "        Required by BaseModel.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        print_rank(\"Model set to evaluation mode.\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "    def set_train(self):\n",
        "        \"\"\"\n",
        "        Sets the underlying PyTorch model to training mode.\n",
        "        Required by BaseModel.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        print_rank(\"Model set to training mode.\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "# --- Test Classes ---\n",
        "\n",
        "class TestBachelorDataset(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"Set up dummy data simulating drive access before running tests.\"\"\"\n",
        "        cleanup_dummy_data() # Ensure cleanup before creating new data\n",
        "        create_dummy_client_data_files(TRAIN_DATA_DIR, DUMMY_CLIENT_DATA)\n",
        "        create_dummy_aggregated_data_file(VAL_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"val_data.jsonl\")\n",
        "        create_dummy_aggregated_data_file(TEST_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"test_data.jsonl\")\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"Clean up dummy data after running tests.\"\"\"\n",
        "        cleanup_dummy_data()\n",
        "\n",
        "    def test_dataset_client_loading(self):\n",
        "        \"\"\"Tests loading data for individual clients from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataset Client Loading (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        # Dummy args similar to what would come from config.yaml\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased', # Or 'BioClinicalBERT' if available publicly\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "        }\n",
        "\n",
        "        # Test loading for each client (0 to 5)\n",
        "        for client_idx in range(6):\n",
        "            client_id = f\"client_{client_idx}\"\n",
        "            print_rank(f\"Testing client: {client_id}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Pass the directory path containing all client files\n",
        "            dataset = BachelorDataset(\n",
        "                data_path=TRAIN_DATA_DIR, # Pass the directory path\n",
        "                args=dummy_args,\n",
        "                test_only=False,\n",
        "                user_idx=client_idx # FLUTE provides the user_idx\n",
        "            )\n",
        "\n",
        "            # Assertions\n",
        "            expected_sample_count = len(DUMMY_CLIENT_DATA.get(client_id, []))\n",
        "            self.assertEqual(len(dataset), expected_sample_count, f\"Dataset length mismatch for {client_id}\")\n",
        "            self.assertEqual(dataset.user, client_id, f\"Dataset user ID mismatch for {client_id}\")\n",
        "            self.assertIsNotNone(dataset.tokenizer)\n",
        "            self.assertGreater(len(dataset.label_map), 0, \"Label map should not be empty\")\n",
        "            # Check that user_list and num_samples contain info for ALL clients in the directory\n",
        "            self.assertEqual(len(dataset.user_list), len(DUMMY_CLIENT_DATA), \"user_list should contain all client IDs\")\n",
        "            self.assertEqual(len(dataset.num_samples), len(DUMMY_CLIENT_DATA), \"num_samples should contain counts for all clients\")\n",
        "\n",
        "            if expected_sample_count > 0:\n",
        "                # Test __getitem__ for the first sample if data exists\n",
        "                sample = dataset[0]\n",
        "                self.assertIn('input_ids', sample)\n",
        "                self.assertIn('attention_mask', sample)\n",
        "                self.assertIn('labels', sample)\n",
        "                self.assertIsInstance(sample['input_ids'], list)\n",
        "                self.assertIsInstance(sample['attention_mask'], list)\n",
        "                self.assertIsInstance(sample['labels'], int)\n",
        "                self.assertGreater(len(sample['input_ids']), 0, f\"Tokenization failed for a sample in {client_id}\")\n",
        "                self.assertIn(sample['labels'], dataset.label_map.values(), f\"Label mapping failed for a sample in {client_id}\")\n",
        "\n",
        "            print_rank(f\"Successfully loaded and tested data for {client_id}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def test_dataset_aggregated_loading(self):\n",
        "        \"\"\"Tests loading aggregated data from simulated drive file.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataset Aggregated Loading (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "        }\n",
        "\n",
        "        # Test loading aggregated validation data (user_idx=-1, test_only=True)\n",
        "        # The data_path points to the directory containing the single aggregated file\n",
        "        aggregated_dataset_val = BachelorDataset(\n",
        "            data_path=VAL_DATA_DIR, # Pass the directory path\n",
        "            args=dummy_args,\n",
        "            test_only=True, # Indicates this is for evaluation\n",
        "            user_idx=-1 # Indicates aggregated data\n",
        "        )\n",
        "\n",
        "        total_eval_samples = len(DUMMY_AGGREGATED_EVAL_DATA)\n",
        "        self.assertEqual(len(aggregated_dataset_val), total_eval_samples, \"Aggregated dataset length mismatch\")\n",
        "        self.assertEqual(aggregated_dataset_val.user, 'aggregated_eval')\n",
        "        self.assertIsNotNone(aggregated_dataset_val.tokenizer)\n",
        "        self.assertGreater(len(aggregated_dataset_val.label_map), 0, \"Label map should not be empty for aggregated data\")\n",
        "\n",
        "        if total_eval_samples > 0:\n",
        "             # Test __getitem__ for aggregated data\n",
        "            sample = aggregated_dataset_val[0]\n",
        "            self.assertIn('input_ids', sample)\n",
        "            self.assertIn('labels', sample)\n",
        "            self.assertIsInstance(sample['labels'], int)\n",
        "            self.assertIn(sample['labels'], aggregated_dataset_val.label_map.values())\n",
        "\n",
        "        print_rank(f\"Successfully loaded and tested aggregated validation data ({total_eval_samples} samples)\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "class TestBachelorDataLoader(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"Set up dummy data simulating drive access before running tests.\"\"\"\n",
        "        cleanup_dummy_data()\n",
        "        create_dummy_client_data_files(TRAIN_DATA_DIR, DUMMY_CLIENT_DATA)\n",
        "        create_dummy_aggregated_data_file(VAL_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"val_data.jsonl\")\n",
        "        create_dummy_aggregated_data_file(TEST_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"test_data.jsonl\")\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"Clean up dummy data after running tests.\"\"\"\n",
        "        cleanup_dummy_data()\n",
        "\n",
        "    def test_dataloader_client_train(self):\n",
        "        \"\"\"Tests DataLoader for a client in training mode from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataLoader Client Train (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        # Dummy args including batch size\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "            'batch_size': 2, # Use a small batch size for testing\n",
        "        }\n",
        "\n",
        "        # Test DataLoader for client 0\n",
        "        client_idx = 0\n",
        "        client_id = f\"client_{client_idx}\"\n",
        "        dataloader = BachelorDataLoader(\n",
        "            mode='train',\n",
        "            data=TRAIN_DATA_DIR, # Pass the directory path\n",
        "            num_workers=0, # Use 0 workers for simplicity in testing\n",
        "            args=dummy_args,\n",
        "            user_idx=client_idx # Specify the client index\n",
        "        )\n",
        "\n",
        "        # Assertions on DataLoader and underlying Dataset\n",
        "        self.assertEqual(dataloader.get_user(), client_id)\n",
        "        self.assertIsInstance(dataloader.dataset, BachelorDataset)\n",
        "        expected_sample_count = len(DUMMY_CLIENT_DATA.get(client_id, []))\n",
        "        self.assertEqual(len(dataloader.dataset), expected_sample_count, f\"Dataset length mismatch for {client_id} in DataLoader\")\n",
        "        self.assertEqual(dataloader.batch_size, 2)\n",
        "\n",
        "        # Iterate through batches\n",
        "        num_batches = 0\n",
        "        total_samples_yielded = 0\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            num_batches += 1\n",
        "            print_rank(f\"Client {client_id} Train Batch {i+1}: Keys - {batch.keys()}, Batch Size - {batch['input_ids'].shape[0]}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Assert batch structure and types\n",
        "            self.assertIn('input_ids', batch)\n",
        "            self.assertIn('attention_mask', batch)\n",
        "            self.assertIn('labels', batch)\n",
        "\n",
        "            self.assertIsInstance(batch['input_ids'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['attention_mask'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['labels'], torch.Tensor)\n",
        "\n",
        "            # Check shapes\n",
        "            self.assertEqual(batch['input_ids'].shape, batch['attention_mask'].shape)\n",
        "            self.assertEqual(batch['input_ids'].shape[0], len(batch['labels']))\n",
        "            self.assertLessEqual(batch['input_ids'].shape[0], dummy_args['batch_size'])\n",
        "\n",
        "            # Check padding\n",
        "            self.assertEqual(batch['input_ids'].shape[1], batch['attention_mask'].shape[1])\n",
        "            self.assertGreater(batch['input_ids'].shape[1], 0)\n",
        "\n",
        "            # Check label tensor type\n",
        "            self.assertEqual(batch['labels'].dtype, torch.long)\n",
        "\n",
        "            total_samples_yielded += batch['input_ids'].shape[0]\n",
        "\n",
        "        self.assertGreater(num_batches, 0, f\"DataLoader for {client_id} should yield at least one batch\")\n",
        "        # If drop_last is False, the total samples yielded should match the dataset size\n",
        "        self.assertEqual(total_samples_yielded, expected_sample_count, f\"Total samples yielded mismatch for {client_id}\")\n",
        "\n",
        "        print_rank(f\"Successfully tested DataLoader for {client_id} in train mode.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def test_dataloader_aggregated_eval(self):\n",
        "        \"\"\"Tests DataLoader for aggregated evaluation data from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataLoader Aggregated Eval (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "            'batch_size': 3, # Use a small batch size for testing\n",
        "        }\n",
        "\n",
        "        # Test loading aggregated validation data (mode='val', user_idx=-1)\n",
        "        dataloader = BachelorDataLoader(\n",
        "            mode='val', # Or 'test'\n",
        "            data=VAL_DATA_DIR, # Pass the directory path\n",
        "            num_workers=0,\n",
        "            args=dummy_args,\n",
        "            user_idx=-1 # Indicates aggregated data\n",
        "        )\n",
        "\n",
        "        # Assertions on DataLoader and underlying Dataset\n",
        "        self.assertEqual(dataloader.get_user(), 'aggregated_eval')\n",
        "        self.assertIsInstance(dataloader.dataset, BachelorDataset)\n",
        "        total_eval_samples = len(DUMMY_AGGREGATED_EVAL_DATA)\n",
        "        self.assertEqual(len(dataloader.dataset), total_eval_samples, \"Aggregated dataset length mismatch in DataLoader\")\n",
        "        self.assertEqual(dataloader.batch_size, 3)\n",
        "\n",
        "        # Iterate through all batches for evaluation\n",
        "        num_batches = 0\n",
        "        total_samples_yielded = 0\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            num_batches += 1\n",
        "            print_rank(f\"Aggregated Eval Batch {i+1}: Keys - {batch.keys()}, Batch Size - {batch['input_ids'].shape[0]}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Assert batch structure and types\n",
        "            self.assertIn('input_ids', batch)\n",
        "            self.assertIn('attention_mask', batch)\n",
        "            self.assertIn('labels', batch)\n",
        "\n",
        "            self.assertIsInstance(batch['input_ids'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['attention_mask'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['labels'], torch.Tensor)\n",
        "\n",
        "            # Check shapes\n",
        "            self.assertEqual(batch['input_ids'].shape, batch['attention_mask'].shape)\n",
        "            self.assertEqual(batch['input_ids'].shape[0], len(batch['labels']))\n",
        "            self.assertLessEqual(batch['input_ids'].shape[0], dummy_args['batch_size'])\n",
        "\n",
        "            # Check padding\n",
        "            self.assertEqual(batch['input_ids'].shape[1], batch['attention_mask'].shape[1])\n",
        "            self.assertGreater(batch['input_ids'].shape[1], 0)\n",
        "\n",
        "            # Check label tensor type\n",
        "            self.assertEqual(batch['labels'].dtype, torch.long)\n",
        "\n",
        "            total_samples_yielded += batch['input_ids'].shape[0]\n",
        "\n",
        "\n",
        "        self.assertGreater(num_batches, 0, \"Aggregated DataLoader should yield at least one batch\")\n",
        "        self.assertEqual(total_samples_yielded, total_eval_samples, \"Total samples yielded mismatch for aggregated data\")\n",
        "\n",
        "        print_rank(f\"Successfully tested DataLoader for aggregated evaluation data.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "# --- BachelorModel Class Definition ---\n",
        "# This class definition is placed AFTER the BaseModel definition\n",
        "\n",
        "# experiments/bachelor/models/model.py\n",
        "\n",
        "class BachelorModel(BaseModel):\n",
        "    \"\"\"\n",
        "    FLUTE BaseModel wrapper for Hugging Face AutoModelForSequenceClassification.\n",
        "    Configured for the Bachelor text classification task.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the BachelorModel.\n",
        "\n",
        "        Args:\n",
        "            model_config (dict): Dictionary containing model-specific configuration\n",
        "                                 from config.yaml (e.g., model_config['BachelorModel']).\n",
        "            **kwargs: Additional keyword arguments passed by FLUTE.\n",
        "                      Expected to include 'num_labels'.\n",
        "        \"\"\"\n",
        "        super(BachelorModel, self).__init__() # Initialize the BaseModel (and thus nn.Module)\n",
        "\n",
        "        # Extract configuration parameters specific to this model\n",
        "        # Assuming model_config['BachelorModel'] holds the relevant args\n",
        "        args = model_config.get('BachelorModel', {})\n",
        "        if not args:\n",
        "             raise ValueError(\"Model configuration for 'BachelorModel' not found in model_config.\")\n",
        "\n",
        "        # Split configuration into model and training arguments\n",
        "        model_args = args.get('model', {})\n",
        "        training_args = args.get('training', {})\n",
        "\n",
        "        # Set seed for reproducibility\n",
        "        set_seed(training_args.get('seed', 42)) # Default seed to 42 if not specified\n",
        "\n",
        "        # Get necessary parameters from config\n",
        "        self.model_name = model_args.get('model_name', 'bert-base-uncased') # Default model name\n",
        "        # gradient_accumulation_steps, eval_accumulation_steps, batch_size\n",
        "        # are typically handled by the FLUTE trainer/orchestrator, not the BaseModel itself.\n",
        "        # self.gradient_accumulation_steps = training_args.get('gradient_accumulation_steps', 1)\n",
        "        # self.eval_accumulation_steps = training_args.get('eval_accumulation_steps', None)\n",
        "        # self.batch_size = training_args.get('batch_size', 4)\n",
        "\n",
        "\n",
        "        # --- Determine number of labels ---\n",
        "        # The number of output labels for the classification head must match the dataset.\n",
        "        # This information is available in the dataset's label_map after it's loaded.\n",
        "        # FLUTE is expected to pass 'num_labels' via kwargs when instantiating the model.\n",
        "        self.num_labels = kwargs.get('num_labels')\n",
        "        if self.num_labels is None:\n",
        "             # Fallback: try to get it from model_args if explicitly set there,\n",
        "             # but getting it from the dataset via kwargs is preferred and more robust.\n",
        "             self.num_labels = model_args.get('num_labels')\n",
        "             if self.num_labels is None:\n",
        "                  # If still not available, raise an error to ensure correct setup.\n",
        "                  raise ValueError(\"Number of classification labels ('num_labels') must be provided to the Model constructor (preferably via kwargs from FLUTE).\")\n",
        "\n",
        "        print_rank(f\"Model expects {self.num_labels} classification labels.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "        # --- Load Model Configuration ---\n",
        "        # Load the configuration for the pre-trained model\n",
        "        config_kwargs = {\n",
        "            \"cache_dir\": model_args.get('cache_dir', None),\n",
        "            \"revision\": model_args.get('model_revision', 'main'), # Default revision\n",
        "            \"use_auth_token\": model_args.get('use_auth_token', None),\n",
        "            \"num_labels\": self.num_labels # Pass the number of labels to the config\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Try loading config by name or path\n",
        "            if 'config_name' in model_args and model_args['config_name']:\n",
        "                config = AutoConfig.from_pretrained(model_args['config_name'], **config_kwargs)\n",
        "            elif 'model_name_or_path' in model_args and model_args['model_name_or_path']:\n",
        "                config = AutoConfig.from_pretrained(model_args['model_name_or_path'], **config_kwargs)\n",
        "            else:\n",
        "                 # If no config name or model path, try loading from model_name\n",
        "                 config = AutoConfig.from_pretrained(self.model_name, **config_kwargs)\n",
        "\n",
        "        except Exception as e:\n",
        "             raise ValueError(f\"Failed to load model configuration for {self.model_name}: {e}\")\n",
        "\n",
        "\n",
        "        # --- Load Tokenizer ---\n",
        "        # Load the tokenizer associated with the model\n",
        "        tokenizer_kwargs = {\n",
        "            \"cache_dir\": model_args.get('cache_dir', None),\n",
        "            \"revision\": model_args.get('model_revision', 'main'), # Use model_revision for tokenizer consistency\n",
        "            \"use_fast\": model_args.get('use_fast_tokenizer', True), # Default to using fast tokenizer\n",
        "            \"use_auth_token\": model_args.get('use_auth_token', None),\n",
        "        }\n",
        "        try:\n",
        "            if 'tokenizer_name' in model_args and model_args['tokenizer_name']:\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(model_args['tokenizer_name'], **tokenizer_kwargs)\n",
        "            elif 'model_name_or_path' in model_args and model_args['model_name_or_path']:\n",
        "                print_rank(f\"Loading Tokenizer from Pretrained: {model_args['model_name_or_path']}\", loglevel=logging.INFO)\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained(model_args['model_name_or_path'], **tokenizer_kwargs)\n",
        "            else:\n",
        "                 # If no tokenizer name or model path, try loading from model_name\n",
        "                 self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, **tokenizer_kwargs)\n",
        "\n",
        "        except Exception as e:\n",
        "             raise ValueError(f\"Failed to load tokenizer for {self.model_name}: {e}\")\n",
        "\n",
        "\n",
        "        # --- Load the Model ---\n",
        "        # Load the pre-trained model for sequence classification\n",
        "        try:\n",
        "            if 'model_name_or_path' in model_args and model_args['model_name_or_path']:\n",
        "                print_rank(f\"Loading Model from Pretrained: {model_args['model_name_or_path']}\", loglevel=logging.INFO)\n",
        "                self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    model_args['model_name_or_path'],\n",
        "                    from_tf=False, # Set to True if loading from a TensorFlow checkpoint\n",
        "                    config=config, # Use the configured config with num_labels\n",
        "                    cache_dir=model_args.get('cache_dir', None),\n",
        "                    use_auth_token=model_args.get('use_auth_token', None),\n",
        "                )\n",
        "            else:\n",
        "                 # If no model_name_or_path, try loading from model_name with the configured config\n",
        "                 self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                    self.model_name,\n",
        "                    config=config, # Use the configured config with num_labels\n",
        "                    cache_dir=model_args.get('cache_dir', None),\n",
        "                    use_auth_token=model_args.get('use_auth_token', None),\n",
        "                )\n",
        "\n",
        "        except Exception as e:\n",
        "             raise ValueError(f\"Failed to load model {self.model_name} for sequence classification: {e}\")\n",
        "\n",
        "\n",
        "        # Resize token embeddings if the tokenizer's vocabulary size is different\n",
        "        # from the model's embedding size (e.g., after adding special tokens)\n",
        "        if len(self.tokenizer) != self.model.get_input_embeddings().weight.shape[0]:\n",
        "             print_rank(f\"Resizing token embeddings from {self.model.get_input_embeddings().weight.shape[0]} to {len(self.tokenizer)}\", loglevel=logging.INFO)\n",
        "             self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "\n",
        "        # --- PEFT Integration (Placeholder for now) ---\n",
        "        # This is where PEFT (e.g., LoRA) would be applied *after* loading the base model.\n",
        "        # We will add this later as requested.\n",
        "        # Example (LoRA):\n",
        "        # from peft import LoraConfig, get_peft_model\n",
        "        # lora_config = LoraConfig(...) # Define LoRA config based on model_args\n",
        "        # self.model = get_peft_model(self.model, lora_config)\n",
        "        # self.model.print_trainable_parameters() # Check trainable parameters\n",
        "\n",
        "\n",
        "        # --- Pruning Integration (Placeholder for now) ---\n",
        "        # Pruning setup would typically happen here after loading the model.\n",
        "        # We will add this later as requested.\n",
        "\n",
        "\n",
        "        # --- Log parameter counts ---\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "\n",
        "        print_rank(f\"Total model parameters: {total_params}\", loglevel=logging.INFO)\n",
        "        print_rank(f\"Trainable model parameters: {trainable_params}\", loglevel=logging.INFO)\n",
        "\n",
        "        # If PEFT is applied, trainable_params will be much smaller than total_params.\n",
        "        # If not using PEFT, trainable_params should equal total_params (unless some layers are frozen).\n",
        "        if total_params > 0:\n",
        "             print_rank(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "        # Move model to device (CPU/GPU)\n",
        "        self.device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        print_rank(f\"Model moved to device: {self.device}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def copy_state_dict(self, state_dict):\n",
        "        \"\"\"\n",
        "        Copies the provided state_dict into the model's state_dict.\n",
        "        Required by BaseModel for federated learning updates.\n",
        "        \"\"\"\n",
        "        # Ensure the state_dict is on the correct device before loading\n",
        "        state_dict = to_device(state_dict, self.device)\n",
        "        self.model.load_state_dict(state_dict)\n",
        "        print_rank(\"Model state_dict updated.\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        \"\"\"\n",
        "        Returns the underlying PyTorch model instance.\n",
        "        Required by BaseModel.\n",
        "        \"\"\"\n",
        "        return self.model\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the model.\n",
        "        Args are typically a batch dictionary from the DataLoader.\n",
        "        \"\"\"\n",
        "        # Move inputs to the correct device\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        # Pass inputs to the Hugging Face model\n",
        "        outputs = self.model(**inputs)\n",
        "        return outputs # Returns model output (logits, loss if labels provided)\n",
        "\n",
        "    def loss(self, inputs):\n",
        "        \"\"\"\n",
        "        Calculates the loss for a batch of inputs.\n",
        "        Required by BaseModel for training.\n",
        "        \"\"\"\n",
        "        # The Hugging Face AutoModelForSequenceClassification calculates loss\n",
        "        # automatically if 'labels' are provided in the inputs.\n",
        "        # We just need to ensure inputs are on the correct device and pass them.\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # Perform forward pass; loss is returned in the output if labels are present\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        # Extract the loss tensor\n",
        "        # The loss is typically the first element or accessed via .loss\n",
        "        loss = outputs.loss if hasattr(outputs, 'loss') and outputs.loss is not None else outputs[0]\n",
        "\n",
        "        # Apply gradient accumulation scaling if needed (though FLUTE handles this differently,\n",
        "        # keeping the structure might be useful if adapting parts of Hugging Face Trainer)\n",
        "        # loss = loss / self.gradient_accumulation_steps # FLUTE's optimizer handles accumulation\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n",
        "        \"\"\"\n",
        "        Prepare inputs by moving tensors to the correct device.\n",
        "        \"\"\"\n",
        "        # Move all tensor inputs to the model's device\n",
        "        for k, v in inputs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                # Call the local to_device function which we've ensured accepts the device argument\n",
        "                inputs[k] = to_device(v, self.device)\n",
        "        return inputs\n",
        "\n",
        "\n",
        "    def inference(\n",
        "            self, inputs, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = \"eval\"\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Performs inference (forward pass without gradient calculation) and returns metrics.\n",
        "        Required by BaseModel for evaluation.\n",
        "\n",
        "        Args:\n",
        "            inputs (Dict[str, Union[torch.Tensor, Any]]): Batch of inputs from the DataLoader.\n",
        "            ignore_keys (List[str], optional): Keys to ignore in model output.\n",
        "            metric_key_prefix (str): Prefix for metric keys (e.g., 'eval').\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, float]: Dictionary of computed metrics (e.g., loss, accuracy).\n",
        "        \"\"\"\n",
        "        self.model.eval() # Set model to evaluation mode\n",
        "\n",
        "        # Move inputs to the correct device\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        # Perform forward pass without calculating gradients\n",
        "        with T.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Extract logits and labels\n",
        "        # Logits are typically the second element or accessed via .logits\n",
        "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[1]\n",
        "        labels = inputs.get('labels') # Get labels from the input batch\n",
        "\n",
        "        metrics = {}\n",
        "        # Calculate loss if labels are available\n",
        "        if labels is not None:\n",
        "            # The model might return loss directly if labels were provided\n",
        "            loss = outputs.loss if hasattr(outputs, 'loss') and outputs.loss is not None else None\n",
        "            if loss is not None:\n",
        "                 metrics[f\"{metric_key_prefix}_loss\"] = loss.mean().item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            # Get predicted class IDs by finding the max logit\n",
        "            predictions = T.argmax(logits, dim=-1)\n",
        "            # Compare predictions to true labels\n",
        "            correct_predictions = (predictions == labels).sum().item()\n",
        "            total_samples = labels.size(0)\n",
        "            accuracy = correct_predictions / total_samples if total_samples > 0 else 0.0\n",
        "            metrics[f\"{metric_key_prefix}_accuracy\"] = accuracy\n",
        "\n",
        "        # Return metrics dictionary\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    # The prediction_loop and prediction_step methods from the example\n",
        "    # are more complex and often part of a Hugging Face Trainer.\n",
        "    # For a basic FLUTE BaseModel, the inference method should be sufficient\n",
        "    # to calculate metrics on a batch. We don't need to replicate the full Trainer loop here.\n",
        "    # If more complex evaluation logic is needed (e.g., gathering predictions across batches/devices),\n",
        "    # that might be handled outside the BaseModel or within a custom evaluation function.\n",
        "\n",
        "    # The floating_point_ops method is for logging FLOPS and is optional for basic setup.\n",
        "    # def floating_point_ops(self, inputs):\n",
        "    #     ...\n",
        "\n",
        "    def set_eval(self):\n",
        "        \"\"\"\n",
        "        Sets the underlying PyTorch model to evaluation mode.\n",
        "        Required by BaseModel.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        print_rank(\"Model set to evaluation mode.\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "    def set_train(self):\n",
        "        \"\"\"\n",
        "        Sets the underlying PyTorch model to training mode.\n",
        "        Required by BaseModel.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        print_rank(\"Model set to training mode.\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "# --- Test Classes ---\n",
        "\n",
        "class TestBachelorDataset(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"Set up dummy data simulating drive access before running tests.\"\"\"\n",
        "        cleanup_dummy_data() # Ensure cleanup before creating new data\n",
        "        create_dummy_client_data_files(TRAIN_DATA_DIR, DUMMY_CLIENT_DATA)\n",
        "        create_dummy_aggregated_data_file(VAL_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"val_data.jsonl\")\n",
        "        create_dummy_aggregated_data_file(TEST_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"test_data.jsonl\")\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"Clean up dummy data after running tests.\"\"\"\n",
        "        cleanup_dummy_data()\n",
        "\n",
        "    def test_dataset_client_loading(self):\n",
        "        \"\"\"Tests loading data for individual clients from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataset Client Loading (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        # Dummy args similar to what would come from config.yaml\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased', # Or 'BioClinicalBERT' if available publicly\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "        }\n",
        "\n",
        "        # Test loading for each client (0 to 5)\n",
        "        for client_idx in range(6):\n",
        "            client_id = f\"client_{client_idx}\"\n",
        "            print_rank(f\"Testing client: {client_id}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Pass the directory path containing all client files\n",
        "            dataset = BachelorDataset(\n",
        "                data_path=TRAIN_DATA_DIR, # Pass the directory path\n",
        "                args=dummy_args,\n",
        "                test_only=False,\n",
        "                user_idx=client_idx # FLUTE provides the user_idx\n",
        "            )\n",
        "\n",
        "            # Assertions\n",
        "            expected_sample_count = len(DUMMY_CLIENT_DATA.get(client_id, []))\n",
        "            self.assertEqual(len(dataset), expected_sample_count, f\"Dataset length mismatch for {client_id}\")\n",
        "            self.assertEqual(dataset.user, client_id, f\"Dataset user ID mismatch for {client_id}\")\n",
        "            self.assertIsNotNone(dataset.tokenizer)\n",
        "            self.assertGreater(len(dataset.label_map), 0, \"Label map should not be empty\")\n",
        "            # Check that user_list and num_samples contain info for ALL clients in the directory\n",
        "            self.assertEqual(len(dataset.user_list), len(DUMMY_CLIENT_DATA), \"user_list should contain all client IDs\")\n",
        "            self.assertEqual(len(dataset.num_samples), len(DUMMY_CLIENT_DATA), \"num_samples should contain counts for all clients\")\n",
        "\n",
        "            if expected_sample_count > 0:\n",
        "                # Test __getitem__ for the first sample if data exists\n",
        "                sample = dataset[0]\n",
        "                self.assertIn('input_ids', sample)\n",
        "                self.assertIn('attention_mask', sample)\n",
        "                self.assertIn('labels', sample)\n",
        "                self.assertIsInstance(sample['input_ids'], list)\n",
        "                self.assertIsInstance(sample['attention_mask'], list)\n",
        "                self.assertIsInstance(sample['labels'], int)\n",
        "                self.assertGreater(len(sample['input_ids']), 0, f\"Tokenization failed for a sample in {client_id}\")\n",
        "                self.assertIn(sample['labels'], dataset.label_map.values(), f\"Label mapping failed for a sample in {client_id}\")\n",
        "\n",
        "            print_rank(f\"Successfully loaded and tested data for {client_id}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def test_dataset_aggregated_loading(self):\n",
        "        \"\"\"Tests loading aggregated data from simulated drive file.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataset Aggregated Loading (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "        }\n",
        "\n",
        "        # Test loading aggregated validation data (user_idx=-1, test_only=True)\n",
        "        # The data_path points to the directory containing the single aggregated file\n",
        "        aggregated_dataset_val = BachelorDataset(\n",
        "            data_path=VAL_DATA_DIR, # Pass the directory path\n",
        "            args=dummy_args,\n",
        "            test_only=True, # Indicates this is for evaluation\n",
        "            user_idx=-1 # Indicates aggregated data\n",
        "        )\n",
        "\n",
        "        total_eval_samples = len(DUMMY_AGGREGATED_EVAL_DATA)\n",
        "        self.assertEqual(len(aggregated_dataset_val), total_eval_samples, \"Aggregated dataset length mismatch\")\n",
        "        self.assertEqual(aggregated_dataset_val.user, 'aggregated_eval')\n",
        "        self.assertIsNotNone(aggregated_dataset_val.tokenizer)\n",
        "        self.assertGreater(len(aggregated_dataset_val.label_map), 0, \"Label map should not be empty for aggregated data\")\n",
        "\n",
        "        if total_eval_samples > 0:\n",
        "             # Test __getitem__ for aggregated data\n",
        "            sample = aggregated_dataset_val[0]\n",
        "            self.assertIn('input_ids', sample)\n",
        "            self.assertIn('labels', sample)\n",
        "            self.assertIsInstance(sample['labels'], int)\n",
        "            self.assertIn(sample['labels'], aggregated_dataset_val.label_map.values())\n",
        "\n",
        "        print_rank(f\"Successfully loaded and tested aggregated validation data ({total_eval_samples} samples)\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "class TestBachelorDataLoader(unittest.TestCase):\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        \"\"\"Set up dummy data simulating drive access before running tests.\"\"\"\n",
        "        cleanup_dummy_data()\n",
        "        create_dummy_client_data_files(TRAIN_DATA_DIR, DUMMY_CLIENT_DATA)\n",
        "        create_dummy_aggregated_data_file(VAL_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"val_data.jsonl\")\n",
        "        create_dummy_aggregated_data_file(TEST_DATA_DIR, DUMMY_AGGREGATED_EVAL_DATA, file_name=\"test_data.jsonl\")\n",
        "\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        \"\"\"Clean up dummy data after running tests.\"\"\"\n",
        "        cleanup_dummy_data()\n",
        "\n",
        "    def test_dataloader_client_train(self):\n",
        "        \"\"\"Tests DataLoader for a client in training mode from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataLoader Client Train (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        # Dummy args including batch size\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "            'batch_size': 2, # Use a small batch size for testing\n",
        "        }\n",
        "\n",
        "        # Test DataLoader for client 0\n",
        "        client_idx = 0\n",
        "        client_id = f\"client_{client_idx}\"\n",
        "        dataloader = BachelorDataLoader(\n",
        "            mode='train',\n",
        "            data=TRAIN_DATA_DIR, # Pass the directory path\n",
        "            num_workers=0, # Use 0 workers for simplicity in testing\n",
        "            args=dummy_args,\n",
        "            user_idx=client_idx # Specify the client index\n",
        "        )\n",
        "\n",
        "        # Assertions on DataLoader and underlying Dataset\n",
        "        self.assertEqual(dataloader.get_user(), client_id)\n",
        "        self.assertIsInstance(dataloader.dataset, BachelorDataset)\n",
        "        expected_sample_count = len(DUMMY_CLIENT_DATA.get(client_id, []))\n",
        "        self.assertEqual(len(dataloader.dataset), expected_sample_count, f\"Dataset length mismatch for {client_id} in DataLoader\")\n",
        "        self.assertEqual(dataloader.batch_size, 2)\n",
        "\n",
        "        # Iterate through batches\n",
        "        num_batches = 0\n",
        "        total_samples_yielded = 0\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            num_batches += 1\n",
        "            print_rank(f\"Client {client_id} Train Batch {i+1}: Keys - {batch.keys()}, Batch Size - {batch['input_ids'].shape[0]}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Assert batch structure and types\n",
        "            self.assertIn('input_ids', batch)\n",
        "            self.assertIn('attention_mask', batch)\n",
        "            self.assertIn('labels', batch)\n",
        "\n",
        "            self.assertIsInstance(batch['input_ids'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['attention_mask'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['labels'], torch.Tensor)\n",
        "\n",
        "            # Check shapes\n",
        "            self.assertEqual(batch['input_ids'].shape, batch['attention_mask'].shape)\n",
        "            self.assertEqual(batch['input_ids'].shape[0], len(batch['labels']))\n",
        "            self.assertLessEqual(batch['input_ids'].shape[0], dummy_args['batch_size'])\n",
        "\n",
        "            # Check padding\n",
        "            self.assertEqual(batch['input_ids'].shape[1], batch['attention_mask'].shape[1])\n",
        "            self.assertGreater(batch['input_ids'].shape[1], 0)\n",
        "\n",
        "            # Check label tensor type\n",
        "            self.assertEqual(batch['labels'].dtype, torch.long)\n",
        "\n",
        "            total_samples_yielded += batch['input_ids'].shape[0]\n",
        "\n",
        "        self.assertGreater(num_batches, 0, f\"DataLoader for {client_id} should yield at least one batch\")\n",
        "        # If drop_last is False, the total samples yielded should match the dataset size\n",
        "        self.assertEqual(total_samples_yielded, expected_sample_count, f\"Total samples yielded mismatch for {client_id}\")\n",
        "\n",
        "        print_rank(f\"Successfully tested DataLoader for {client_id} in train mode.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def test_dataloader_aggregated_eval(self):\n",
        "        \"\"\"Tests DataLoader for aggregated evaluation data from simulated drive directory.\"\"\"\n",
        "        print_rank(\"\\n--- Testing BachelorDataLoader Aggregated Eval (Simulated Drive) ---\", loglevel=logging.INFO)\n",
        "\n",
        "        dummy_args = {\n",
        "            'model_name_or_path': 'bert-base-uncased',\n",
        "            'max_seq_length': 128,\n",
        "            'padding': True,\n",
        "            'cache_dir': None,\n",
        "            'tokenizer_type_fast': True,\n",
        "            'batch_size': 3, # Use a small batch size for testing\n",
        "        }\n",
        "\n",
        "        # Test loading aggregated validation data (mode='val', user_idx=-1)\n",
        "        dataloader = BachelorDataLoader(\n",
        "            mode='val', # Or 'test'\n",
        "            data=VAL_DATA_DIR, # Pass the directory path\n",
        "            num_workers=0,\n",
        "            args=dummy_args,\n",
        "            user_idx=-1 # Indicates aggregated data\n",
        "        )\n",
        "\n",
        "        # Assertions on DataLoader and underlying Dataset\n",
        "        self.assertEqual(dataloader.get_user(), 'aggregated_eval')\n",
        "        self.assertIsInstance(dataloader.dataset, BachelorDataset)\n",
        "        total_eval_samples = len(DUMMY_AGGREGATED_EVAL_DATA)\n",
        "        self.assertEqual(len(dataloader.dataset), total_eval_samples, \"Aggregated dataset length mismatch in DataLoader\")\n",
        "        self.assertEqual(dataloader.batch_size, 3)\n",
        "\n",
        "        # Iterate through all batches for evaluation\n",
        "        num_batches = 0\n",
        "        total_samples_yielded = 0\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            num_batches += 1\n",
        "            print_rank(f\"Aggregated Eval Batch {i+1}: Keys - {batch.keys()}, Batch Size - {batch['input_ids'].shape[0]}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            # Assert batch structure and types\n",
        "            self.assertIn('input_ids', batch)\n",
        "            self.assertIn('attention_mask', batch)\n",
        "            self.assertIn('labels', batch)\n",
        "\n",
        "            self.assertIsInstance(batch['input_ids'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['attention_mask'], torch.Tensor)\n",
        "            self.assertIsInstance(batch['labels'], torch.Tensor)\n",
        "\n",
        "            # Check shapes\n",
        "            self.assertEqual(batch['input_ids'].shape, batch['attention_mask'].shape)\n",
        "            self.assertEqual(batch['input_ids'].shape[0], len(batch['labels']))\n",
        "            self.assertLessEqual(batch['input_ids'].shape[0], dummy_args['batch_size'])\n",
        "\n",
        "            # Check padding\n",
        "            self.assertEqual(batch['input_ids'].shape[1], batch['attention_mask'].shape[1])\n",
        "            self.assertGreater(batch['input_ids'].shape[1], 0)\n",
        "\n",
        "            # Check label tensor type\n",
        "            self.assertEqual(batch['labels'].dtype, torch.long)\n",
        "\n",
        "            total_samples_yi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag7QQx8bCrvG",
        "outputId": "f75dc703-ce07-4fda-db29-e71408d68c0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using dummy print_rank implementation.\n",
            "[INFO] [INFO] Using custom to_device implementation that accepts device argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ieegLdHZJfLE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbfdc359-f74e-4f71-af9c-2ecc08b960c9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6jSRQmmK4LE",
        "outputId": "43943f2a-1ce1-400e-c07c-02a464cdc582"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "azure-pipelines.yml  configs\t      e2e_trainer.py  NOTICE.txt\ttesting\n",
            "CHANGELOG.md\t     CONTRIBUTING.md  experiments     README.md\t\tutils\n",
            "CITATION.cff\t     core\t      extensions      requirements.txt\n",
            "CODE_OF_CONDUCT.md   doc\t      LICENSE.TXT     SECURITY.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd msrflute"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xN1zwtOZK6nl",
        "outputId": "3b50be5d-dace-4a55-aeb1-715c5559f4bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/msrflute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.run --nproc_per_node=2 /content/msrflute/e2e_trainer.py -config config.yaml -dataPath /content/drive/MyDrive/simulated_drive_data -outputPath /content/drive/MyDrive/flute_output -backend gloo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkjhZ2xUKSZm",
        "outputId": "79abe1ef-1820-40b8-cd18-8e5a58e51e4b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*****************************************\n",
            "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
            "*****************************************\n",
            "The data can be found here:  /content/drive/MyDrive/simulated_drive_data\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/msrflute/e2e_trainer.py\", line 235, in <module>\n",
            "    shutil.copyfile(args.config, cfg_out)\n",
            "  File \"/usr/lib/python3.11/shutil.py\", line 256, in copyfile\n",
            "    with open(src, 'rb') as fsrc:\n",
            "         ^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'\n",
            "The data can be found here:  /content/drive/MyDrive/simulated_drive_data\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/msrflute/e2e_trainer.py\", line 235, in <module>\n",
            "    shutil.copyfile(args.config, cfg_out)\n",
            "  File \"/usr/lib/python3.11/shutil.py\", line 256, in copyfile\n",
            "    with open(src, 'rb') as fsrc:\n",
            "         ^^^^^^^^^^^^^^^\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'\n",
            "W0513 11:48:16.674000 35491 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 35512 closing signal SIGTERM\n",
            "E0513 11:48:16.708000 35491 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 35511) of binary: /usr/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 922, in <module>\n",
            "    main()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 918, in main\n",
            "    run(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n",
            "    elastic_launch(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "/content/msrflute/e2e_trainer.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2025-05-13_11:48:16\n",
            "  host      : 7fe75e53d87f\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 35511)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to test the data loading functionality of MedicalTextDataset\n",
        "\n",
        "import os\n",
        "import json\n",
        "import logging\n",
        "\n",
        "# Configure logging for better output during testing\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define a simple print_rank if you don't have a utils file\n",
        "def print_rank(msg, loglevel=logging.INFO):\n",
        "    \"\"\"Simple placeholder for distributed logging.\"\"\"\n",
        "    if loglevel >= logging.INFO: # Adjust based on desired verbosity\n",
        "        logger.log(loglevel, f\"[Test Script] {msg}\") # Use logger\n",
        "\n",
        "\n",
        "# --- Dummy BaseDataset for testing purposes ---\n",
        "# We only need the __init__ method calling load_data for this test.\n",
        "class BaseDataset:\n",
        "    \"\"\"Dummy BaseDataset for testing purposes.\"\"\"\n",
        "    def __init__(self, data, args, tokenizer=None, test_only=False, user_idx=0, **kwargs):\n",
        "        # Simulate the call to load_data that BaseDataset would make\n",
        "        self.load_data(data, user_idx)\n",
        "\n",
        "\n",
        "# --- Relevant parts of the MedicalTextDataset class ---\n",
        "class MedicalTextDataset(BaseDataset):\n",
        "    \"\"\"\n",
        "    Dataset class for Medical Text Classification data.\n",
        "    Loads data from per-user JSONL files on Google Drive.\n",
        "    (Simplified for testing data loading only)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, args, tokenizer=None, test_only=False, user_idx=0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the MedicalTextDataset.\n",
        "\n",
        "        Args:\n",
        "            data: The path to the directory containing user data files on Google Drive.\n",
        "                  This is passed by FLUTE based on your config.yaml.\n",
        "            args: Dictionary of arguments from the FLUTE config.yaml.\n",
        "            tokenizer: Optional pre-loaded tokenizer (ignored in this test).\n",
        "            test_only: Boolean indicating if this is a test dataset instance.\n",
        "            user_idx: Index of the current user/client. -1 for test/validation data.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "        \"\"\"\n",
        "        # Initialize attributes to hold loaded data in FLUTE's required format\n",
        "        self.user_list = []         # List of user IDs (e.g., ['user_0', 'user_1', ...])\n",
        "        self.user_data = {}         # Dict mapping user ID to their raw data list\n",
        "        self.num_samples = []       # List of number of samples per user\n",
        "        self.user_data_labels = {}  # Dict mapping user ID to their raw labels list\n",
        "\n",
        "        # This list would hold the processed (tokenized) data - not used in this test\n",
        "        self.processed_data_list = []\n",
        "\n",
        "        self.test_only = test_only\n",
        "        # Get padding and max_seq_length from args - not used in this test\n",
        "        self.padding = args.get('padding', True)\n",
        "        self.max_seq_length = args.get('max_seq_length', 512)\n",
        "\n",
        "        # Store args for later use if needed\n",
        "        self.args = args\n",
        "\n",
        "        # --- Tokenizer Loading (SKIPPED for this test) ---\n",
        "        # self.tokenizer = None # Or load a dummy tokenizer if BaseDataset requires it\n",
        "\n",
        "        print_rank(\"Initialized MedicalTextDataset (tokenizer loading skipped for test)\", loglevel=logging.INFO)\n",
        "\n",
        "        # --- Data Loading ---\n",
        "        # The load_data method is called by the BaseDataset __init__ automatically.\n",
        "        # We pass the 'data' argument (which is the path to the data directory) and user_idx.\n",
        "        self.load_data(data, user_idx)\n",
        "\n",
        "        # --- Data Processing (SKIPPED for this test) ---\n",
        "        # The _tokenize_and_format method is not called in this test as we only\n",
        "        # want to verify raw data loading.\n",
        "        # if user_idx != -1:\n",
        "        #      self._tokenize_and_format()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of processed samples in this dataset instance.\n",
        "        (Returns 0 in this test as processed_data_list is not populated)\n",
        "        \"\"\"\n",
        "        return len(self.processed_data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single processed sample by index.\n",
        "        (Not functional in this test)\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"getitem is not implemented in this data loading test\")\n",
        "\n",
        "\n",
        "    def load_data(self, data_dir_path, user_idx):\n",
        "        \"\"\"\n",
        "        Reads the raw data from the specified directory on Google Drive.\n",
        "        Populates user_list, user_data, num_samples, and user_data_labels.\n",
        "\n",
        "        Args:\n",
        "            data_dir_path: The path to the directory containing user data files on Google Drive.\n",
        "            user_idx: The index of the current user/client. -1 for test/validation data.\n",
        "        \"\"\"\n",
        "        print_rank(f\"Attempting to load data from directory: {data_dir_path}\", loglevel=logging.INFO)\n",
        "\n",
        "        # List all files in the data directory.\n",
        "        try:\n",
        "            # Get list of user data files (e.g., user_0.jsonl, user_1.jsonl)\n",
        "            user_files = [f for f in os.listdir(data_dir_path) if f.endswith('.jsonl') and f.startswith('user_')]\n",
        "            user_files.sort() # Ensure consistent order\n",
        "\n",
        "            # Extract user IDs from filenames (e.g., 'user_0' from 'user_0.jsonl')\n",
        "            self.user_list = [f.replace('.jsonl', '') for f in user_files]\n",
        "            print_rank(f\"Found {len(self.user_list)} user files: {self.user_list}\", loglevel=logging.INFO)\n",
        "\n",
        "            # In this test, we load data for all users to print samples from each.\n",
        "            # In the actual FLUTE run, user_idx != -1 would load only one user's data.\n",
        "\n",
        "            print_rank(\"Loading data for all users (for testing purposes)\", loglevel=logging.INFO)\n",
        "            all_raw_data = {}\n",
        "            all_raw_labels = {}\n",
        "            all_num_samples = []\n",
        "\n",
        "            for user_id in self.user_list:\n",
        "                user_file_path = os.path.join(data_dir_path, f\"{user_id}.jsonl\")\n",
        "                print_rank(f\"Reading data from {user_file_path}\", loglevel=logging.DEBUG)\n",
        "                raw_user_data, raw_user_labels = self._read_jsonl_file(user_file_path)\n",
        "\n",
        "                all_raw_data[user_id] = raw_user_data\n",
        "                all_raw_labels[user_id] = raw_user_labels\n",
        "                all_num_samples.append(len(raw_user_data))\n",
        "                print_rank(f\"Read {len(raw_user_data)} samples from {user_id}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            self.user_data = all_raw_data\n",
        "            self.user_data_labels = all_raw_labels\n",
        "            self.num_samples = all_num_samples\n",
        "\n",
        "            total_samples_loaded = sum(self.num_samples)\n",
        "            print_rank(f\"Finished reading raw data for all {len(self.user_list)} users. Total samples read: {total_samples_loaded}\", loglevel=logging.INFO)\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print_rank(f\"Error: Data directory not found at {data_dir_path}. Ensure Google Drive is mounted and path is correct.\", loglevel=logging.ERROR)\n",
        "            raise e\n",
        "        except Exception as e:\n",
        "            print_rank(f\"An error occurred while listing files or reading data: {e}\", loglevel=logging.ERROR)\n",
        "            raise e\n",
        "\n",
        "\n",
        "    def _read_jsonl_file(self, file_path):\n",
        "        \"\"\"\n",
        "        Reads data from a single JSONL file.\n",
        "\n",
        "        Args:\n",
        "            file_path: The path to the JSONL file.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing two lists: raw_texts and raw_labels.\n",
        "        \"\"\"\n",
        "        raw_texts = []\n",
        "        raw_labels = []\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    if line.strip(): # Skip empty lines\n",
        "                        data_point = json.loads(line)\n",
        "                        # Assuming your data has 'prompt' for input text and 'label' for the target class\n",
        "                        raw_texts.append(data_point.get('prompt', data_point.get('text', ''))) # Use 'prompt' or fallback to 'text'\n",
        "                        raw_labels.append(data_point.get('label', '')) # Get the label\n",
        "\n",
        "            return raw_texts, raw_labels # Return raw strings\n",
        "\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print_rank(f\"Error: File not found at {file_path}\", loglevel=logging.ERROR)\n",
        "            return [], []\n",
        "        except json.JSONDecodeError:\n",
        "            print_rank(f\"Error decoding JSON from file: {file_path}. Ensure it's a valid JSONL file.\", loglevel=logging.ERROR)\n",
        "            return [], []\n",
        "        except Exception as e:\n",
        "            print_rank(f\"An error occurred while reading {file_path}: {e}\", loglevel=logging.ERROR)\n",
        "            return [], []\n",
        "\n",
        "    # _tokenize_and_format and post_process_list are excluded for this test.\n",
        "\n",
        "\n",
        "# --- Test Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # IMPORTANT: Replace this with the actual path to your dataset directory on Google Drive\n",
        "    # Example for Google Colab: /content/drive/MyDrive/my_flute_formatted_dataset_users\n",
        "    # Example for a mounted drive: /path/to/your/mounted/drive/my_flute_formatted_dataset_users\n",
        "    DATA_DIRECTORY_PATH = \"/content/drive/MyDrive/my_flute_formatted_dataset_users\" # <<<--- REPLACE THIS LINE\n",
        "\n",
        "    # Dummy args dictionary - only the data path is strictly needed for this test\n",
        "    dummy_args = {}\n",
        "\n",
        "    print_rank(\"Starting data loading test...\", loglevel=logging.INFO)\n",
        "\n",
        "    try:\n",
        "        # Instantiate the dataset for testing (user_idx=-1 to load all data)\n",
        "        # We pass DATA_DIRECTORY_PATH as the 'data' argument\n",
        "        test_dataset_instance = MedicalTextDataset(data=DATA_DIRECTORY_PATH, args=dummy_args, user_idx=-1, test_only=True)\n",
        "\n",
        "        print_rank(\"\\n--- Loaded Data Summary ---\", loglevel=logging.INFO)\n",
        "        print_rank(f\"User List: {test_dataset_instance.user_list}\", loglevel=logging.INFO)\n",
        "        print_rank(f\"Number of Samples per User: {test_dataset_instance.num_samples}\", loglevel=logging.INFO)\n",
        "\n",
        "        print_rank(\"\\n--- First 3 Samples per User ---\", loglevel=logging.INFO)\n",
        "\n",
        "        if not test_dataset_instance.user_list:\n",
        "            print_rank(\"No users found or data not loaded.\", loglevel=logging.WARNING)\n",
        "        else:\n",
        "            for user_id in test_dataset_instance.user_list:\n",
        "                print(f\"\\nUser: {user_id}\")\n",
        "                user_texts = test_dataset_instance.user_data.get(user_id, [])\n",
        "                user_labels = test_dataset_instance.user_data_labels.get(user_id, [])\n",
        "\n",
        "                if not user_texts:\n",
        "                    print_rank(f\"  No data found for user {user_id}\", loglevel=logging.WARNING)\n",
        "                    continue\n",
        "\n",
        "                # Print the first 3 samples\n",
        "                for i in range(min(3, len(user_texts))):\n",
        "                    print(f\"  Sample {i+1}:\")\n",
        "                    print(f\"    Text: {user_texts[i][:100]}...\") # Print first 100 chars\n",
        "                    print(f\"    Label: {user_labels[i]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print_rank(f\"Test failed due to an error: {e}\", loglevel=logging.ERROR)\n",
        "\n",
        "    print_rank(\"\\nData loading test finished.\", loglevel=logging.INFO)\n",
        "\n"
      ],
      "metadata": {
        "id": "Tli-pLuk1Lnh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41488724-24e8-4c70-ccda-0d45b2c654e0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "User: user_0\n",
            "  Sample 1:\n",
            "    Text: Symptoms: Symptoms: Fatigue. Age: 30, Gender: Male, Blood Pressure: High, Cholesterol Level: High.. ...\n",
            "    Label: Muscular Dystrophy\n",
            "  Sample 2:\n",
            "    Text: Symptoms: Symptoms: Cough, Difficulty Breathing. Age: 60, Gender: Female, Blood Pressure: Normal, Ch...\n",
            "    Label: Osteoarthritis\n",
            "  Sample 3:\n",
            "    Text: Symptoms: I've had a terrible cough and cold for days. My face is tired and my sinuses are blocked. ...\n",
            "    Label: Common Cold\n",
            "\n",
            "User: user_1\n",
            "  Sample 1:\n",
            "    Text: Symptoms: I have been experiencing a headache, chest pain, dizziness, and difficulty maintaining my ...\n",
            "    Label: Hypertension\n",
            "  Sample 2:\n",
            "    Text: Symptoms: I've had a pretty uncomfortable itch all over my body, as well as red and bumpy areas on m...\n",
            "    Label: Fungal infection\n",
            "  Sample 3:\n",
            "    Text: Symptoms: The high fever I have been experiencing is accompanied by sweating and weakness. My muscle...\n",
            "    Label: Dengue\n",
            "\n",
            "User: user_2\n",
            "  Sample 1:\n",
            "    Text: Symptoms: My throat often feels like it is burning, especially after eating. A bitter or sour aftert...\n",
            "    Label: gastroesophageal reflux disease\n",
            "  Sample 2:\n",
            "    Text: Symptoms: Especially after eating, my throat regularly feels like it is burning. I also occasionally...\n",
            "    Label: gastroesophageal reflux disease\n",
            "  Sample 3:\n",
            "    Text: Symptoms: Symptoms: Fever, Fatigue. Age: 35, Gender: Female, Blood Pressure: High, Cholesterol Level...\n",
            "    Label: Rubella\n",
            "\n",
            "User: user_3\n",
            "  Sample 1:\n",
            "    Text: Symptoms: I inadvertently lose weight and have a hard time gaining it back. I use antacids to get ri...\n",
            "    Label: peptic ulcer disease\n",
            "  Sample 2:\n",
            "    Text: Symptoms: My vision is foggy, and it seems to be growing worse. I'm constantly feeling worn out and ...\n",
            "    Label: diabetes\n",
            "  Sample 3:\n",
            "    Text: Symptoms: Particularly in the crevices of my skin, I have skin rashes and irritations. My skin bruis...\n",
            "    Label: diabetes\n",
            "\n",
            "User: user_4\n",
            "  Sample 1:\n",
            "    Text: Symptoms: Hey, I've had a dry cough, breathing difficulties, as well as a high fever. a lot of mucou...\n",
            "    Label: Bronchial Asthma\n",
            "  Sample 2:\n",
            "    Text: Symptoms: Symptoms: Fever, Fatigue. Age: 70, Gender: Female, Blood Pressure: Normal, Cholesterol Lev...\n",
            "    Label: Gout\n",
            "  Sample 3:\n",
            "    Text: Symptoms: I am worried about the constant peeling of the skin on my palm, elbow and knee. I have dev...\n",
            "    Label: Psoriasis\n",
            "\n",
            "User: user_5\n",
            "  Sample 1:\n",
            "    Text: Symptoms: Symptoms: Fever, Cough, Fatigue. Age: 35, Gender: Female, Blood Pressure: High, Cholestero...\n",
            "    Label: Hypertension\n",
            "  Sample 2:\n",
            "    Text: Symptoms: Extreme itchiness, nausea, and fatigue have been troubling me. In moreover, I experienced ...\n",
            "    Label: Jaundice\n",
            "  Sample 3:\n",
            "    Text: Symptoms: Mom, Dad, I've been feeling really tired and weak lately, and I've had this cough that jus...\n",
            "    Label: Bronchial Asthma\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to test the MedicalTextDataloader class\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import tempfile\n",
        "import torch\n",
        "import logging\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification # Import necessary transformers components\n",
        "from transformers.data.data_collator import DataCollatorWithPadding # Import the data collator\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler # Import standard DataLoader and Samplers\n",
        "from torch.utils.data.distributed import DistributedSampler # Import DistributedSampler if needed\n",
        "\n",
        "# Configure logging for better output during testing\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define a simple print_rank if you don't have a utils file\n",
        "def print_rank(msg, loglevel=logging.INFO):\n",
        "    \"\"\"Simple placeholder for distributed logging.\"\"\"\n",
        "    # In a real FLUTE distributed setting, this would handle printing only from rank 0\n",
        "    if loglevel >= logging.INFO: # Adjust based on desired verbosity\n",
        "        logger.log(loglevel, f\"[Test Script] {msg}\") # Use logger\n",
        "\n",
        "\n",
        "# --- Dummy BaseDataset for testing purposes ---\n",
        "# We need the __init__ method calling load_data for this test.\n",
        "class BaseDataset:\n",
        "    \"\"\"Dummy BaseDataset for testing purposes.\"\"\"\n",
        "    def __init__(self, data, args, tokenizer=None, test_only=False, user_idx=0, **kwargs):\n",
        "        # Simulate the call to load_data that BaseDataset would make\n",
        "        # Note: In a real BaseDataset, this might involve more setup before load_data\n",
        "        self.load_data(data, user_idx)\n",
        "\n",
        "\n",
        "# --- Dummy BaseDataLoader for testing purposes ---\n",
        "# This class is needed because MedicalTextDataloader inherits from it.\n",
        "# It simply inherits from PyTorch's DataLoader.\n",
        "class BaseDataLoader(DataLoader):\n",
        "    \"\"\"Dummy BaseDataLoader for testing purposes.\"\"\"\n",
        "    # The __init__ will implicitly call torch.utils.data.DataLoader's __init__\n",
        "    # when super().__init__ is called in MedicalTextDataloader.\n",
        "    pass # No additional logic needed for this dummy\n",
        "\n",
        "\n",
        "# --- MedicalTextDataset class (needed by the Dataloader) ---\n",
        "# Include the relevant parts of your MedicalTextDataset class here\n",
        "class MedicalTextDataset(BaseDataset):\n",
        "    \"\"\"\n",
        "    Dataset class for Medical Text Classification data.\n",
        "    Loads data from per-user JSONL files on Google Drive.\n",
        "    (Included for Dataloader test)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, args, tokenizer=None, test_only=False, user_idx=0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the MedicalTextDataset.\n",
        "\n",
        "        Args:\n",
        "            data: The path to the directory containing user data files.\n",
        "            args: Dictionary of arguments from the FLUTE config.yaml.\n",
        "            tokenizer: Optional pre-loaded tokenizer.\n",
        "            test_only: Boolean indicating if this is a test dataset instance.\n",
        "            user_idx: Index of the current user/client. -1 for test/validation data.\n",
        "            **kwargs: Additional keyword arguments.\n",
        "        \"\"\"\n",
        "        # Initialize attributes to hold loaded data in FLUTE's required format\n",
        "        self.user_list = []         # List of user IDs (e.g., ['user_0', 'user_1', ...])\n",
        "        self.user_data = {}         # Dict mapping user ID to their raw data list\n",
        "        self.num_samples = []       # List of number of samples per user\n",
        "        self.user_data_labels = {}  # Dict mapping user ID to their raw labels list\n",
        "\n",
        "        # This list will hold the processed (tokenized) data ready for the model\n",
        "        self.processed_data_list = []\n",
        "\n",
        "        self.test_only = test_only\n",
        "        # Get padding and max_seq_length from args, with default values\n",
        "        self.padding = args.get('padding', True)\n",
        "        self.max_seq_length = args.get('max_seq_length', 512)\n",
        "        self.args = args # Store args for later use if needed\n",
        "\n",
        "        # --- Tokenizer Loading ---\n",
        "        # Load the tokenizer needed for tokenization in _tokenize_and_format\n",
        "        if tokenizer is not None:\n",
        "            self.tokenizer = tokenizer\n",
        "        else:\n",
        "            tokenizer_kwargs = {\n",
        "                 \"cache_dir\": args.get('cache_dir', None),\n",
        "                 \"use_fast\": args.get('tokenizer_type_fast', True),\n",
        "                 \"use_auth_token\": None # Set to your auth token if required\n",
        "            }\n",
        "            tokenizer_path = args.get('tokenizer_path', None)\n",
        "            if tokenizer_path is None:\n",
        "                 raise ValueError(\"tokenizer_path must be provided in args for MedicalTextDataset\")\n",
        "\n",
        "            print_rank(f\"Loading tokenizer from: {tokenizer_path}\", loglevel=logging.INFO)\n",
        "            try:\n",
        "                 self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, **tokenizer_kwargs)\n",
        "            except Exception as e:\n",
        "                 print_rank(f\"Error loading tokenizer from {tokenizer_path}: {e}\", loglevel=logging.ERROR)\n",
        "                 raise\n",
        "\n",
        "        # Validate and set max_seq_length based on tokenizer's model_max_length\n",
        "        if self.max_seq_length is None:\n",
        "            self.max_seq_length = self.tokenizer.model_max_length\n",
        "            if self.max_seq_length > 512:\n",
        "                print_rank(\n",
        "                    f\"The tokenizer picked seems to have a very large `model_max_length` ({self.tokenizer.model_max_length}). \"\n",
        "                    \"Picking 512 instead. You can change that default value by passing --max_seq_length xxx.\", loglevel=logging.DEBUG\n",
        "                )\n",
        "                self.max_seq_length = 512\n",
        "        else:\n",
        "            if self.max_seq_length > self.tokenizer.model_max_length:\n",
        "                print_rank(\n",
        "                    f\"The max_seq_length passed ({self.max_seq_length}) is larger than the maximum length for the\"\n",
        "                    f\"model ({self.tokenizer.model_max_length}). Using max_seq_length={self.tokenizer.model_max_length}.\", loglevel=logging.DEBUG\n",
        "                )\n",
        "            self.max_seq_length = min(self.max_seq_length, self.tokenizer.model_max_length)\n",
        "\n",
        "        print_rank(f\"Initialized MedicalTextDataset with max_seq_length: {self.max_seq_length}\", loglevel=logging.INFO)\n",
        "\n",
        "        # --- Data Loading ---\n",
        "        # The load_data method is called by the BaseDataset __init__ automatically.\n",
        "        # We pass the 'data' argument (which is the path to the data directory) and user_idx.\n",
        "        self.load_data(data, user_idx)\n",
        "\n",
        "        # --- Data Processing ---\n",
        "        # Process the data (tokenize and format) only if it's for a specific user (training)\n",
        "        if user_idx != -1:\n",
        "             self._tokenize_and_format()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of processed samples in this dataset instance.\n",
        "        \"\"\"\n",
        "        return len(self.processed_data_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves a single processed sample by index.\n",
        "\n",
        "        Args:\n",
        "            idx: The index of the sample to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the tokenized input features and label for the sample.\n",
        "        \"\"\"\n",
        "        return self.processed_data_list[idx]\n",
        "\n",
        "\n",
        "    def load_data(self, data_dir_path, user_idx):\n",
        "        \"\"\"\n",
        "        Reads the raw data from the specified directory.\n",
        "        Populates user_list, user_data, num_samples, and user_data_labels.\n",
        "\n",
        "        Args:\n",
        "            data_dir_path: The path to the directory containing user data files.\n",
        "            user_idx: The index of the current user/client. -1 for test/validation data.\n",
        "        \"\"\"\n",
        "        print_rank(f\"Attempting to load data from directory: {data_dir_path}\", loglevel=logging.INFO)\n",
        "\n",
        "        try:\n",
        "            # Get list of user data files (e.g., user_0.jsonl, user_1.jsonl)\n",
        "            # This assumes the files are directly in the data_dir_path\n",
        "            user_files = [f for f in os.listdir(data_dir_path) if f.endswith('.jsonl') and f.startswith('user_')]\n",
        "            user_files.sort() # Ensure consistent order\n",
        "\n",
        "            # Extract user IDs from filenames (e.g., 'user_0' from 'user_0.jsonl')\n",
        "            self.user_list = [f.replace('.jsonl', '') for f in user_files]\n",
        "            print_rank(f\"Found {len(self.user_list)} user files: {self.user_list}\", loglevel=logging.INFO)\n",
        "\n",
        "            # Load data only for the specified user_idx (for training) or all (for test/val)\n",
        "            if user_idx != -1:\n",
        "                if user_idx < 0 or user_idx >= len(self.user_list):\n",
        "                    raise IndexError(f\"user_idx {user_idx} out of bounds for user list of size {len(self.user_list)}\")\n",
        "                current_user_id = self.user_list[user_idx]\n",
        "                user_file_path = os.path.join(data_dir_path, f\"{current_user_id}.jsonl\")\n",
        "                print_rank(f\"Loading data for user {current_user_id} from {user_file_path}\", loglevel=logging.DEBUG)\n",
        "                raw_user_data, raw_user_labels = self._read_jsonl_file(user_file_path)\n",
        "\n",
        "                # Store data for the current user in the required FLUTE format\n",
        "                self.user_data = {current_user_id: raw_user_data}\n",
        "                self.user_data_labels = {current_user_id: raw_user_labels}\n",
        "                self.num_samples = [len(raw_user_data)] # Number of samples for this user\n",
        "\n",
        "                print_rank(f\"Loaded {len(raw_user_data)} samples for user {current_user_id}\", loglevel=logging.INFO)\n",
        "\n",
        "            # If it's a test/validation dataset (user_idx == -1), load data for ALL users\n",
        "            # FLUTE's evaluation might require access to all test/val data.\n",
        "            else:\n",
        "                print_rank(\"Loading data for all users (test/validation)\", loglevel=logging.INFO)\n",
        "                all_raw_data = {}\n",
        "                all_raw_labels = {}\n",
        "                all_num_samples = []\n",
        "\n",
        "                for user_id in self.user_list:\n",
        "                    user_file_path = os.path.join(data_dir_path, f\"{user_id}.jsonl\")\n",
        "                    print_rank(f\"Reading data for user {user_id} from {user_file_path}\", loglevel=logging.DEBUG)\n",
        "                    raw_user_data, raw_user_labels = self._read_jsonl_file(user_file_path)\n",
        "\n",
        "                    all_raw_data[user_id] = raw_user_data\n",
        "                    all_raw_labels[user_id] = raw_user_labels\n",
        "                    all_num_samples.append(len(raw_user_data))\n",
        "                    print_rank(f\"Read {len(raw_user_data)} samples from {user_id}\", loglevel=logging.DEBUG)\n",
        "\n",
        "                self.user_data = all_raw_data\n",
        "                self.user_data_labels = all_raw_labels\n",
        "                self.num_samples = all_num_samples\n",
        "\n",
        "                total_samples_loaded = sum(self.num_samples)\n",
        "                print_rank(f\"Finished reading raw data for all {len(self.user_list)} users. Total samples read: {total_samples_loaded}\", loglevel=logging.INFO)\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            print_rank(f\"Error: Data directory or file not found at {data_dir_path}.\", loglevel=logging.ERROR)\n",
        "            raise e\n",
        "        except Exception as e:\n",
        "            print_rank(f\"An error occurred while listing files or reading data: {e}\", loglevel=logging.ERROR)\n",
        "            raise e\n",
        "\n",
        "\n",
        "    def _read_jsonl_file(self, file_path):\n",
        "        \"\"\"\n",
        "        Reads data from a single JSONL file.\n",
        "\n",
        "        Args:\n",
        "            file_path: The path to the JSONL file.\n",
        "        \"\"\"\n",
        "        raw_texts = []\n",
        "        raw_labels = []\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    if line.strip(): # Skip empty lines\n",
        "                        data_point = json.loads(line)\n",
        "                        # Assuming your data has 'prompt' for input text and 'label' for the target class\n",
        "                        raw_texts.append(data_point.get('prompt', data_point.get('text', ''))) # Use 'prompt' or fallback to 'text'\n",
        "                        raw_labels.append(data_point.get('label', '')) # Get the label\n",
        "\n",
        "            return raw_texts, raw_labels # Return raw strings\n",
        "\n",
        "\n",
        "        except FileNotFoundError:\n",
        "            print_rank(f\"Error: File not found at {file_path}\", loglevel=logging.ERROR)\n",
        "            return [], []\n",
        "        except json.JSONDecodeError:\n",
        "            print_rank(f\"Error decoding JSON from file: {file_path}. Ensure it's a valid JSONL file.\", loglevel=logging.ERROR)\n",
        "            return [], []\n",
        "        except Exception as e:\n",
        "            print_rank(f\"An error occurred while reading {file_path}: {e}\", loglevel=logging.ERROR)\n",
        "            return [], []\n",
        "\n",
        "    def _tokenize_and_format(self):\n",
        "        \"\"\"\n",
        "        Tokenizes the raw text data and formats it into input features and labels\n",
        "        ready for the model. Populates self.processed_data_list.\n",
        "        This is called after raw data is loaded for a specific user (user_idx != -1).\n",
        "        \"\"\"\n",
        "        print_rank(\"Tokenizing and formatting data...\", loglevel=logging.INFO)\n",
        "\n",
        "        # Combine all raw data and labels for the current user (since user_idx != -1)\n",
        "        # The user_data and user_data_labels dictionaries will only contain one entry\n",
        "        # corresponding to the current user when user_idx != -1.\n",
        "        current_user_id = list(self.user_data.keys())[0] # Get the single user ID\n",
        "        raw_texts = self.user_data[current_user_id]\n",
        "        raw_labels = self.user_data_labels[current_user_id]\n",
        "\n",
        "        if not raw_texts:\n",
        "            print_rank(f\"No raw data found for user {current_user_id} to tokenize.\", loglevel=logging.WARNING)\n",
        "            self.processed_data_list = []\n",
        "            return\n",
        "\n",
        "        # --- Tokenization ---\n",
        "        # Use the loaded tokenizer to process the raw text data\n",
        "        # This will return input_ids, attention_mask, and potentially token_type_ids\n",
        "        try:\n",
        "            tokenized_inputs = self.tokenizer(\n",
        "                raw_texts,\n",
        "                padding=self.padding,         # Pad sequences to max_seq_length or batch max length\n",
        "                truncation=True,              # Truncate sequences longer than max_seq_length\n",
        "                max_length=self.max_seq_length, # Set the maximum sequence length\n",
        "                return_tensors=\"pt\"           # Return PyTorch tensors\n",
        "            )\n",
        "            print_rank(f\"Tokenization complete. Shape of input_ids: {tokenized_inputs['input_ids'].shape}\", loglevel=logging.INFO)\n",
        "\n",
        "        except Exception as e:\n",
        "            print_rank(f\"Error during tokenization: {e}\", loglevel=logging.ERROR)\n",
        "            self.processed_data_list = []\n",
        "            return\n",
        "\n",
        "        # --- Label Processing ---\n",
        "        # Convert string labels to integer IDs.\n",
        "        # You need a consistent mapping from label strings to integers.\n",
        "        # For simplicity here, we'll generate a temporary map. In a real scenario,\n",
        "        # this map should be consistent across all clients.\n",
        "        unique_labels = sorted(list(set(raw_labels)))\n",
        "        self.label_map = {label: i for i, label in enumerate(unique_labels)}\n",
        "        print_rank(f\"Generated label map: {self.label_map}\", loglevel=logging.INFO)\n",
        "\n",
        "        try:\n",
        "            # Convert string labels to integer IDs using the label map\n",
        "            integer_labels = [self.label_map.get(label, -1) for label in raw_labels] # Use -1 for unknown labels\n",
        "\n",
        "            # Convert labels to a PyTorch tensor\n",
        "            # Ensure labels are of type long for classification loss\n",
        "            label_tensors = torch.tensor(integer_labels, dtype=torch.long)\n",
        "            print_rank(f\"Label tensor shape: {label_tensors.shape}\", loglevel=logging.INFO)\n",
        "\n",
        "        except Exception as e:\n",
        "            print_rank(f\"Error processing labels: {e}\", loglevel=logging.ERROR)\n",
        "            self.processed_data_list = []\n",
        "            return\n",
        "\n",
        "        # --- Combine Tokenized Inputs and Labels ---\n",
        "        # Create a list of dictionaries, where each dictionary represents one sample\n",
        "        # and contains the tokenized input features and the corresponding label.\n",
        "        self.processed_data_list = []\n",
        "        for i in range(len(raw_texts)):\n",
        "            sample = {\n",
        "                'input_ids': tokenized_inputs['input_ids'][i],\n",
        "                'attention_mask': tokenized_inputs['attention_mask'][i],\n",
        "                # Include token_type_ids if your model uses them and tokenizer provides them\n",
        "                # 'token_type_ids': tokenized_inputs.get('token_type_ids', torch.tensor([0]*self.max_seq_length))[i],\n",
        "                'labels': label_tensors[i] # Add the integer label\n",
        "            }\n",
        "            # Optionally add token_type_ids if present\n",
        "            if 'token_type_ids' in tokenized_inputs:\n",
        "                sample['token_type_ids'] = tokenized_inputs['token_type_ids'][i]\n",
        "\n",
        "            self.processed_data_list.append(sample)\n",
        "\n",
        "        print_rank(f\"Finished tokenizing and formatting {len(self.processed_data_list)} samples.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "# --- MedicalTextDataloader class ---\n",
        "# This class wraps the MedicalTextDataset and provides PyTorch DataLoaders\n",
        "class MedicalTextDataloader(BaseDataLoader):\n",
        "    \"\"\"\n",
        "    PyTorch dataloader for loading Medical Text Classification data.\n",
        "    Wraps the MedicalTextDataset and provides PyTorch DataLoaders.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, mode, data, num_workers=0, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the MedicalTextDataloader.\n",
        "\n",
        "        Args:\n",
        "            mode (str): The mode of the dataloader ('train', 'val', or 'test').\n",
        "            data: The data source (path to the dataset directory on Google Drive).\n",
        "                  This is passed by FLUTE based on your config.yaml.\n",
        "            num_workers (int): Number of subprocesses to use for data loading.\n",
        "            **kwargs: Additional keyword arguments passed by FLUTE,\n",
        "                      including 'args' (config dictionary) and 'user_idx').\n",
        "        \"\"\"\n",
        "        print_rank(f'Creating Dataloader for mode: {mode} and user_idx: {kwargs.get(\"user_idx\", -1)}', loglevel=logging.DEBUG)\n",
        "\n",
        "        # Extract arguments from kwargs that are explicitly expected by Dataloader __init__\n",
        "        args = kwargs.get('args')\n",
        "        if args is None:\n",
        "             raise ValueError(\"Arguments 'args' must be provided in kwargs\")\n",
        "\n",
        "        user_idx = kwargs.get('user_idx', -1) # Get user_idx, default to -1 for test/val\n",
        "        self.mode = mode\n",
        "        self.num_workers = num_workers\n",
        "        self.batch_size = args.get('batch_size') # Get batch size from args\n",
        "\n",
        "        if self.batch_size is None:\n",
        "             raise ValueError(\"Batch size must be provided in args\")\n",
        "\n",
        "        # Determine if this is a test/validation instance\n",
        "        test_only = self.mode != 'train'\n",
        "\n",
        "        # --- Prepare kwargs for MedicalTextDataset ---\n",
        "        # Create a copy of kwargs to pass to MedicalTextDataset\n",
        "        dataset_kwargs = kwargs.copy()\n",
        "\n",
        "        # Remove keys that are explicitly handled by MedicalTextDataset's __init__ signature\n",
        "        # or are not expected by it.\n",
        "        # The MedicalTextDataset __init__ signature is:\n",
        "        # __init__(self, data, args, tokenizer=None, test_only=False, user_idx=0, **kwargs)\n",
        "        # We need to ensure 'data', 'args', 'tokenizer', 'test_only', 'user_idx' are NOT\n",
        "        # present in dataset_kwargs when we unpack it, as they are passed explicitly.\n",
        "        dataset_kwargs.pop('data', None) # data is passed explicitly\n",
        "        dataset_kwargs.pop('args', None) # args is passed explicitly\n",
        "        dataset_kwargs.pop('tokenizer', None) # tokenizer is passed explicitly (as None here)\n",
        "        dataset_kwargs.pop('test_only', None) # test_only is passed explicitly\n",
        "        dataset_kwargs.pop('user_idx', None) # user_idx is passed explicitly\n",
        "        dataset_kwargs.pop('mode', None) # mode is used to derive test_only\n",
        "        dataset_kwargs.pop('num_workers', None) # num_workers is for the DataLoader, not Dataset\n",
        "\n",
        "\n",
        "        # --- Instantiate your custom Dataset class ---\n",
        "        # Pass data, args, test_only, user_idx explicitly, and the rest via **kwargs\n",
        "        self.dataset = MedicalTextDataset(\n",
        "            data=data,          # Pass the data path explicitly\n",
        "            args=args,          # Pass the args dictionary explicitly\n",
        "            tokenizer=None,     # Let the Dataset class load its own tokenizer based on args\n",
        "            test_only=test_only,# Pass test_only based on mode explicitly\n",
        "            user_idx=user_idx,  # Pass the user index explicitly\n",
        "            **dataset_kwargs    # Pass any remaining relevant kwargs here\n",
        "        )\n",
        "        print_rank('MedicalTextDataset instantiated successfully', loglevel=logging.DEBUG)\n",
        "\n",
        "        # --- Define the Data Collator ---\n",
        "        # For text classification, DataCollatorWithPadding is typically used\n",
        "        # to dynamically pad sequences within a batch to the longest sequence length.\n",
        "        # It requires the tokenizer for padding information.\n",
        "        # We pass the tokenizer instance obtained from the dataset.\n",
        "        self.data_collator = DataCollatorWithPadding(tokenizer=self.dataset.tokenizer)\n",
        "        print_rank('DataCollatorWithPadding created', loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "        # --- Determine the Sampler ---\n",
        "        # Use RandomSampler for training data to shuffle samples.\n",
        "        # Use SequentialSampler for validation/test data to maintain order.\n",
        "        if self.mode == 'train':\n",
        "            # For distributed training, you might use DistributedSampler\n",
        "            # Check if distributed training is enabled via args\n",
        "            if args.get('is_distributed', False):\n",
        "                 # Assuming DistributedSampler is available and imported\n",
        "                 self.sampler = DistributedSampler(self.dataset, shuffle=True)\n",
        "                 print_rank('Using DistributedSampler for training', loglevel=logging.DEBUG)\n",
        "            else:\n",
        "                 self.sampler = RandomSampler(self.dataset)\n",
        "                 print_rank('Using RandomSampler for training', loglevel=logging.DEBUG)\n",
        "        elif self.mode in ['val', 'test']:\n",
        "            self.sampler = SequentialSampler(self.dataset)\n",
        "            print_rank(f'Using SequentialSampler for {self.mode}', loglevel=logging.DEBUG)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {self.mode}. Must be 'train', 'val', or 'test'.\")\n",
        "\n",
        "\n",
        "        # --- Create the PyTorch DataLoader instance ---\n",
        "        # Call the parent class's __init__ (BaseDataLoader, which likely calls torch.utils.data.DataLoader)\n",
        "        # Pass the dataset, sampler, batch size, and data collator.\n",
        "        super(MedicalTextDataloader, self).__init__(\n",
        "            self.dataset,\n",
        "            batch_size=self.batch_size,\n",
        "            sampler=self.sampler,\n",
        "            collate_fn=self.data_collator,\n",
        "            drop_last=False, # Set to True for training if needed, False for eval\n",
        "            num_workers=self.num_workers,\n",
        "            pin_memory=True if torch.cuda.is_available() else False, # Pin memory if GPU is available\n",
        "        )\n",
        "        print_rank('PyTorch DataLoader created successfully', loglevel=logging.INFO)\n",
        "\n",
        "    def get_dataloader(self):\n",
        "        \"\"\"\n",
        "        Returns the instantiated PyTorch DataLoader object.\n",
        "        FLUTE will call this method to get the DataLoader.\n",
        "        \"\"\"\n",
        "        # Since this class inherits from DataLoader, the instance itself is the dataloader\n",
        "        return self\n",
        "\n",
        "    def get_dataset(self):\n",
        "        \"\"\"\n",
        "        Returns the instantiated custom Dataset object.\n",
        "        \"\"\"\n",
        "        return self.dataset\n",
        "\n",
        "    def get_num_samples(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    # The example had a get_user method, you can include it if needed\n",
        "    # def get_user(self):\n",
        "    #     \"\"\"\n",
        "    #     Returns the user ID associated with this dataloader instance (if applicable).\n",
        "    #     \"\"\"\n",
        "    #     # Assuming MedicalTextDataset stores the user ID\n",
        "    #     return self.dataset.user\n",
        "\n",
        "\n",
        "# --- Test Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Create temporary directories and dummy data/tokenizer ---\n",
        "    # Use tempfile to create a temporary directory that will be cleaned up automatically\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    data_dir = os.path.join(temp_dir, \"my_flute_formatted_dataset_users\")\n",
        "    tokenizer_dir = os.path.join(temp_dir, \"dummy_tokenizer\")\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "\n",
        "    # Create dummy tokenizer files (minimal set for AutoTokenizer)\n",
        "    # Using a small pre-trained model like 'bert-base-uncased' for dummy tokenizer\n",
        "    try:\n",
        "        dummy_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        dummy_tokenizer.save_pretrained(tokenizer_dir)\n",
        "        print_rank(f\"Dummy tokenizer saved to {tokenizer_dir}\", loglevel=logging.INFO)\n",
        "    except Exception as e:\n",
        "        print_rank(f\"Error creating dummy tokenizer: {e}\", loglevel=logging.ERROR)\n",
        "        # Clean up the temporary directory if tokenizer creation fails\n",
        "        shutil.rmtree(temp_dir)\n",
        "        raise # Re-raise the exception\n",
        "\n",
        "    # Create dummy data files simulating your JSONL format\n",
        "    dummy_data = {\n",
        "        \"user_0\": [\n",
        "            {\"prompt\": \"Symptoms: Patient has fever and cough. Diagnosis:\", \"label\": \"Common Cold\"},\n",
        "            {\"prompt\": \"Symptoms: Severe headache and stiff neck. Diagnosis:\", \"label\": \"Meningitis\"},\n",
        "            {\"prompt\": \"Symptoms: Fatigue and weight loss. Diagnosis:\", \"label\": \"Cancer\"},\n",
        "            {\"prompt\": \"Symptoms: Runny nose and sneezing. Diagnosis:\", \"label\": \"Allergy\"},\n",
        "            {\"prompt\": \"Symptoms: Chest pain and shortness of breath. Diagnosis:\", \"label\": \"Heart Disease\"},\n",
        "        ],\n",
        "        \"user_1\": [\n",
        "            {\"prompt\": \"Symptoms: Skin rash and itching. Diagnosis:\", \"label\": \"Eczema\"},\n",
        "            {\"prompt\": \"Symptoms: Joint pain and swelling. Diagnosis:\", \"label\": \"Arthritis\"},\n",
        "            {\"prompt\": \"Symptoms: Blurred vision and frequent urination. Diagnosis:\", \"label\": \"Diabetes\"},\n",
        "            {\"prompt\": \"Symptoms: Sore throat and swollen glands. Diagnosis:\", \"label\": \"Strep Throat\"},\n",
        "        ],\n",
        "         \"user_2\": [\n",
        "            {\"prompt\": \"Symptoms: Nausea and vomiting. Diagnosis:\", \"label\": \"Food Poisoning\"},\n",
        "            {\"prompt\": \"Symptoms: Back pain and difficulty moving. Diagnosis:\", \"label\": \"Sciatica\"},\n",
        "            {\"prompt\": \"Symptoms: Difficulty sleeping and anxiety. Diagnosis:\", \"label\": \"Insomnia\"},\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    for user_id, data_points in dummy_data.items():\n",
        "        file_path = os.path.join(data_dir, f\"{user_id}.jsonl\")\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            for dp in data_points:\n",
        "                f.write(json.dumps(dp) + '\\n')\n",
        "        print_rank(f\"Dummy data created for {user_id} at {file_path}\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    # --- Define dummy args for the Dataloader ---\n",
        "    # These args simulate what FLUTE would pass from your config.yaml\n",
        "    dummy_args = {\n",
        "        'batch_size': 2, # Set a small batch size for testing\n",
        "        'tokenizer_path': tokenizer_dir, # Point to the dummy tokenizer\n",
        "        'max_seq_length': 128, # Set a max sequence length\n",
        "        'padding': True,\n",
        "        'tokenizer_type_fast': True,\n",
        "        'is_distributed': False, # Simulate non-distributed run for this test\n",
        "        # Add other args that your Dataset or Dataloader might expect from config\n",
        "        # 'cache_dir': None, # Example of another arg\n",
        "        # 'mlm_probability': 0.15 # Example of another arg related to a different task\n",
        "    }\n",
        "\n",
        "    print_rank(\"\\nStarting Dataloader test...\", loglevel=logging.INFO)\n",
        "\n",
        "    try:\n",
        "        # --- Test Case 1: Instantiate Dataloader for a specific user (simulating training) ---\n",
        "        print_rank(\"\\nTesting Dataloader for User 0 (Training Mode):\", loglevel=logging.INFO)\n",
        "        # We need to find the index for 'user_0' from the generated user_list\n",
        "        # Instantiate a temporary dataset just to get the user list\n",
        "        # This is a workaround because the main Dataloader only loads data for the specified user_idx\n",
        "        temp_dataset_for_user_list = MedicalTextDataset(data=data_dir, args=dummy_args, user_idx=-1, test_only=True)\n",
        "        if 'user_0' not in temp_dataset_for_user_list.user_list:\n",
        "             raise ValueError(\"Dummy data does not contain 'user_0'\")\n",
        "        user_0_idx = temp_dataset_for_user_list.user_list.index('user_0')\n",
        "        del temp_dataset_for_user_list # Clean up the temporary dataset instance\n",
        "\n",
        "\n",
        "        dataloader_user0_train = MedicalTextDataloader(\n",
        "            mode='train',\n",
        "            data=data_dir, # Pass the directory path where dummy data files are\n",
        "            num_workers=0, # Use 0 workers for simplicity in testing\n",
        "            args=dummy_args, # Pass args as a single dictionary\n",
        "            user_idx=user_0_idx # Specify the user index for this client\n",
        "            # No need to unpack **kwargs here as we are passing args explicitly\n",
        "        )\n",
        "\n",
        "        print_rank(f\"Dataloader for User 0 (Training) has {len(dataloader_user0_train.dataset)} samples.\", loglevel=logging.INFO)\n",
        "\n",
        "        # Iterate through a few batches and print their structure\n",
        "        print_rank(\"Iterating through first 2 batches from User 0 Training Dataloader:\", loglevel=logging.INFO)\n",
        "        for i, batch in enumerate(dataloader_user0_train):\n",
        "            if i >= 2: # Limit to first 2 batches\n",
        "                break\n",
        "            print(f\"\\nBatch {i+1} structure (User 0 Training):\")\n",
        "            for key, tensor in batch.items():\n",
        "                if isinstance(tensor, torch.Tensor):\n",
        "                    # Print shape, dtype, and device\n",
        "                    print(f\"  {key}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}\")\n",
        "                else:\n",
        "                     print(f\"  {key}: type={type(tensor)}\")\n",
        "\n",
        "        # --- Test Case 2: Instantiate Dataloader for test/validation (simulating server eval) ---\n",
        "        print_rank(\"\\nTesting Dataloader for Test Mode (All Users):\", loglevel=logging.INFO)\n",
        "        dataloader_test = MedicalTextDataloader(\n",
        "            mode='test',\n",
        "            data=data_dir, # Pass the directory path where dummy data files are\n",
        "            num_workers=0, # Use 0 workers for simplicity in testing\n",
        "            args=dummy_args, # Pass args as a single dictionary\n",
        "            user_idx=-1 # Indicate test/validation mode (FLUTE loads data for all users)\n",
        "             # No need to unpack **kwargs here as we are passing args explicitly\n",
        "        )\n",
        "\n",
        "        print_rank(f\"Test Dataloader has {len(dataloader_test.dataset)} samples (total across all users).\", loglevel=logging.INFO)\n",
        "\n",
        "        # Iterate through a few batches from the test dataloader\n",
        "        print_rank(\"Iterating through first 2 batches from Test Dataloader:\", loglevel=logging.INFO)\n",
        "        for i, batch in enumerate(dataloader_test):\n",
        "            if i >= 2: # Limit to first 2 batches\n",
        "                break\n",
        "            print(f\"\\nBatch {i+1} structure (Test Mode):\")\n",
        "            for key, tensor in batch.items():\n",
        "                if isinstance(tensor, torch.Tensor):\n",
        "                    # Print shape, dtype, and device\n",
        "                    print(f\"  {key}: shape={tensor.shape}, dtype={tensor.dtype}, device={tensor.device}\")\n",
        "                else:\n",
        "                     print(f\"  {key}: type={type(tensor)}\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print_rank(f\"Test failed due to an error: {e}\", loglevel=logging.ERROR)\n",
        "\n",
        "    finally:\n",
        "        # --- Clean up temporary directories ---\n",
        "        print_rank(f\"\\nCleaning up temporary directory: {temp_dir}\", loglevel=logging.INFO)\n",
        "        shutil.rmtree(temp_dir)\n",
        "        print_rank(\"Cleanup complete.\", loglevel=logging.INFO)\n",
        "\n",
        "    print_rank(\"\\nDataloader test finished.\", loglevel=logging.INFO)\n"
      ],
      "metadata": {
        "id": "Qedo2aJ63s_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff3484d7-dbac-4979-b112-98ad3fac69a8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Batch 1 structure (User 0 Training):\n",
            "  input_ids: shape=torch.Size([2, 14]), dtype=torch.int64, device=cpu\n",
            "  attention_mask: shape=torch.Size([2, 14]), dtype=torch.int64, device=cpu\n",
            "  labels: shape=torch.Size([2]), dtype=torch.int64, device=cpu\n",
            "  token_type_ids: shape=torch.Size([2, 14]), dtype=torch.int64, device=cpu\n",
            "\n",
            "Batch 2 structure (User 0 Training):\n",
            "  input_ids: shape=torch.Size([2, 14]), dtype=torch.int64, device=cpu\n",
            "  attention_mask: shape=torch.Size([2, 14]), dtype=torch.int64, device=cpu\n",
            "  labels: shape=torch.Size([2]), dtype=torch.int64, device=cpu\n",
            "  token_type_ids: shape=torch.Size([2, 14]), dtype=torch.int64, device=cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Script to test the MedicalClassificationModel class\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import tempfile\n",
        "import torch as T # Corrected import: Alias torch as T\n",
        "import logging\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification, # Import the classification model\n",
        "    AutoTokenizer,\n",
        "    set_seed,\n",
        ")\n",
        "# Import PEFT libraries (needed for the model class definition and dummy creation)\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig # Assuming you used LoRA or similar PEFT method\n",
        "\n",
        "# Import logging and potentially your utils file\n",
        "import logging\n",
        "\n",
        "# Import metrics calculation tools (e.g., from scikit-learn)\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import numpy as np # Needed for numpy operations\n",
        "\n",
        "# Configure logging for better output during testing\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define a simple print_rank if you don't have a utils file\n",
        "def print_rank(msg, loglevel=logging.INFO):\n",
        "    \"\"\"Simple placeholder for distributed logging.\"\"\"\n",
        "    # In a real FLUTE distributed setting, this would handle printing only from rank 0\n",
        "    if loglevel >= logging.INFO: # Adjust based on desired verbosity\n",
        "        logger.log(loglevel, f\"[Test Script] {msg}\") # Use logger\n",
        "\n",
        "# Define a simple to_device utility if you don't have one in utils\n",
        "def to_device(tensor, device=None):\n",
        "    \"\"\"Moves a tensor to the specified device.\"\"\"\n",
        "    if device is None:\n",
        "        device = 'cuda' if T.cuda.is_available() else 'cpu' # Use T.cuda\n",
        "    # Check if tensor is already on the target device\n",
        "    if isinstance(tensor, T.Tensor) and tensor.device.type != device: # Use T.Tensor\n",
        "        return tensor.to(device)\n",
        "    return tensor # Return as is if not a tensor or already on device\n",
        "\n",
        "\n",
        "# --- Dummy BaseModel for testing purposes ---\n",
        "class BaseModel(T.nn.Module): # Use T.nn.Module\n",
        "    \"\"\"Dummy BaseModel inheriting from torch.nn.Module.\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # BaseModel might have other common methods, but for this test,\n",
        "        # inheriting from Module is sufficient to test loss/inference methods.\n",
        "        pass\n",
        "\n",
        "    # Include dummy signatures for methods expected by FLUTE if needed by the test\n",
        "    # def loss(self, inputs): pass\n",
        "    # def inference(self, inputs): pass\n",
        "    # def copy_state_dict(self, state_dict): pass\n",
        "    # def get_model(self): pass\n",
        "    # def set_eval(self): pass\n",
        "    # def set_train(self): pass\n",
        "\n",
        "\n",
        "# --- MedicalClassificationModel class (Code from medical_model_code immersive) ---\n",
        "# Include the full code of your MedicalClassificationModel class here\n",
        "class MedicalClassificationModel(BaseModel):\n",
        "    \"\"\"\n",
        "    Medical Text Classification Model using BioClinical-BERT with PEFT.\n",
        "    Inherits from FLUTE's BaseModel.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_config, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the MedicalClassificationModel.\n",
        "\n",
        "        Args:\n",
        "            model_config (dict): Dictionary containing model-specific configuration.\n",
        "                                 Expected to have a key like 'MedicalClassificationModel'.\n",
        "            **kwargs: Additional keyword arguments passed by FLUTE.\n",
        "        \"\"\"\n",
        "        super(MedicalClassificationModel, self).__init__()\n",
        "        print_rank(\"Initializing MedicalClassificationModel...\", loglevel=logging.INFO)\n",
        "\n",
        "        # Extract model configuration from model_config dictionary\n",
        "        model_args = model_config.get('MedicalClassificationModel', {})\n",
        "        if not model_args:\n",
        "             raise ValueError(\"Model configuration for 'MedicalClassificationModel' not found in model_config\")\n",
        "\n",
        "        seed = model_args.get('seed', 42)\n",
        "        set_seed(seed)\n",
        "        print_rank(f\"Set random seed to {seed}\", loglevel=logging.DEBUG)\n",
        "\n",
        "        # --- Load Base Model and Config ---\n",
        "        model_name_or_path = model_args.get('model_name_or_path')\n",
        "        if model_name_or_path is None:\n",
        "             raise ValueError(\"'model_name_or_path' must be provided in model_config['MedicalClassificationModel']\")\n",
        "\n",
        "        num_labels = model_args.get('num_labels')\n",
        "        if num_labels is None:\n",
        "             raise ValueError(\"'num_labels' must be provided in model_config['MedicalClassificationModel']\")\n",
        "\n",
        "        config_kwargs = {\n",
        "            \"cache_dir\": model_args.get('cache_dir', None),\n",
        "            \"revision\": model_args.get('revision', None),\n",
        "            \"use_auth_token\": model_args.get('use_auth_token', None),\n",
        "            \"num_labels\": num_labels, # Specify the number of output labels for classification\n",
        "        }\n",
        "\n",
        "        print_rank(f\"Loading base model config from: {model_name_or_path}\", loglevel=logging.INFO)\n",
        "        try:\n",
        "            config = AutoConfig.from_pretrained(model_name_or_path, **config_kwargs)\n",
        "        except Exception as e:\n",
        "            print_rank(f\"Error loading model config from {model_name_or_path}: {e}\", loglevel=logging.ERROR)\n",
        "            raise\n",
        "\n",
        "        print_rank(f\"Loading base model from: {model_name_or_path}\", loglevel=logging.INFO)\n",
        "        try:\n",
        "            # Load the base BioClinical-BERT model for sequence classification\n",
        "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
        "                model_name_or_path,\n",
        "                config=config,\n",
        "                cache_dir=model_args.get('cache_dir', None),\n",
        "                use_auth_token=model_args.get('use_auth_token', None),\n",
        "            )\n",
        "            print_rank(\"Base model loaded successfully.\", loglevel=logging.INFO)\n",
        "        except Exception as e:\n",
        "            print_rank(f\"Error loading base model from {model_name_or_path}: {e}\", loglevel=logging.ERROR)\n",
        "            raise\n",
        "\n",
        "\n",
        "        # --- Load PEFT Adapter ---\n",
        "        peft_model_path = model_args.get('peft_model_path')\n",
        "        if peft_model_path is None:\n",
        "             print_rank(\"No 'peft_model_path' provided. Loading base model without PEFT adapter.\", loglevel=logging.WARNING)\n",
        "             # If no PEFT path, the model is ready as is (e.g., for fine-tuning the full model)\n",
        "        else:\n",
        "            print_rank(f\"Loading PEFT adapter from: {peft_model_path}\", loglevel=logging.INFO)\n",
        "            try:\n",
        "                # Load the PEFT model (adapter) from the specified path\n",
        "                # This assumes the path is accessible in your environment\n",
        "                self.model = PeftModel.from_pretrained(self.model, peft_model_path)\n",
        "                print_rank(\"PEFT adapter loaded and integrated successfully.\", loglevel=logging.INFO)\n",
        "\n",
        "                # Print trainable parameters again after adding PEFT\n",
        "                total_params = 0\n",
        "                trainable_params = 0\n",
        "                for p in self.model.parameters():\n",
        "                    total_params += p.numel()\n",
        "                    if p.requires_grad:\n",
        "                        trainable_params += p.numel()\n",
        "                print_rank(f\"Total parameters after PEFT: {total_params}\", loglevel=logging.DEBUG)\n",
        "                print_rank(f\"Trainable parameters after PEFT: {trainable_params}\", loglevel=logging.DEBUG)\n",
        "\n",
        "            except Exception as e:\n",
        "                print_rank(f\"Error loading PEFT adapter from {peft_model_path}: {e}\", loglevel=logging.ERROR)\n",
        "                raise # Raise if PEFT is essential\n",
        "\n",
        "\n",
        "        print_rank(\"MedicalClassificationModel initialized.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    def copy_state_dict(self, state_dict):\n",
        "        \"\"\"\n",
        "        Copies the provided state dictionary to the model.\n",
        "        This method is used by FLUTE for model aggregation.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.model.load_state_dict(state_dict)\n",
        "            print_rank(\"Model state dictionary copied.\", loglevel=logging.DEBUG)\n",
        "        except Exception as e:\n",
        "            print_rank(f\"Error copying state dictionary: {e}\", loglevel=logging.ERROR)\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        \"\"\"\n",
        "        Returns the underlying PyTorch model instance (Hugging Face model).\n",
        "        \"\"\"\n",
        "        return self.model\n",
        "\n",
        "\n",
        "    def _prepare_inputs(self, inputs):\n",
        "        \"\"\"\n",
        "        Prepare inputs before feeding them to the model, moving them to the correct device.\n",
        "        \"\"\"\n",
        "        device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "        return {k: to_device(v, device) for k, v in inputs.items() if isinstance(v, T.Tensor)}\n",
        "\n",
        "\n",
        "    def loss(self, inputs):\n",
        "        \"\"\"\n",
        "        Computes the training loss for a batch of inputs.\n",
        "        Required by BaseModel.\n",
        "        \"\"\"\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "        outputs = self.model(**inputs)\n",
        "        loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def inference(self, inputs):\n",
        "        \"\"\"\n",
        "        Performs inference and computes evaluation metrics for a batch of inputs.\n",
        "        Required by BaseModel.\n",
        "        \"\"\"\n",
        "        inputs = self._prepare_inputs(inputs)\n",
        "\n",
        "        labels = inputs.get('labels', None)\n",
        "        if labels is not None:\n",
        "             labels = labels.detach().cpu().numpy()\n",
        "\n",
        "        batch_size = inputs['input_ids'].shape[0]\n",
        "\n",
        "        self.model.eval()\n",
        "        with T.no_grad(): # Use T.no_grad()\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        logits = outputs.logits if hasattr(outputs, 'logits') else outputs[1]\n",
        "        predictions = T.argmax(logits, dim=-1).detach().cpu().numpy() # Use T.argmax\n",
        "\n",
        "        metrics = {}\n",
        "        if labels is not None:\n",
        "             loss_fn = T.nn.CrossEntropyLoss() # Use T.nn.CrossEntropyLoss\n",
        "             # Ensure logits and labels are on the same device for loss calculation\n",
        "             loss_tensor = loss_fn(logits, inputs['labels'].to(logits.device))\n",
        "             metrics['loss'] = loss_tensor.item()\n",
        "\n",
        "             accuracy = accuracy_score(labels, predictions)\n",
        "             metrics['accuracy'] = accuracy\n",
        "\n",
        "             f1 = f1_score(labels, predictions, average='micro')\n",
        "             metrics['f1_score'] = f1\n",
        "\n",
        "        flute_metrics = {\n",
        "            'batch_size': batch_size\n",
        "        }\n",
        "\n",
        "        if 'loss' in metrics:\n",
        "             flute_metrics['output'] = metrics['loss']\n",
        "        elif 'accuracy' in metrics:\n",
        "             flute_metrics['output'] = metrics['accuracy']\n",
        "\n",
        "        if 'accuracy' in metrics:\n",
        "             flute_metrics['acc'] = metrics['accuracy']\n",
        "\n",
        "        if 'f1_score' in metrics:\n",
        "             flute_metrics['f1_score'] = {'value': metrics['f1_score'], 'higher_is_better': True}\n",
        "\n",
        "        return flute_metrics\n",
        "\n",
        "\n",
        "    def floating_point_ops(self, inputs):\n",
        "        \"\"\"\n",
        "        Computes the number of floating point operations.\n",
        "        \"\"\"\n",
        "        if hasattr(self.model, \"floating_point_ops\"):\n",
        "            return self.model.floating_point_ops(inputs)\n",
        "        else:\n",
        "            print_rank(\"Floating point ops calculation not implemented for this model.\", loglevel=logging.WARNING)\n",
        "            return 0\n",
        "\n",
        "\n",
        "    def set_eval(self):\n",
        "        \"\"\"\n",
        "        Bring the model into evaluation mode.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        print_rank(\"Model set to evaluation mode.\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "    def set_train(self):\n",
        "        \"\"\"\n",
        "        Bring the model into train mode.\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "        print_rank(\"Model set to training mode.\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "# --- Test Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Create temporary directories and dummy model/tokenizer ---\n",
        "    temp_dir = tempfile.mkdtemp()\n",
        "    model_dir = os.path.join(temp_dir, \"dummy_model\")\n",
        "    tokenizer_dir = os.path.join(temp_dir, \"dummy_tokenizer\")\n",
        "    peft_dir = os.path.join(temp_dir, \"dummy_peft_adapter\") # Directory for dummy PEFT adapter\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "    os.makedirs(peft_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "    # Create dummy tokenizer and save it\n",
        "    try:\n",
        "        dummy_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "        dummy_tokenizer.save_pretrained(tokenizer_dir)\n",
        "        print_rank(f\"Dummy tokenizer saved to {tokenizer_dir}\", loglevel=logging.INFO)\n",
        "    except Exception as e:\n",
        "        print_rank(f\"Error creating dummy tokenizer: {e}\", loglevel=logging.ERROR)\n",
        "        shutil.rmtree(temp_dir)\n",
        "        raise\n",
        "\n",
        "    # Create dummy base model for sequence classification and save it\n",
        "    # Need to specify num_labels for the dummy model\n",
        "    dummy_num_labels = 3 # Example: Common Cold, Meningitis, Cancer\n",
        "    try:\n",
        "        dummy_config = AutoConfig.from_pretrained(\"bert-base-uncased\", num_labels=dummy_num_labels)\n",
        "        dummy_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", config=dummy_config)\n",
        "        # We don't need to save the base model to disk for this test,\n",
        "        # as we'll instantiate it directly before applying PEFT.\n",
        "        # dummy_model.save_pretrained(model_dir)\n",
        "        print_rank(\"Dummy base model created in memory.\", loglevel=logging.INFO)\n",
        "    except Exception as e:\n",
        "        print_rank(f\"Error creating dummy base model: {e}\", loglevel=logging.ERROR)\n",
        "        shutil.rmtree(temp_dir)\n",
        "        raise\n",
        "\n",
        "    # --- Simulate creating and saving a dummy PEFT adapter ---\n",
        "    # Create a dummy PEFT model and save it so PeftModel.from_pretrained can load a valid file.\n",
        "    try:\n",
        "        # Define a dummy LoRA config\n",
        "        dummy_peft_config = LoraConfig(\n",
        "            peft_type=\"LORA\",\n",
        "            task_type=\"SEQ_CLS\",\n",
        "            r=8,\n",
        "            lora_alpha=16,\n",
        "            target_modules=[\"query\", \"value\"], # Example target modules for BERT\n",
        "            bias=\"none\",\n",
        "            # Add other necessary LoRA config parameters if needed\n",
        "        )\n",
        "\n",
        "        # Get a dummy PEFT model by applying the config to the dummy base model\n",
        "        dummy_peft_model_instance = get_peft_model(dummy_model, dummy_peft_config)\n",
        "        print_rank(\"Dummy PEFT model instance created.\", loglevel=logging.INFO)\n",
        "\n",
        "        # Save the dummy PEFT model. This will create the adapter_config.json and adapter_model.bin\n",
        "        dummy_peft_model_instance.save_pretrained(peft_dir)\n",
        "        print_rank(f\"Dummy PEFT adapter saved to {peft_dir}\", loglevel=logging.INFO)\n",
        "\n",
        "    except Exception as e:\n",
        "        print_rank(f\"Error creating and saving dummy PEFT adapter: {e}\", loglevel=logging.ERROR)\n",
        "        shutil.rmtree(temp_dir)\n",
        "        raise\n",
        "\n",
        "\n",
        "    # --- Define dummy model_config and kwargs for the Model ---\n",
        "    # These simulate what FLUTE would pass to the MedicalClassificationModel __init__\n",
        "    dummy_model_config = {\n",
        "        'model_type': 'MedicalClassificationModel', # Should match the class name\n",
        "        'model_folder': 'experiments/bachelor/model.py', # Relative path (not used in test init)\n",
        "        'MedicalClassificationModel': { # This key should match the one extracted in __init__\n",
        "            'model_name_or_path': \"bert-base-uncased\", # Use a public model name for base loading\n",
        "            'peft_model_path': peft_dir, # Point to the dummy PEFT adapter directory\n",
        "            'num_labels': dummy_num_labels, # Number of labels for classification\n",
        "            'cache_dir': None, # Or a path for caching\n",
        "            'use_fast_tokenizer': True,\n",
        "            'seed': 42,\n",
        "            # Add other model-specific args here if needed by your __init__\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # kwargs passed to the model __init__ might contain other things from FLUTE\n",
        "    dummy_kwargs = {\n",
        "        # Example: 'args' might contain general experiment arguments\n",
        "        'args': {'some_general_arg': 'value'},\n",
        "        # Add other potential kwargs here if your model __init__ uses them\n",
        "    }\n",
        "\n",
        "\n",
        "    print_rank(\"\\nStarting MedicalClassificationModel test...\", loglevel=logging.INFO)\n",
        "\n",
        "    try:\n",
        "        # --- Instantiate the Model ---\n",
        "        print_rank(\"Instantiating MedicalClassificationModel...\", loglevel=logging.INFO)\n",
        "        model_instance = MedicalClassificationModel(\n",
        "            model_config=dummy_model_config,\n",
        "            **dummy_kwargs # Unpack other potential kwargs\n",
        "        )\n",
        "        print_rank(\"MedicalClassificationModel instantiated successfully.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "        # --- Create a dummy batch ---\n",
        "        # This simulates a batch from your MedicalTextDataloader\n",
        "        batch_size = 4\n",
        "        seq_length = 50 # Example sequence length (should be <= max_seq_length)\n",
        "        dummy_batch = {\n",
        "            'input_ids': T.randint(0, dummy_tokenizer.vocab_size, (batch_size, seq_length)), # Use T.randint\n",
        "            'attention_mask': T.ones((batch_size, seq_length), dtype=T.long), # Use T.ones, T.long\n",
        "            'labels': T.randint(0, dummy_num_labels, (batch_size,), dtype=T.long), # Use T.randint, T.long\n",
        "            # Include token_type_ids if your model expects them and dataloader provides them\n",
        "            'token_type_ids': T.zeros((batch_size, seq_length), dtype=T.long) # Use T.zeros, T.long\n",
        "        }\n",
        "        print_rank(f\"\\nCreated dummy batch with size {batch_size} and sequence length {seq_length}.\", loglevel=logging.INFO)\n",
        "        for key, tensor in dummy_batch.items():\n",
        "             print_rank(f\"  Dummy batch['{key}'] shape: {tensor.shape}\", loglevel=logging.DEBUG)\n",
        "\n",
        "\n",
        "        # --- Test loss method ---\n",
        "        print_rank(\"\\nTesting loss method...\", loglevel=logging.INFO)\n",
        "        loss_output = model_instance.loss(dummy_batch)\n",
        "        print_rank(f\"Loss method returned: {loss_output}\", loglevel=logging.INFO)\n",
        "        # Check if the output is a tensor\n",
        "        assert isinstance(loss_output, T.Tensor), f\"Loss method did not return a torch.Tensor, but {type(loss_output)}\" # Use T.Tensor\n",
        "        # Check if the loss is a scalar tensor\n",
        "        assert loss_output.ndim == 0, f\"Loss tensor is not a scalar, has {loss_output.ndim} dimensions\"\n",
        "        print_rank(\"Loss method test successful.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "        # --- Test inference method ---\n",
        "        print_rank(\"\\nTesting inference method...\", loglevel=logging.INFO)\n",
        "        inference_output = model_instance.inference(dummy_batch)\n",
        "        print_rank(f\"Inference method returned: {inference_output}\", loglevel=logging.INFO)\n",
        "        # Check if the output is a dictionary\n",
        "        assert isinstance(inference_output, dict), f\"Inference method did not return a dict, but {type(inference_output)}\"\n",
        "        # Check for required keys\n",
        "        required_keys = ['output', 'acc', 'batch_size']\n",
        "        for key in required_keys:\n",
        "            assert key in inference_output, f\"Inference output missing required key: {key}\"\n",
        "        # Check types of required keys\n",
        "        assert isinstance(inference_output['output'], (float, int)), f\"Inference output['output'] is not a float/int, but {type(inference_output['output'])}\"\n",
        "        assert isinstance(inference_output['acc'], (float, int)), f\"Inference output['acc'] is not a float/int, but {type(inference_output['acc'])}\"\n",
        "        assert isinstance(inference_output['batch_size'], int), f\"Inference output['batch_size'] is not an int, but {type(inference_output['batch_size'])}\"\n",
        "        # Check custom metric format\n",
        "        if 'f1_score' in inference_output:\n",
        "            assert isinstance(inference_output['f1_score'], dict), f\"Inference output['f1_score'] is not a dict, but {type(inference_output['f1_score'])}\"\n",
        "            assert 'value' in inference_output['f1_score'], \"Inference output['f1_score'] missing 'value' key\"\n",
        "            assert 'higher_is_better' in inference_output['f1_score'], \"Inference output['f1_score'] missing 'higher_is_better' key\"\n",
        "            assert isinstance(inference_output['f1_score']['value'], (float, int)), f\"Inference output['f1_score']['value'] is not a float/int, but {type(inference_output['f1_score']['value'])}\"\n",
        "            assert isinstance(inference_output['f1_score']['higher_is_better'], bool), f\"Inference output['f1_score']['higher_is_better'] is not a bool, but {type(inference_output['f1_score']['higher_is_better'])}\"\n",
        "\n",
        "        print_rank(\"Inference method test successful.\", loglevel=logging.INFO)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print_rank(f\"Test failed due to an error: {e}\", loglevel=logging.ERROR)\n",
        "\n",
        "    finally:\n",
        "        # --- Clean up temporary directories ---\n",
        "        print_rank(f\"\\nCleaning up temporary directory: {temp_dir}\", loglevel=logging.INFO)\n",
        "        shutil.rmtree(temp_dir)\n",
        "        print_rank(\"Cleanup complete.\", loglevel=logging.INFO)\n",
        "\n",
        "    print_rank(\"\\nMedicalClassificationModel test finished.\", loglevel=logging.INFO)\n"
      ],
      "metadata": {
        "id": "gQOsRRnU-BBb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "1d43a1156d1a495fb0971ec3714d6962",
            "172749fe2f2d4a76a88f6f270dfc8a3a",
            "73b6e1f4046c42008c02814ba8619bba",
            "b4aaaf94fd9a42cb9a6642bf733fcc57",
            "0d47e241da74459db1d90b4109b400ee",
            "1edf157dd0bf4b70bae3f40e33ed6452",
            "e0e6eef66dc6476a9410fd023ced88d1",
            "456e5395de9340aca645af39c6505300",
            "8e0e43c4f31c455296a6f65880619fc3",
            "c6d9d2cebc994625824ba3fd01dd46c8",
            "937b0fc2f6f84c629edf39f0d2f4ed53"
          ]
        },
        "outputId": "bc3108cb-6fa7-4b94-e8da-5a9fa79d0ff0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d43a1156d1a495fb0971ec3714d6962"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd experiments"
      ],
      "metadata": {
        "id": "W1mf7njtriP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "UjXUpTKFrjuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bachelor"
      ],
      "metadata": {
        "id": "kdsxHm3Trz3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "ezLTSClKr4PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd dataloaders"
      ],
      "metadata": {
        "id": "u97BcnGisH3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "LHCG2hg7sJ3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "X8r6aMUXsLcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.run --nproc_per_node=2 e2e_trainer.py -outputPath ./medical_output -config /content/msrflute/experiments/bachelor/config.yaml -task MedicalTextClassification -backend gloo -dataPath /content/drive/MyDrive/my_flute_formatted_dataset_users"
      ],
      "metadata": {
        "id": "QGedsGBaC28P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd utils"
      ],
      "metadata": {
        "id": "EbBAWYrfJoFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "Gh3iFD-dJrhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(f\"Number of CPU cores available: {os.cpu_count()}\")"
      ],
      "metadata": {
        "id": "ZWX5zqnx0ZQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.run --nproc_per_node=2 e2e_trainer.py -dataPath \"/content/drive/My Drive/my_flute_formatted_dataset_users/\" -outputPath scratch -config experiments/bachelor/config.yaml -task bachelor -backend gloo"
      ],
      "metadata": {
        "id": "FuAnOjx8J17p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "6fn4-iLX0tu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "mIJesIOTtfy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m torch.distributed.run --nproc_per_node=2 /content/msrflute/e2e_trainer.py -outputPath /content/msrflute/experiments/bachelor/output -config /content/msrflute/experiments/bachelor/config.yaml -task bachelor -backend gloo -dataPath \"/content/drive/My Drive/my_flute_formatted_datasets_users/\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "62W3Eojg0kdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "# !ls\n",
        "%cd experiments/\n",
        "# !ls\n",
        "# %cd bachelor/\n",
        "!ls"
      ],
      "metadata": {
        "id": "YWpBC9Ek-HFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir bachelor_dir\n"
      ],
      "metadata": {
        "id": "4FmmsikBbTXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "18bV_D55e1Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "WjvGj83NetGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd bachelor_dir/"
      ],
      "metadata": {
        "id": "28gEaoAKfGy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "df7Sj-3LfMW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "id": "AzCozr0DjWTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd ../"
      ],
      "metadata": {
        "id": "s_hOcC_UjZ0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "c!mkdir dataloaders"
      ],
      "metadata": {
        "id": "35hMZs_jfNms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "f_yNNzKifqiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /msrflute/experiments/bachelor/msrflute/experiments/bachelor_dir/config.model.py"
      ],
      "metadata": {
        "id": "lG03_1huigDe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd dataloaders/\n"
      ],
      "metadata": {
        "id": "FmK3SGvAiqiH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!touch dataset.py"
      ],
      "metadata": {
        "id": "gKaItrbFiw1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile '/msrflute/experiments/bachelor/msrflute/experiments/bachelor_dir/dataloaders/dataset.py'\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Import BaseDataset from FLUTE.\n",
        "# Adjust the import path based on where you place this file relative to core/dataset.py\n",
        "# Example if saved in the msrflute root or a sibling directory to core:\n",
        "from core.dataset import BaseDataset\n",
        "# Example if saved in a 'client' subdirectory:\n",
        "# from client.core.dataset import BaseDataset\n",
        "\n",
        "\n",
        "# Note: The tokenizer will be passed to the __init__ method by the FLUTE framework.\n",
        "# You do NOT need to import or load the tokenizer within this class definition.\n",
        "# from transformers import AutoTokenizer # REMOVED\n",
        "\n",
        "class Dataset(BaseDataset):\n",
        "    \"\"\"\n",
        "    Custom Dataset class for your language model data, implementing FLUTE's BaseDataset.\n",
        "    Reads user-partitioned JSON Lines files from a directory in Google Drive.\n",
        "    Handles tokenization using a tokenizer provided during initialization.\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset_path: str, tokenizer, args, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the dataset, sets up the tokenizer, and calls load_data.\n",
        "\n",
        "        Args:\n",
        "            dataset_path (str): The path to your dataset folder in Google Drive\n",
        "                                containing user_<id>.jsonl files.\n",
        "            tokenizer: The tokenizer object provided by FLUTE (should be compatible\n",
        "                       with your model, e.g., from transformers).\n",
        "            args: Additional arguments passed during initialization (should contain\n",
        "                  max_seq_length).\n",
        "            **kwargs: Additional keyword arguments accepted by BaseDataset's init.\n",
        "        \"\"\"\n",
        "        # Pass all kwargs to super().__init__ as required by BaseDataset's __init__ signature\n",
        "        super().__init__(dataset_path=dataset_path, tokenizer=tokenizer, args=args, **kwargs)\n",
        "\n",
        "        self.dataset_path = dataset_path # Path to your dataset files in Google Drive\n",
        "        self.tokenizer = tokenizer       # Tokenizer instance provided by FLUTE\n",
        "        self.args = args                 # Arguments (should contain max_seq_length)\n",
        "\n",
        "        # Attributes required by FLUTE, to be populated by load_data()\n",
        "        self.user_list: List[str] = []      # List of all user IDs (e.g., ['0', '1', '2', '3', '4', '5'])\n",
        "        # user_data will store the *tokenized* data, structured as per FLUTE doc\n",
        "        # {\"user_id\": {\"x\": [list of tokenized samples (dicts of tensors)]}}\n",
        "        self.user_data: Dict[str, Dict[str, List[Dict[str, torch.Tensor]]]] = {}\n",
        "        self.num_samples: List[int] = []    # Number of samples for each user (in order of user_list)\n",
        "        # Optional: user_data_labels if your task requires explicit labels separate from inputs\n",
        "        self.user_data_labels: Dict[str, Dict[str, List[torch.Tensor]]] = {} # {\"user_id\": {\"x\": [list of label tensors]}}\n",
        "\n",
        "        # Call the load_data method to read and process the data\n",
        "        # Pass kwargs to load_data as required by BaseDataset's load_data signature\n",
        "        self.load_data(dataset_path=dataset_path, tokenizer=tokenizer, args=args, **kwargs)\n",
        "\n",
        "        # Build a global index mapping for __getitem__ after load_data populates user_data\n",
        "        self._build_global_index_mapping()\n",
        "\n",
        "        # CORRECTED: Updated print statement to reflect the new class name\n",
        "        print(f\"Dataset initialized with {len(self.user_list)} users and {len(self)} total samples.\")\n",
        "\n",
        "\n",
        "    def load_data(self, dataset_path: str, tokenizer, args, **kwargs):\n",
        "        \"\"\"\n",
        "        Reads user-partitioned JSON Lines files from dataset_path,\n",
        "        loads and tokenizes data for each user, and populates\n",
        "        self.user_list, self.user_data, and self.num_samples.\n",
        "        Uses the tokenizer provided during initialization.\n",
        "        \"\"\"\n",
        "        print(f\"Loading and tokenizing data from {dataset_path}...\")\n",
        "\n",
        "        self.user_list = []\n",
        "        self.user_data = {}\n",
        "        self.num_samples = []\n",
        "        self.user_data_labels = {} # Initialize labels attribute\n",
        "\n",
        "        # --- Your Code Here: Read User Data and Tokenize ---\n",
        "        # This logic assumes your data is saved as user_<id>.jsonl files\n",
        "        # in the dataset_path directory (your Google Drive folder).\n",
        "\n",
        "        all_user_ids = []\n",
        "        all_num_samples = []\n",
        "        all_user_tokenized_data = {} # Temp storage for tokenized data\n",
        "        # all_user_tokenized_labels = {} # Temp storage for tokenized labels if needed\n",
        "\n",
        "        try:\n",
        "            # Iterate through files in the dataset directory\n",
        "            # List files and sort them by filename to ensure consistent user order (user_0, user_1, ...)\n",
        "            filenames = sorted(os.listdir(dataset_path))\n",
        "\n",
        "            for filename in filenames:\n",
        "                # Adjust filename pattern based on how your user files are named\n",
        "                if filename.startswith(\"user_\") and filename.endswith(\".jsonl\"):\n",
        "                    # Extract user ID from filename (assuming format like 'user_0.jsonl')\n",
        "                    # User ID will be a string ('0', '1', etc.)\n",
        "                    user_id = filename.replace(\"user_\", \"\").replace(\".jsonl\", \"\")\n",
        "                    user_data_filepath = os.path.join(dataset_path, filename)\n",
        "\n",
        "                    raw_user_samples = []\n",
        "                    # raw_user_labels = [] # If you have raw labels in the file\n",
        "\n",
        "                    # Read raw data for this user from the JSON Lines file\n",
        "                    try:\n",
        "                        with open(user_data_filepath, 'r', encoding='utf-8') as f:\n",
        "                            for line in f:\n",
        "                                sample = json.loads(line)\n",
        "                                # Assuming your JSONL has 'prompt' and 'completion' keys\n",
        "                                raw_user_samples.append({\"prompt\": sample['prompt'], \"completion\": sample['completion']})\n",
        "                                # If you had labels in the file, add them here:\n",
        "                                # raw_user_labels.append(sample['label']) # Adjust key name if needed\n",
        "                    except Exception as e:\n",
        "                         print(f\"Error reading file {user_data_filepath}: {e}\")\n",
        "                         continue # Skip this user if their file can't be read\n",
        "\n",
        "                    if raw_user_samples:\n",
        "                        # Tokenize and format the raw samples for this user\n",
        "                        tokenized_samples_list = [] # List of tokenized sample dicts for this user\n",
        "                        # tokenized_labels_list = [] # List of label tensors for this user if needed\n",
        "\n",
        "                        for sample in raw_user_samples:\n",
        "                             prompt = sample['prompt']\n",
        "                             completion = sample['completion']\n",
        "\n",
        "                             # --- Your Code Here: Tokenization and Formatting ---\n",
        "                             # Use self.tokenizer which was passed during __init__\n",
        "                             # Adjust based on your model's expected input format and task (e.g., causal LM)\n",
        "                             # For a causal language model fine-tuning task, you typically concatenate\n",
        "                             # prompt and completion and use the tokenized sequence as input and labels.\n",
        "                             # Labels are often shifted internally by the model or trainer,\n",
        "                             # but sometimes you need to provide them explicitly, often with\n",
        "                             # prompt tokens masked out (set to -100 ignore_index).\n",
        "\n",
        "                             full_text = prompt + completion\n",
        "                             tokenized_input = self.tokenizer( # Use self.tokenizer here\n",
        "                                 full_text,\n",
        "                                 padding='max_length', # Or 'longest'. 'max_length' is common for fixed batch sizes\n",
        "                                 truncation=True,\n",
        "                                 max_length=self.args.max_seq_length, # Assuming args has this attribute\n",
        "                                 return_tensors=\"pt\" # Return PyTorch tensors\n",
        "                             )\n",
        "\n",
        "                             # Prepare labels. For causal LM, labels are usually the input IDs.\n",
        "                             # The loss is computed on the predicted token vs the next actual token.\n",
        "                             # If you want to only compute loss on the 'completion' part,\n",
        "                             # you need to set the label IDs corresponding to the 'prompt' tokens to -100.\n",
        "                             labels = tokenized_input[\"input_ids\"].clone()\n",
        "\n",
        "                             # Example of masking prompt tokens for loss calculation:\n",
        "                             # This requires tokenizing the prompt separately to find its length in tokens.\n",
        "                             # Be mindful of truncation when calculating prompt length.\n",
        "                             # Use self.tokenizer here as well\n",
        "                             with self.tokenizer.as_target_tokenizer(): # Some tokenizers need this context for labels\n",
        "                                 prompt_tokenization_result = self.tokenizer(\n",
        "                                     prompt,\n",
        "                                     truncation=True,\n",
        "                                     max_length=self.args.max_seq_length # Use the same max_length as input\n",
        "                                 )\n",
        "                             # Get the actual number of tokens in the prompt after potential truncation\n",
        "                             prompt_len = len(prompt_tokenization_result.input_ids)\n",
        "\n",
        "                             # Mask labels for prompt tokens\n",
        "                             if prompt_len < labels.size(1): # Ensure prompt_len is within tensor bounds\n",
        "                                 labels[:, :prompt_len] = -100 # -100 is the common ignore_index\n",
        "\n",
        "                             # Store the tokenized sample as a dictionary of tensors\n",
        "                             tokenized_sample_dict = {\n",
        "                                 \"input_ids\": tokenized_input[\"input_ids\"].squeeze(), # Squeeze to remove batch dim\n",
        "                                 \"attention_mask\": tokenized_input[\"attention_mask\"].squeeze(),\n",
        "                                 # Add other required tensor fields like \"token_type_ids\" if needed\n",
        "                                 \"labels\": labels.squeeze() # Squeeze the labels tensor\n",
        "                             }\n",
        "                             tokenized_samples_list.append(tokenized_sample_dict)\n",
        "\n",
        "                             # If you have labels, process and store them\n",
        "                             # tokenized_labels_list.append(process_and_tokenize_label(raw_label, self.tokenizer, self.args)) # Implement this helper\n",
        "\n",
        "                             # --- End Your Code: Tokenization and Formatting ---\n",
        "\n",
        "                        # Store the tokenized data for the user in the required {\"x\": [...]} format\n",
        "                        all_user_tokenized_data[user_id] = {\"x\": tokenized_samples_list}\n",
        "                        all_user_ids.append(user_id)\n",
        "                        all_num_samples.append(len(raw_user_samples)) # Store original sample count\n",
        "\n",
        "                        # If you have labels, populate the labels temp storage\n",
        "                        # all_user_tokenized_labels[user_id] = {\"x\": tokenized_labels_list}\n",
        "\n",
        "\n",
        "        except FileNotFoundError:\n",
        "             print(f\"Error: Dataset path not found at {dataset_path}\")\n",
        "             # Handle this error appropriately, maybe raise it\n",
        "\n",
        "        # Populate the required FLUTE attributes\n",
        "        self.user_list = all_user_ids\n",
        "        self.user_data = all_user_tokenized_data\n",
        "        self.num_samples = all_num_samples\n",
        "        # If you have labels:\n",
        "        # self.user_data_labels = all_user_tokenized_labels\n",
        "\n",
        "\n",
        "        print(f\"Data loading and tokenization complete. Loaded data for {len(self.user_list)} users.\")\n",
        "\n",
        "        # --- End Your Code: Read User Data and Tokenize ---\n",
        "\n",
        "    def _build_global_index_mapping(self):\n",
        "        \"\"\"\n",
        "        Builds a mapping from global sample index to (user_id, sample_index_within_user).\n",
        "        This is used by __getitem__. Populated after load_data.\n",
        "        \"\"\"\n",
        "        self._sample_indices: List[tuple] = []\n",
        "        for user_id in self.user_list:\n",
        "            # Assuming user_data[user_id][\"x\"] is a list of samples for this user\n",
        "            num_samples_this_user = len(self.user_data.get(user_id, {}).get(\"x\", []))\n",
        "            for i in range(num_samples_this_user):\n",
        "                self._sample_indices.append((user_id, i))\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total number of samples in the entire dataset across all users.\n",
        "        \"\"\"\n",
        "        # The total number of samples is the sum of samples for each user\n",
        "        # Or simply the length of the global index mapping\n",
        "        return len(self._sample_indices)\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Fetches a tokenized data sample given its global index.\n",
        "        Accesses data already loaded into self.user_data.\n",
        "\n",
        "        Args:\n",
        "            idx (int): The global index of the sample to fetch.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary of tensors representing the tokenized data sample,\n",
        "            e.g., {\"input_ids\": ..., \"attention_mask\": ..., \"labels\": ...}.\n",
        "        \"\"\"\n",
        "        if idx < 0 or idx >= len(self._sample_indices):\n",
        "             raise IndexError(\"Dataset index out of range\")\n",
        "\n",
        "        # Get the user_id and the sample index within that user's data\n",
        "        user_id, sample_index_within_user = self._sample_indices[idx]\n",
        "\n",
        "        # Access the tokenized sample directly from the loaded user_data\n",
        "        # Assuming user_data[user_id][\"x\"] is a list of tokenized sample dictionaries\n",
        "        try:\n",
        "            # Use .get() with default values for safer access\n",
        "            user_data_list = self.user_data.get(user_id, {}).get(\"x\", [])\n",
        "            if sample_index_within_user < len(user_data_list):\n",
        "                 tokenized_sample = user_data_list[sample_index_within_user]\n",
        "                 # If you have labels in user_data_labels, fetch them similarly\n",
        "                 # tokenized_label = self.user_data_labels.get(user_id, {}).get(\"x\", [])[sample_index_within_user]\n",
        "                 # tokenized_sample[\"labels\"] = tokenized_label # Add labels to the sample dict\n",
        "\n",
        "                 return tokenized_sample\n",
        "            else:\n",
        "                 print(f\"Index {sample_index_within_user} out of range for user {user_id} data list.\")\n",
        "                 return None # Or raise an error\n",
        "\n",
        "        except Exception as e: # Catch potential errors during access\n",
        "            print(f\"Error accessing data for user {user_id} or index {sample_index_within_user}: {e}\")\n",
        "            return None # Or raise an error\n",
        "\n"
      ],
      "metadata": {
        "id": "2vD6DdZri2_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/msrflute/experiments/bachelor/msrflute/experiments/bachelor_dir/dataloaders/dataset.py', 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "YmBJvTUHjLdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!touch config.yaml"
      ],
      "metadata": {
        "id": "eFSmcpkAfrk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!touch model.py"
      ],
      "metadata": {
        "id": "Yc7skl11fwBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "BXqiGRshfx6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uohzE2KRiZBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile '/msrflute/experiments/bachelor/msrflute/experiments/bachelor_dir/model.py'\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoConfig # Or AutoModelForSequenceClassification, etc.\n",
        "from peft import PeftModel, PeftConfig # Import necessary PEFT classes\n",
        "from typing import Dict, Any, Tuple\n",
        "\n",
        "# Import BaseModel from FLUTE.\n",
        "# Adjust the import path based on where you place this file relative to core/model.py\n",
        "from core.model import BaseModel # Assuming this file is in the msrflute root or a sibling directory to core\n",
        "\n",
        "# Import any necessary metrics if you plan to compute them in inference\n",
        "# from sklearn.metrics import f1_score # Example import for F1 score\n",
        "\n",
        "\n",
        "class Model(BaseModel):\n",
        "    \"\"\"\n",
        "    Custom Model class for your PEFT-tuned language model, implementing FLUTE's BaseModel.\n",
        "    Handles model loading, loss computation, and inference.\n",
        "    \"\"\"\n",
        "    def __init__(self, model_config: Dict[str, Any]):\n",
        "        \"\"\"\n",
        "        Initializes the model by loading the base model and the PEFT adapter.\n",
        "\n",
        "        Args:\n",
        "            model_config (Dict[str, Any]): A dictionary containing model configuration.\n",
        "                                          Expected keys:\n",
        "                                          - 'model_name_or_path': Path to the saved PEFT model directory\n",
        "                                                                  (containing adapter_config.json, etc.)\n",
        "                                          - 'base_model_name_or_path': Name or path of the original base model\n",
        "                                                                       (e.g., \"bert-base-uncased\")\n",
        "                                          - Any other model-specific parameters.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        print(\"Initializing MyLanguageModel...\")\n",
        "\n",
        "        self.model_name_or_path = model_config.get('model_name_or_path')\n",
        "        self.base_model_name_or_path = model_config.get('base_model_name_or_path')\n",
        "\n",
        "        if not self.model_name_or_path or not self.base_model_name_or_path:\n",
        "             raise ValueError(\"model_name_or_path and base_model_name_or_path must be provided in model_config\")\n",
        "\n",
        "        # Determine the device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        try:\n",
        "            # --- Your Code Here: Load Base Model and PEFT Adapter ---\n",
        "            # Load the base model first\n",
        "            # Use AutoModelForCausalLM for generative tasks, AutoModelForSequenceClassification for classification, etc.\n",
        "            # Ensure you load the correct model class for your task.\n",
        "            # Using .to(self.device) moves the model to the appropriate device\n",
        "            self.base_model = AutoModelForCausalLM.from_pretrained(self.base_model_name_or_path).to(self.device)\n",
        "            print(f\"Base model '{self.base_model_name_or_path}' loaded successfully.\")\n",
        "\n",
        "            # Load the PEFT adapter and merge it with the base model (or keep separate if preferred)\n",
        "            # Loading directly from the saved PEFT directory path\n",
        "            self.model = PeftModel.from_pretrained(self.base_model, self.model_name_or_path).to(self.device)\n",
        "            print(f\"PEFT adapter from '{self.model_name_or_path}' loaded successfully.\")\n",
        "\n",
        "            # Optional: If you want to merge the adapter weights into the base model for inference speed\n",
        "            # self.model = self.model.merge_and_unload()\n",
        "            # print(\"PEFT adapter merged into base model.\")\n",
        "\n",
        "            # Set the model to training mode initially\n",
        "            self.model.train()\n",
        "\n",
        "            # --- End Your Code: Model Loading ---\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading model or adapter: {e}\")\n",
        "            raise # Re-raise the exception to stop initialization\n",
        "\n",
        "        print(\"MyLanguageModel initialization complete.\")\n",
        "\n",
        "\n",
        "    def loss(self, input: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass and computes the loss for a batch of input data.\n",
        "\n",
        "        Args:\n",
        "            input (Dict[str, torch.Tensor]): A dictionary containing input tensors\n",
        "                                             from the dataset, e.g., {'input_ids': ...,\n",
        "                                             'attention_mask': ..., 'labels': ...}.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The computed loss for the batch.\n",
        "        \"\"\"\n",
        "        # Move input tensors to the correct device\n",
        "        # Assuming input dictionary keys match the model's expected inputs\n",
        "        input = {k: v.to(self.device) for k, v in input.items()}\n",
        "\n",
        "        # --- Your Code Here: Forward Pass and Loss Computation ---\n",
        "        # For Hugging Face models, passing labels directly to the forward method\n",
        "        # computes the loss automatically for tasks like causal language modeling.\n",
        "        # The model's forward method returns an object (like CausalLMOutput)\n",
        "        # which contains the loss if labels are provided.\n",
        "\n",
        "        # Ensure model is in training mode for loss computation\n",
        "        self.model.train()\n",
        "\n",
        "        # Perform the forward pass\n",
        "        outputs = self.model(**input)\n",
        "\n",
        "        # The loss is typically available in the outputs object\n",
        "        # For CausalLMOutput, it's outputs.loss\n",
        "        computed_loss = outputs.loss\n",
        "\n",
        "        # --- End Your Code: Forward Pass and Loss Computation ---\n",
        "\n",
        "        return computed_loss\n",
        "\n",
        "\n",
        "    def inference(self, input: Dict[str, torch.Tensor]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Performs a forward pass and computes metrics for evaluation.\n",
        "\n",
        "        Args:\n",
        "            input (Dict[str, torch.Tensor]): A dictionary containing input tensors\n",
        "                                             from the dataset for evaluation.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, Any]: A dictionary containing computed metrics.\n",
        "                            Must include 'batch_size'.\n",
        "                            Metrics other than 'output', 'acc', 'batch_size'\n",
        "                            must be in the format {'metric_name': {'value': ..., 'higher_is_better': True/False}}.\n",
        "        \"\"\"\n",
        "        # Move input tensors to the correct device\n",
        "        input = {k: v.to(self.device) for k, v in input.items()}\n",
        "\n",
        "        # Ensure model is in evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # --- Your Code Here: Forward Pass and Metric Computation ---\n",
        "        # For inference, you might not need labels, or you might use them\n",
        "        # to compute metrics like perplexity or accuracy.\n",
        "        # If you are doing text generation, you might use model.generate().\n",
        "        # For simplicity, let's compute loss (which can serve as a proxy for perplexity)\n",
        "        # and potentially other metrics if applicable to your task.\n",
        "\n",
        "        n_samples = input['input_ids'].shape[0] # Get batch size\n",
        "\n",
        "        with torch.no_grad(): # Disable gradient calculation for inference\n",
        "            outputs = self.model(**input)\n",
        "\n",
        "            # Compute loss (e.g., for perplexity calculation)\n",
        "            # Note: This loss is the same as in the loss method, but calculated in eval mode\n",
        "            inference_loss = outputs.loss.item() if outputs.loss is not None else None\n",
        "\n",
        "            # Example: Compute a simple accuracy if applicable (e.g., token prediction accuracy)\n",
        "            # This requires comparing model predictions (logits) to labels.\n",
        "            # This is more complex for causal LM than classification.\n",
        "            # For causal LM, accuracy is often measured on the predicted next token.\n",
        "            # If labels are masked (-100) in the prompt, you can compute accuracy on non-masked tokens.\n",
        "            accuracy = None\n",
        "            if 'labels' in input and outputs.logits is not None:\n",
        "                 # Example: Simple token accuracy ignoring masked labels\n",
        "                 predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "                 # Create a mask for non-masked labels\n",
        "                 label_mask = input['labels'] != -100\n",
        "                 # Compare predictions to labels only where labels are not masked\n",
        "                 correct_predictions = (predictions[label_mask] == input['labels'][label_mask]).sum()\n",
        "                 total_predictions = label_mask.sum()\n",
        "                 if total_predictions > 0:\n",
        "                      accuracy = correct_predictions.item() / total_predictions.item()\n",
        "                 else:\n",
        "                      accuracy = 0.0\n",
        "\n",
        "\n",
        "            # --- End Your Code: Forward Pass and Metric Computation ---\n",
        "\n",
        "        # Return metrics in the required FLUTE format\n",
        "        metrics = {\n",
        "            'batch_size': n_samples,\n",
        "            # Add other metrics here\n",
        "            # Example: Loss (as a proxy for perplexity)\n",
        "            'inference_loss': {'value': inference_loss, 'higher_is_better': False} if inference_loss is not None else None,\n",
        "            # Example: Token Accuracy (if computed)\n",
        "            'token_accuracy': {'value': accuracy, 'higher_is_better': True} if accuracy is not None else None,\n",
        "            # 'output': outputs.logits # You can optionally return raw outputs if needed by FLUTE\n",
        "        }\n",
        "\n",
        "        # Remove None values from the metrics dictionary\n",
        "        metrics = {k: v for k, v in metrics.items() if v is not None}\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    # Optional: Implement other methods if required by FLUTE or your task\n",
        "    # def configure_optimizers(self):\n",
        "    #     \"\"\"\n",
        "    #     Configures the optimizer and learning rate scheduler.\n",
        "    #     FLUTE might handle this externally based on config, but sometimes\n",
        "    #     it's defined here. Check FLUTE documentation.\n",
        "    #     \"\"\"\n",
        "    #     optimizer = torch.optim.AdamW(self.parameters(), lr=self.model_config.get('learning_rate', 1e-4))\n",
        "    #     # Add scheduler if needed\n",
        "    #     return optimizer\n",
        "\n"
      ],
      "metadata": {
        "id": "s3afA3B9hhGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/msrflute/experiments/bachelor/msrflute/experiments/bachelor_dir/model.py', 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "xPUCMsTnh8HC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile '/msrflute/experiments/bachelor/msrflute/experiments/bachelor_dir/config.yaml'\n",
        "# FLUTE Configuration File for Language Model Fine-tuning - Based on Example Structure\n",
        "\n",
        "# This configuration is structured based on the example provided,\n",
        "# aiming to resolve validation errors by matching the expected schema.\n",
        "\n",
        "# ==============================================================================\n",
        "# model_config: Configuration for loading and initializing the model.\n",
        "# ==============================================================================\n",
        "model_config:\n",
        "    # model_type: The name of your custom model class (inheriting from BaseModel).\n",
        "    # This should match the class name in your model.py file.\n",
        "    model_type: Model # Ensure this matches your class name exactly (Uppercase M)\n",
        "\n",
        "    # model_folder: The relative path from the msrflute root directory\n",
        "    # to the Python file containing your custom model class.\n",
        "    # Based on your structure and the error message, this path should be\n",
        "    # relative to the msrflute root.\n",
        "    model_folder: experiments/bachelor/\n",
        "\n",
        "    # Added 'type' key to satisfy error message formatting in make_model\n",
        "    # This value is likely not used for anything else.\n",
        "    #type: my_custom_model # Dummy type to prevent KeyError in make_model error message\n",
        "\n",
        "    # model_name_or_path: Path to your saved PEFT model directory in Google Drive.\n",
        "    # This is used by your custom Model class to load the adapter.\n",
        "    # CORRECTED: Uncommented and added placeholder comment\n",
        "    model_name_or_path: /content/drive/My Drive/clinical_peft_model/ # REPLACE with your actual PEFT model path\n",
        "\n",
        "    # base_model_name_or_path: Name or path of the original base model used for PEFT.\n",
        "    # This is used by your custom Model class to load the base model architecture.\n",
        "    # CORRECTED: Uncommented and added placeholder comment\n",
        "    base_model_name_or_path: emilyalsentzer/Bio_ClinicalBERT # REPLACE with your actual base model name\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# dp_config: Differential Privacy setup.\n",
        "# Set enable_local_dp to true to enable DP.\n",
        "# ==============================================================================\n",
        "dp_config:\n",
        "    enable_local_dp: false # Set to true if you want to use Differential Privacy\n",
        "    # Add DP parameters here if enabling, e.g.:\n",
        "    # dp_type: gaussian\n",
        "    # noise_multiplier: 1.0\n",
        "    # l2_norm_clip: 1.0\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# privacy_metrics_config: Configuration for computing privacy metrics.\n",
        "# Set apply_metrics to true to enable.\n",
        "# ==============================================================================\n",
        "privacy_metrics_config:\n",
        "    apply_metrics: false # Set to true if you want to compute privacy metrics\n",
        "    # Add privacy metrics parameters here if enabling.\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# strategy: Defines the federated optimization strategy.\n",
        "# Common options are DGA or FedAvg.\n",
        "# ==============================================================================\n",
        "strategy: FedAvg # Choose your federated optimization strategy (e.g., FedAvg, DGA)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# server_config: Server-side settings for the federated learning process.\n",
        "# ==============================================================================\n",
        "server_config:\n",
        "    wantRL: false # Whether to use RL-based meta-optimizers (usually false)\n",
        "    resume_from_checkpoint: false # Set to true to resume training from a checkpoint\n",
        "    do_profiling: false # Enable profiling for performance analysis\n",
        "\n",
        "    # Optimizer used to update the global model on the server.\n",
        "    optimizer_config:\n",
        "        type: sgd # Server optimizer type (e.g., sgd, adam) - Matching example\n",
        "        lr: 1.0 # Learning rate for the server optimizer (adjust as needed)\n",
        "\n",
        "    # Annealer for the server learning rate.\n",
        "    # Matching example structure\n",
        "    annealing_config:\n",
        "        type: step_lr # Annealer type (e.g., step_lr, cosine_annealing)\n",
        "        step_interval: epoch # When to step the learning rate (e.g., epoch, iteration)\n",
        "        gamma: 1.0 # Decay factor (e.g., 0.1 for step_lr) - Set to 1.0 for no decay initially\n",
        "        step_size: 100 # Stepsize for step_lr (e.g., decay every 100 epochs) - Set a value\n",
        "\n",
        "\n",
        "    val_freq: 50 # Validation rounds frequency (set to a high number or 0 if no validation) - Matching example\n",
        "    rec_freq: 100 # Testing rounds frequency (set to a high number or 0 if no testing) - Matching example\n",
        "    # CORRECTED: Set initial_val and initial_rec back to false\n",
        "    initial_val: false # Enable initial validation round - Matching example\n",
        "    initial_rec: false # Enable initial initial testing round - Matching example\n",
        "\n",
        "    max_iteration: 500 # Total number of federated learning rounds (iterations) - Matching example (adjust if needed)\n",
        "    num_clients_per_iteration: 6 # Number of clients participating in each round (set to your number of artificial users) - Matching example (adjust if needed)\n",
        "\n",
        "    # Data configuration for the test/val dataloaders on the server.\n",
        "    # These typically load data for evaluation purposes.\n",
        "    # Matching example structure\n",
        "    data_config:\n",
        "        val:\n",
        "            batch_size: 32 # Batch size for server-side validation (if used) - Matching example (adjust if needed)\n",
        "            # REMOVED: dataset_type and dataset_folder are not expected here\n",
        "            val_data: null # Assigned to null because dataset is being instantiated - Matching example\n",
        "\n",
        "        test:\n",
        "            batch_size: 32 # Batch size for server-side testing (if used) - Matching example (adjust if needed)\n",
        "            # REMOVED: dataset_type and dataset_folder are not expected here\n",
        "            test_data: null # Assigned to null because dataset is being instantiated - Matching example\n",
        "\n",
        "    type: model_optimization # Server type (model_optimization is the only available for now) - Matching example\n",
        "    aggregate_median: softmax # How aggregations weights are computed - Matching example\n",
        "\n",
        "    initial_lr_client: 0.0001 # Learning rate used for client optimizers (important!) - Matching example (adjust if needed)\n",
        "    lr_decay_factor: 1.0 # Decay factor for client LR - Matching example\n",
        "    weight_train_loss: train_loss # Determines how each client's weight is computed - Matching example\n",
        "    best_model_criterion: inference_loss # Determines the best model based on a metric from your model's inference method - Matching example (adjust if needed)\n",
        "    fall_back_to_best_model: false # If a model degrades, use the previous best model - Matching example\n",
        "    softmax_beta: 1.0 # Beta value to use for the softmax DGA (adjust if using DGA) - Matching example\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# client_config: Client-side settings for the federated learning process.\n",
        "# ==============================================================================\n",
        "client_config:\n",
        "    do_profiling: false # run profiling and compute runtime metrics - Matching example\n",
        "    ignore_subtask: false # Matching example\n",
        "    data_config: # where to get training data from - Matching example\n",
        "        train:\n",
        "            batch_size: 4 # Batch size for client-side training (ADJUST based on Colab memory) - Matching example (adjust if needed)\n",
        "            # REMOVED: dataset_type and dataset_folder are not expected here\n",
        "            list_of_train_data: null # Assigned to null because dataset is being instantiated - Matching example\n",
        "            desired_max_samples: -1 # Use -1 to use all samples for the client - Matching example (adjust if needed)\n",
        "\n",
        "    optimizer_config: # this is the optimizer used by the client - Matching example\n",
        "        type: sgd # Using SGD - Matching example\n",
        "        lr: 0.001 # This value is typically overridden by initial_lr_client from server_config - Matching example\n",
        "        # Removed Adam specific parameters (eps, weight_decay) as we are using SGD\n",
        "\n",
        "    type: optimization # The type of client (always set \"optimization for now\")\n",
        "\n",
        "    # Number of local epochs each client trains for per round\n",
        "    local_epochs: 1 # ADJUST as needed (e.g., 1, 3, 5) - Matching example (adjust if needed)\n",
        "\n",
        "# ==============================================================================\n",
        "# End of Configuration File\n",
        "# ==============================================================================\n"
      ],
      "metadata": {
        "id": "hT1Vgnpff6lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/msrflute/experiments/bachelor/msrflute/experiments/bachelor_dir/config.yaml', 'r') as f:\n",
        "    print(f.read())"
      ],
      "metadata": {
        "id": "fO6Cr0y_g_PA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}